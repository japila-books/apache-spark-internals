== [[SparkSession]] `SparkSession` -- The Entry Point to Spark SQL

`SparkSession` is the entry point to Spark SQL. It is the very first object you have to create to start developing Spark SQL applications using the fully-typed link:spark-sql-dataset.adoc[Dataset] (and untyped link:spark-sql-dataframe.adoc[DataFrame]) data abstractions.

NOTE: `SparkSession` has merged link:spark-sql-sqlcontext.adoc[SQLContext] and link:spark-sql-hive-integration.adoc[HiveContext] in one object as of Spark *2.0.0*.

You use the <<builder, SparkSession.builder>> method to create an instance of `SparkSession`.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // avoid hardcoding the deployment environment
  .enableHiveSupport()              // self-explanatory, isn't it?
  .config("spark.sql.warehouse.dir", "target/spark-warehouse")
  .getOrCreate
----

And stop the current `SparkSession` using <<stop, stop>> method.

[source, scala]
----
spark.stop
----

You can have multiple ``SparkSession``s in a single Spark application.

Internally, `SparkSession` requires a link:spark-sparkcontext.adoc[SparkContext] and an optional <<SharedState, SharedState>> (that represents the shared state across `SparkSession` instances).

.`SparkSession` Class and Instance Methods
[cols="1,2",options="header",width="100%"]
|===
| Method | Description
| <<builder, builder>> | "Opens" a builder to get or create a `SparkSession` instance
| <<version, version>> | Returns the current version of Spark.
| <<implicits, implicits>> | Use `import spark.implicits._` to import the implicits conversions and create `Datasets` from (almost arbitrary) Scala objects.
| <<emptyDataset, emptyDataset[T]>> | Creates an empty `Dataset[T]`.
| <<range, range>> | Creates a `Dataset[Long]`.
| <<sql, sql>> | Executes a SQL query (and returns a `DataFrame`).
| <<udf, udf>> | Access to user-defined functions (UDFs).
| <<table, table>> | Creates a `DataFrame` from a table.
| <<catalog, catalog>> | Access to the catalog of the entities of structured queries
| <<read, read>> | Access to `DataFrameReader` to read a `DataFrame` from external files and storage systems.
| <<conf, conf>> | Access to the current runtime configuration.
| <<readStream, readStream>> | Access to `DataStreamReader` to read streaming datasets.
| <<streams, streams>> | Access to `StreamingQueryManager` to manage structured streaming queries.
| <<newSession, newSession>> | Creates a new `SparkSession`.
| <<stop, stop>> | Stops the `SparkSession`.
|===

[TIP]
====
Use link:spark-sql-settings.adoc#spark_sql_warehouse_dir[spark.sql.warehouse.dir] Spark property to change the location of Hive's `hive.metastore.warehouse.dir` property, i.e. the location of the Hive local/embedded metastore database (using Derby).

Refer to <<SharedState, SharedState>> in this document to learn about (the low-level details of) Spark SQL support for Apache Hive.

See also the official https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin[Hive Metastore Administration] document.
====

=== [[builder]] Creating `SparkSession` Using `Builder` -- `builder` method

[source, scala]
----
builder(): Builder
----

`builder` creates a new link:spark-sql-sparksession-builder.adoc[Builder] that you use to build a fully-configured `SparkSession` using a _fluent API_.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val builder = SparkSession.builder
----

TIP: Read about https://en.wikipedia.org/wiki/Fluent_interface[Fluent interface] design pattern in Wikipedia, the free encyclopedia.

=== [[version]] Accessing Version of Spark -- `version` Method

[source, scala]
----
version: String
----

`version` returns the version of Apache Spark in use.

Internally, `version` uses `spark.SPARK_VERSION` value that is the `version` property in `spark-version-info.properties` properties file on CLASSPATH.

=== [[implicits]] Implicit Conversions -- `implicits` object

The `implicits` object is a helper class with the Scala implicit methods (aka _conversions_) to convert Scala objects to link:spark-sql-dataset.adoc[Datasets], link:spark-sql-dataframe.adoc[DataFrames] and link:spark-sql-columns.adoc[Columns]. It also defines link:spark-sql-Encoder.adoc[Encoders] for Scala's "primitive" types, e.g. `Int`, `Double`, `String`, and their products and collections.

[NOTE]
====
Import the implicits by `import spark.implicits._`.

[source, scala]
----
val spark = SparkSession.builder.getOrCreate()
import spark.implicits._
----
====

`implicits` object offers support for creating `Dataset` from `RDD` of any type (for which an link:spark-sql-Encoder.adoc[encoder] exists in scope), or case classes or tuples, and `Seq`.

`implicits` object also offers conversions from Scala's `Symbol` or `$` to `Column`.

It also offers conversions from `RDD` or `Seq` of `Product` types (e.g. case classes or tuples) to `DataFrame`. It has direct conversions from `RDD` of `Int`, `Long` and `String` to `DataFrame` with a single column name `_1`.

NOTE: It is only possible to call `toDF` methods on `RDD` objects of `Int`, `Long`, and `String` "primitive" types.

=== [[emptyDataset]] Creating Empty Dataset -- `emptyDataset` method

[source, scala]
----
emptyDataset[T: Encoder]: Dataset[T]
----

`emptyDataset` creates an empty link:spark-sql-dataset.adoc[Dataset] (assuming that future records being of type `T`).

[source, scala]
----
scala> val strings = spark.emptyDataset[String]
strings: org.apache.spark.sql.Dataset[String] = [value: string]

scala> strings.printSchema
root
 |-- value: string (nullable = true)
----

`emptyDataset` creates a  link:spark-sql-logical-plan-LocalRelation.adoc[`LocalRelation` logical query plan].

=== [[createDataset]] Creating Dataset from Local Collections and RDDs -- `createDataset` methods

[source, scala]
----
createDataset[T : Encoder](data: Seq[T]): Dataset[T]
createDataset[T : Encoder](data: RDD[T]): Dataset[T]
----

`createDataset` is an experimental API to create a link:spark-sql-dataset.adoc[Dataset] from a local Scala collection, i.e. `Seq[T]`, Java's `List[T]`, or a distributed `RDD[T]`.

[source, scala]
----
scala> val one = spark.createDataset(Seq(1))
one: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> one.show
+-----+
|value|
+-----+
|    1|
+-----+
----

`createDataset` creates a link:spark-sql-logical-plan-LocalRelation.adoc[`LocalRelation` logical query plan] (for the input `data` collection) or `LogicalRDD` (for the input `RDD[T]`).

[TIP]
====
You'd be better off using link:spark-sql-dataset.adoc#implicits[Scala implicits and `toDS` method] instead (that does this conversion automatically for you).

[source, scala]
----
val spark: SparkSession = ...
import spark.implicits._

scala> val one = Seq(1).toDS
one: org.apache.spark.sql.Dataset[Int] = [value: int]
----
====

Internally, `createDataset` first looks up the implicit link:spark-sql-Encoder.adoc#ExpressionEncoder[expression encoder] in scope to access the ``AttributeReference``s (of the link:spark-sql-schema.adoc[schema]).

NOTE: Only unresolved link:spark-sql-Encoder.adoc#ExpressionEncoder[expression encoders] are currently supported.

The expression encoder is then used to map elements (of the input `Seq[T]`) into a collection of link:spark-sql-InternalRow.adoc[InternalRows]. With the references and rows, `createDataset` returns a link:spark-sql-dataset.adoc[Dataset] with a link:spark-sql-logical-plan-LocalRelation.adoc[`LocalRelation` logical query plan].

=== [[range]] Creating Dataset With Single Long Column -- `range` methods

[source, scala]
----
range(end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long]
----

`range` family of methods create a link:spark-sql-dataset.adoc[Dataset] of `Long` numbers.

[source, scala]
----
scala> spark.range(start = 0, end = 4, step = 2, numPartitions = 5).show
+---+
| id|
+---+
|  0|
|  2|
+---+
----

NOTE: The three first variants (that do not specify `numPartitions` explicitly) use link:spark-sparkcontext.adoc#defaultParallelism[SparkContext.defaultParallelism] for the number of partitions `numPartitions`.

Internally, `range` creates a new `Dataset[Long]` with `Range` link:spark-sql-LogicalPlan.adoc[logical plan] and `Encoders.LONG` link:spark-sql-Encoder.adoc[encoder].

=== [[emptyDataFrame]]  Creating Empty DataFrame --  `emptyDataFrame` method

[source, scala]
----
emptyDataFrame: DataFrame
----

`emptyDataFrame` creates an empty `DataFrame` (with no rows and columns).

It calls <<createDataFrame, createDataFrame>> with an empty `RDD[Row]` and an empty schema link:spark-sql-StructType.adoc[StructType(Nil)].

=== [[createDataFrame]] Creating DataFrames from RDDs with Explicit Schema -- `createDataFrame` method

[source, scala]
----
createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame
----

`createDataFrame` creates a `DataFrame` using `RDD[Row]` and the input `schema`. It is assumed that the rows in `rowRDD` all match the `schema`.

=== [[sql]] Executing SQL Queries -- `sql` method

[source, scala]
----
sql(sqlText: String): DataFrame
----

`sql` executes the `sqlText` SQL statement.

```
scala> sql("SHOW TABLES")
res0: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]

scala> sql("DROP TABLE IF EXISTS testData")
res1: org.apache.spark.sql.DataFrame = []

// Let's create a table to SHOW it
spark.range(10).write.option("path", "/tmp/test").saveAsTable("testData")

scala> sql("SHOW TABLES").show
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
| testdata|      false|
+---------+-----------+
```

Internally, it creates a link:spark-sql-dataset.adoc[Dataset] using the current `SparkSession` and a link:spark-sql-LogicalPlan.adoc[logical plan]. The plan is created by parsing the input `sqlText` using <<sessionState, sessionState.sqlParser>>.

CAUTION: FIXME See link:spark-sql-sqlcontext.adoc#sql[Executing SQL Queries].

=== [[udf]] Accessing UDF Registration Interface -- `udf` Attribute

[source, scala]
----
udf: UDFRegistration
----

`udf` attribute gives access to `UDFRegistration` that allows registering link:spark-sql-udfs.adoc[user-defined functions] for SQL-based query expressions.

[source, scala]
----
val spark: SparkSession = ...
spark.udf.register("myUpper", (s: String) => s.toUpperCase)

val strs = ('a' to 'c').map(_.toString).toDS
strs.registerTempTable("strs")

scala> sql("SELECT *, myUpper(value) UPPER FROM strs").show
+-----+-----+
|value|UPPER|
+-----+-----+
|    a|    A|
|    b|    B|
|    c|    C|
+-----+-----+
----

Internally, it is an alias for link:spark-sql-sessionstate.adoc#udf[SessionState.udf].

=== [[table]] Creating DataFrames from Tables -- `table` method

[source, scala]
----
table(tableName: String): DataFrame
----

`table` creates a link:spark-sql-dataframe.adoc[DataFrame] from records in the `tableName` table (if exists).

[source, scala]
----
val df = spark.table("mytable")
----

=== [[catalog]] Accessing Metastore -- `catalog` Attribute

[source, scala]
----
catalog: Catalog
----

`catalog` attribute is a (lazy) interface to the current metastore, i.e. link:spark-sql-Catalog.adoc[data catalog] (of relational entities like databases, tables, functions, table columns, and temporary views).

TIP: All methods in `Catalog` return `Datasets`.

[source, scala]
----
scala> spark.catalog.listTables.show
+------------------+--------+-----------+---------+-----------+
|              name|database|description|tableType|isTemporary|
+------------------+--------+-----------+---------+-----------+
|my_permanent_table| default|       null|  MANAGED|      false|
|              strs|    null|       null|TEMPORARY|       true|
+------------------+--------+-----------+---------+-----------+
----

Internally, `catalog` creates a link:spark-sql-Catalog.adoc#CatalogImpl[CatalogImpl] (referencing the current `SparkSession`).

=== [[read]] Accessing DataFrameReader -- `read` method

[source, scala]
----
read: DataFrameReader
----

`read` method returns a link:spark-sql-dataframereader.adoc[DataFrameReader] that is used to read data from external storage systems and load it into a `DataFrame`.

[source, scala]
----
val spark: SparkSession = // create instance
val dfReader: DataFrameReader = spark.read
----

=== [[conf]] Runtime Configuration -- `conf` attribute

[source, scala]
----
conf: RuntimeConfig
----

`conf` returns the current runtime configuration (as `RuntimeConfig`) that wraps link:spark-sql-SQLConf.adoc[SQLConf].

CAUTION: FIXME

=== [[sessionState]] `sessionState` Property

`sessionState` is a transient lazy value that represents the current link:spark-sql-sessionstate.adoc[SessionState].

NOTE: `sessionState` is a `private[sql]` value so you can only access it in a code inside `org.apache.spark.sql` package.

`sessionState` is a lazily-created value based on the internal link:spark-sql-settings.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] setting that can be:

* `org.apache.spark.sql.hive.HiveSessionState` for `hive`
* `org.apache.spark.sql.internal.SessionState` for `in-memory`

=== [[readStream]] `readStream` method

[source, scala]
----
readStream: DataStreamReader
----

`readStream` returns a new link:spark-sql-streaming-DataStreamReader.adoc[DataStreamReader].

=== [[streams]] `streams` Attribute

[source, scala]
----
streams: StreamingQueryManager
----

`streams` attribute gives access to link:spark-sql-streaming-StreamingQueryManager.adoc[StreamingQueryManager] (through link:spark-sql-sessionstate.adoc#streamingQueryManager[SessionState]).

[source, scala]
----
val spark: SparkSession = ...
spark.streams.active.foreach(println)
----

=== [[streamingQueryManager]] `streamingQueryManager` Attribute

`streamingQueryManager` is...

=== [[listenerManager]] `listenerManager` Attribute

`listenerManager` is...

=== [[ExecutionListenerManager]] `ExecutionListenerManager`

`ExecutionListenerManager` is...

=== [[functionRegistry]] `functionRegistry` Attribute

`functionRegistry` is...

=== [[experimentalMethods]] `experimentalMethods` Attribute

[source, scala]
----
experimental: ExperimentalMethods
----

`experimentalMethods` is an extension point with `ExperimentalMethods` that is a per-session collection of extra strategies and ``Rule[LogicalPlan]``s.

NOTE: `experimental` is used in link:spark-sql-SparkPlanner.adoc[SparkPlanner] and link:spark-sql-catalyst-Optimizer.adoc#SparkOptimizer[SparkOptimizer]. Hive and link:spark-structured-streaming.adoc[Structured Streaming] use it for their own extra strategies and optimization rules.

=== [[newSession]] `newSession` method

[source, scala]
----
newSession(): SparkSession
----

`newSession` creates (starts) a new `SparkSession` (with the current link:spark-sparkcontext.adoc[SparkContext] and <<SharedState, SharedState>>).

[source, scala]
----
scala> println(sc.version)
2.0.0-SNAPSHOT

scala> val newSession = spark.newSession
newSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@122f58a
----

=== [[sharedState]] `sharedState` Attribute

`sharedState` is the current <<SharedState, SharedState>>. It is created lazily when first accessed.

=== [[SharedState]] `SharedState`

`SharedState` is an internal class that holds the shared state across active SQL sessions (as <<SparkSession, SparkSession>> instances) by sharing link:spark-sql-CacheManager.adoc[CacheManager], link:spark-webui-SQLListener.adoc[SQLListener], and link:spark-sql-ExternalCatalog.adoc[ExternalCatalog].

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.sql.internal.SharedState` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.internal.SharedState=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

`SharedState` requires a link:spark-sparkcontext.adoc[SparkContext] when created. It also adds `hive-site.xml` to link:spark-sparkcontext.adoc#hadoopConfiguration[Hadoop's `Configuration` in the current SparkContext] if found on CLASSPATH.

NOTE: `hive-site.xml` is an optional Hive configuration file when working with Hive in Spark.

The fully-qualified class name is `org.apache.spark.sql.internal.SharedState`.

`SharedState` is created lazily, i.e. when first accessed after <<creating-instance, `SparkSession` is created>>. It can happen when a <<newSession, new session is created>> or when the shared services are accessed. It is created with a link:spark-sparkcontext.adoc[SparkContext].

When created, `SharedState` sets `hive.metastore.warehouse.dir` to link:spark-sql-settings.adoc#spark_sql_warehouse_dir[spark.sql.warehouse.dir] if `hive.metastore.warehouse.dir` is not set or `spark.sql.warehouse.dir` is set. Otherwise, when `hive.metastore.warehouse.dir` is set and `spark.sql.warehouse.dir` is not, `spark.sql.warehouse.dir` gets set to `hive.metastore.warehouse.dir`. You should see the following INFO message in the logs:

```
INFO spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('[hiveWarehouseDir]').
```

You should see the following INFO message in the logs:

```
INFO SharedState: Warehouse path is '[warehousePath]'.
```

=== [[stop]] Stopping SparkSession -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` stops the `SparkSession`, i.e. link:spark-sparkcontext.adoc#stop[stops the underlying `SparkContext`].

=== [[creating-instance]] Creating `SparkSession` Instance

CAUTION: FIXME

=== [[baseRelationToDataFrame]] `baseRelationToDataFrame` Method

CAUTION: FIXME
