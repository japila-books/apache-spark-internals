== [[EventLoggingListener]] EventLoggingListener -- Spark Listener for Persisting Events

`EventLoggingListener` is a link:spark-SparkListener.adoc[SparkListener] that <<logEvent, persists JSON-encoded events>> to a file.

When <<spark_eventLog_enabled, event logging is enabled>>, `EventLoggingListener` writes events to a log file under <<spark_eventLog_dir, spark.eventLog.dir>> directory. All link:spark-SparkListener.adoc[Spark events] are logged (except  link:spark-SparkListener.adoc#SparkListenerBlockUpdated[SparkListenerBlockUpdated] and link:spark-SparkListener.adoc#SparkListenerExecutorMetricsUpdate[SparkListenerExecutorMetricsUpdate]).

TIP: Use link:spark-history-server.adoc[Spark History Server] to view the event logs in a browser.

Events can optionally be <<compressing-events, compressed>>.

In-flight log files are with `.inprogress` extension.

`EventLoggingListener` is a `private[spark]` class in `org.apache.spark.scheduler` package.

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.scheduler.EventLoggingListener` logger to see what happens inside `EventLoggingListener`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.EventLoggingListener=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating EventLoggingListener Instance

`EventLoggingListener` requires an application id (`appId`), the application's optional attempt id (`appAttemptId`), `logBaseDir`, a link:spark-SparkConf.adoc[SparkConf] (as `sparkConf`) and Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[Configuration] (as `hadoopConf`).

NOTE: When initialized with no Hadoop's `Configuration` it calls link:spark-SparkHadoopUtil.adoc#newConfiguration[SparkHadoopUtil.get.newConfiguration(sparkConf)].

=== [[start]] Starting EventLoggingListener -- `start` method

[source, scala]
----
start(): Unit
----

`start` checks whether `logBaseDir` is really a directory, and if it is not, it throws a `IllegalArgumentException` with the following message:

```
Log directory [logBaseDir] does not exist.
```

The log file's working name is created based on `appId` with or without the compression codec used and `appAttemptId`, i.e. `local-1461696754069`. It also uses `.inprogress` extension.

If <<spark_eventLog_overwrite, overwrite is enabled>>, you should see the WARN message:

```
WARN EventLoggingListener: Event log [path] already exists. Overwriting...
```

The working log `.inprogress` is attempted to be deleted. In case it could not be deleted, the following WARN message is printed out to the logs:

```
WARN EventLoggingListener: Error deleting [path]
```

The buffered output stream is created with metadata with Spark's version and `SparkListenerLogStart` class' name as the first line.

```
{"Event":"SparkListenerLogStart","Spark Version":"2.0.0-SNAPSHOT"}
```

At this point, `EventLoggingListener` is ready for event logging and you should see the following INFO message in the logs:

```
INFO EventLoggingListener: Logging events to [logPath]
```

NOTE: `start` is executed while link:spark-sparkcontext-creating-instance-internals.adoc#_eventLogger[`SparkContext` is created].

=== [[logEvent]] Logging Event as JSON -- `logEvent` method

[source, scala]
----
logEvent(event: SparkListenerEvent, flushLogger: Boolean = false)
----

`logEvent` logs `event` as JSON.

CAUTION: FIXME

=== [[stop]] Stopping EventLoggingListener -- `stop` method

[source, scala]
----
stop(): Unit
----

`stop` closes `PrintWriter` for the log file and renames the file to be without `.inprogress` extension.

If the target log file exists (one without `.inprogress` extension), it overwrites the file if <<spark_eventLog_overwrite, spark.eventLog.overwrite>> is enabled. You should see the following WARN message in the logs:

```
WARN EventLoggingListener: Event log [target] already exists. Overwriting...
```

If the target log file exists and overwrite is disabled, an `java.io.IOException` is thrown with the following message:

```
Target log file already exists ([logPath])
```

NOTE: `stop` is executed while link:spark-sparkcontext.adoc#stop[`SparkContext` is stopped].

=== [[compressing-events]] Compressing Logged Events

If <<spark_eventLog_compress, event compression is enabled>>, events are compressed using link:spark-CompressionCodec.adoc#createCodec[CompressionCodec].

TIP: Refer to link:spark-CompressionCodec.adoc[CompressionCodec] to learn about the available compression codecs.

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark_eventLog_enabled]] `spark.eventLog.enabled`
| `false`
| Enables (`true`) or disables (`false`) persisting Spark events.

| [[spark_eventLog_dir]] `spark.eventLog.dir`
| `/tmp/spark-events`
| Directory where events are logged, e.g. `hdfs://namenode:8021/directory`.

The directory must exist before link:spark-sparkcontext-creating-instance-internals.adoc#_eventLogger[Spark starts up].

| [[spark_eventLog_buffer_kb]] `spark.eventLog.buffer.kb`
| `100`
| Size of the buffer to use when writing to output streams.

| [[spark_eventLog_overwrite]] `spark.eventLog.overwrite`
| `false`
| Enables (`true`) or disables (`false`) deleting (or at least overwriting) an existing `.inprogress` log file.

| [[spark_eventLog_compress]] `spark.eventLog.compress`
| `false`
| Enables (`true`) or disables (`false`) <<compressing-events, event compression>>.

| [[spark_eventLog_testing]] `spark.eventLog.testing`
| `false`
| Internal flag for testing purposes that enables adding JSON events to the internal `loggedEvents` array.
|===
