== [[Task]] Tasks

In Spark, a *Task* (aka _command_) is the smallest individual unit of execution that corresponds to a link:spark-rdd-Partition.adoc[RDD partition]. Tasks are link:spark-executor.adoc#launchTask[launched on executors].

.Tasks correspond to partitions in RDD
image::images/spark-rdd-partitions-job-stage-tasks.png[align="center"]

In other (more technical) words, a task is a computation on a data partition in a stage of a RDD in a Spark job.

[[contract]]
The `Task` contract expects that custom tasks define <<runTask, runTask>> method.

[source, scala]
----
runTask(context: TaskContext): T
----

NOTE: `T` is the type defined when a <<creating-instance, `Task` is created>>.

[[internal-registries]]
.`Task` Internal Registries and Counters
[frame="topbot",cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[metrics]] `metrics`
| Used when ???

| [[taskMemoryManager]] `taskMemoryManager`
| link:spark-taskscheduler-taskmemorymanager.adoc[TaskMemoryManager] that manages the memory allocated by the task.

Used when ???

| [[context]] `context`
| Used when ???

| [[_killed]] `_killed`
| Used when ???

| [[_executorDeserializeTime]] `_executorDeserializeTime`
| Used when ???

| [[_executorDeserializeCpuTime]] `_executorDeserializeCpuTime`
| Used when ???

| [[taskThread]] `taskThread`
| Used when ???

| [[epoch]] `epoch`
| Set for a `Task` when link:spark-tasksetmanager.adoc#creating-instance[`TaskSetManager` is created] and later used when link:spark-executor-taskrunner.adoc#run[`TaskRunner` runs] and when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ShuffleMapTask[`DAGScheduler` handles a `ShuffleMapTask` successful completion].

|===

A task can only belong to one stage and operate on a single partition. All tasks in a stage must be completed before the stages that follow can start.

Tasks are spawned one by one for each stage and partition.

CAUTION: FIXME What are `stageAttemptId` and `taskAttemptId`?

A task in Spark is represented by the <<contract, `Task` abstract class>> with two concrete implementations:

* link:spark-taskscheduler-ShuffleMapTask.adoc[ShuffleMapTask] that executes a task and divides the task's output to multiple buckets (based on the task's partitioner).
* link:spark-taskscheduler-ResultTask.adoc[ResultTask] that executes a task and sends the task's output back to the driver application.

The very last stage in a Spark job consists of multiple link:spark-taskscheduler-ResultTask.adoc[ResultTask]s, while earlier stages can only be link:spark-taskscheduler-ShuffleMapTask.adoc[ShuffleMapTask]s.

CAUTION: FIXME You could have a Spark job with link:spark-taskscheduler-ShuffleMapTask.adoc[ShuffleMapTask] being the last.

=== [[creating-instance]] Creating `Task` Instance

[source, scala]
----
Task[T](
  val stageId: Int,
  val stageAttemptId: Int,
  val partitionId: Int,
  var localProperties: Properties = new Properties,
  serializedTaskMetrics: Array[Byte] =
    SparkEnv.get.closureSerializer.newInstance().serialize(TaskMetrics.registered).array(),
  val jobId: Option[Int] = None,
  val appId: Option[String] = None,
  val appAttemptId: Option[String] = None)
extends Serializable
----

=== [[attributes]] Task Attributes

A `Task` instance is uniquely identified by the following task attributes:

* `stageId` - there can be many stages in a job. Every stage has its own unique `stageId` that the task belongs to.

* `stageAttemptId` - a stage can be re-attempted for execution in case of failure. `stageAttemptId` represents the attempt id of a stage that the task belongs to.

* `partitionId` - a task is a unit of work on a partitioned distributed dataset. Every partition has its own unique `partitionId` that a task processes.

* `metrics` - an instance of link:spark-taskscheduler-taskmetrics.adoc[TaskMetrics] for the task.

* `localProperties` - local private properties of the task.

=== [[run]][[execution]] Running Task Thread -- `run` Method

[source, scala]
----
run(
  taskAttemptId: Long,
  attemptNumber: Int,
  metricsSystem: MetricsSystem): T
----

`run` link:spark-blockmanager.adoc#registerTask[registers task attempt id to the executor's BlockManager] and link:spark-taskscheduler-taskcontext.adoc#creating-instance[creates a `TaskContextImpl`] that in turn gets set as the thread local link:spark-taskscheduler-taskcontext.adoc[TaskContext].

If the task has been killed before the task runs it is <<kill, killed>> (with `interruptThread` flag disabled).

The <<runTask, task runs>>.

CAUTION: FIXME Describe `catch` and `finally` blocks.

NOTE: When `run` is called from link:spark-executor-taskrunner.adoc#run[TaskRunner.run], the `Task` has just been deserialized from `taskBytes` that were sent over the wire to an executor. `localProperties` and link:spark-taskscheduler-taskmemorymanager.adoc[TaskMemoryManager] are already assigned.

=== [[states]] Task States

A task can be in one of the following states:

* `LAUNCHING`
* `RUNNING` when the task is being started.
* `FINISHED` when the task finished with the serialized result.
* `FAILED` when the task fails, e.g. when `FetchFailedException` (see link:spark-executor.adoc#FetchFailedException[FetchFailedException]), `CommitDeniedException` or any `Throwable` occur
* `KILLED` when an executor kills a task.
* `LOST`

States are the values of `org.apache.spark.TaskState`.

NOTE: Task status updates are sent from executors to the driver through link:spark-executor-backends.adoc[ExecutorBackend].

Task is finished when it is in one of `FINISHED`, `FAILED`, `KILLED`, `LOST`

`LOST` and `FAILED` states are considered failures.

TIP: Task states correspond to https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto[org.apache.mesos.Protos.TaskState].

=== [[collectAccumulatorUpdates]] Collect Latest Values of Accumulators -- `collectAccumulatorUpdates` Method

[source, scala]
----
collectAccumulatorUpdates(taskFailed: Boolean = false): Seq[AccumulableInfo]
----

`collectAccumulatorUpdates` collects the latest values of accumulators used in a task (and returns the values as a collection of link:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo]).

NOTE: It is used in link:spark-executor-taskrunner.adoc[TaskRunner] to send a task's final results with the latest values of accumulators used.

When `taskFailed` is `true` it filters out link:spark-accumulators.adoc[accumulators] with `countFailedValues` disabled.

CAUTION: FIXME Why is the check `context != null`?

NOTE: It uses `context.taskMetrics.accumulatorUpdates()`.

CAUTION: FIXME What is `context.taskMetrics.accumulatorUpdates()` doing?

=== [[kill]] Killing Task -- `kill` Method

[source, scala]
----
kill(interruptThread: Boolean)
----

`kill` marks the task to be killed, i.e. it sets the internal `_killed` flag to `true`.

It calls link:spark-taskscheduler-taskcontext.adoc#markInterrupted[TaskContextImpl.markInterrupted] when `context` is set.

If `interruptThread` is enabled and the internal `taskThread` is available, `kill` interrupts it.

CAUTION: FIXME When could `context` and `interruptThread` not be set?
