== [[RunnableCommand]] RunnableCommand -- Generic Logical Command with Side Effects

`RunnableCommand` is the generic link:spark-sql-LogicalPlan.adoc#Command[logical command] that is <<run, executed>> for its side effects.

[[contract]]
[[run]]
`RunnableCommand` defines one abstract method `run` that computes a collection of link:spark-sql-Row.adoc[Row] records with the side effect, i.e. the result of executing a command.

[source, scala]
----
run(sparkSession: SparkSession): Seq[Row]
----

NOTE: `RunnableCommand` logical operator is translated to link:spark-sql-SparkPlan-ExecutedCommandExec.adoc[ExecutedCommandExec] physical operator in link:spark-sql-SparkStrategy-BasicOperators.adoc#RunnableCommand[BasicOperators] strategy.

[NOTE]
====
`run` is executed when:

* `ExecutedCommandExec` link:spark-sql-SparkPlan-ExecutedCommandExec.adoc#sideEffectResult[executes logical RunnableCommand and caches the result as InternalRows]
* `InsertIntoHadoopFsRelationCommand` runs
* `QueryExecution` link:spark-sql-QueryExecution.adoc#hiveResultString[transforms the result of executing `DescribeTableCommand` to a Hive-compatible output format]
====

[[available-commands]]
.Available RunnableCommands (in alphabetical order)
[width="100%",cols="1,2",options="header"]
|===
| RunnableCommand
| Description

| AddFileCommand
|

| AddJarCommand
|

| AlterDatabasePropertiesCommand
|

| AlterTableAddPartitionCommand
|

| AlterTableChangeColumnCommand
|

| AlterTableDropPartitionCommand
|

| AlterTableRecoverPartitionsCommand
|

| AlterTableRenameCommand
|

| AlterTableRenamePartitionCommand
|

| AlterTableSerDePropertiesCommand
|

| AlterTableSetLocationCommand
|

| AlterTableSetPropertiesCommand
|

| AlterTableUnsetPropertiesCommand
|

| AlterViewAsCommand
|

| AnalyzeColumnCommand
|

| AnalyzeTableCommand
|

| [[CacheTableCommand]] CacheTableCommand
a| When <<run, executed>>, `CacheTableCommand` link:spark-sql-Dataset.adoc#ofRows[creates a DataFrame] followed by link:spark-sql-dataset-operators.adoc#createTempView[registering a temporary view] for the optional `query`.

[source, scala]
----
CACHE LAZY? TABLE [table] (AS? [query])?
----

`CacheTableCommand` requests the session-specific `Catalog` to link:spark-sql-Catalog.adoc#cacheTable[cache the table].

NOTE: `CacheTableCommand` uses `SparkSession` link:spark-sql-SparkSession.adoc#catalog[to access the `Catalog`].

If the caching is not `LAZY` (which is not by default), `CacheTableCommand` link:spark-sql-SparkSession.adoc#table[creates a DataFrame for the table] and link:spark-sql-dataset-operators.adoc#count[counts the rows] (that will trigger the caching).

NOTE: `CacheTableCommand` uses a Spark SQL pattern to trigger DataFrame caching by executing `count` operation.

[source, scala]
----
val q = "CACHE TABLE ids AS SELECT * from range(5)"
scala> println(sql(q).queryExecution.logical.numberedTreeString)
00 CacheTableCommand `ids`, false
01    +- 'Project [*]
02       +- 'UnresolvedTableValuedFunction range, [5]

// ids table is already cached but let's use it anyway (and see what happens)
val q2 = "CACHE LAZY TABLE ids"
scala> println(sql(q2).queryExecution.logical.numberedTreeString)
17/05/17 06:16:39 WARN CacheManager: Asked to cache already cached data.
00 CacheTableCommand `ids`, true
----

| ClearCacheCommand
|

| CreateDatabaseCommand
|

| [[CreateDataSourceTableAsSelectCommand]] `CreateDataSourceTableAsSelectCommand`
| When <<run, executed>>, ...FIXME

Used exclusively when link:spark-sql-SessionState.adoc#DataSourceAnalysis[DataSourceAnalysis] evaluation rule resolves `CreateTable` logical operator with queries using non-Hive table providers (which is when `DataFrameWriter` link:spark-sql-DataFrameWriter.adoc#saveAsTable[saves a DataFrame to a non-Hive table] or for link:spark-sql-SparkSqlAstBuilder.adoc#visitCreateTable[Create Table As Select] SQL statements)

| link:spark-sql-LogicalPlan-RunnableCommand-CreateDataSourceTableCommand.adoc[CreateDataSourceTableCommand]
|

| CreateFunctionCommand
|

| CreateHiveTableAsSelectCommand
|

| CreateTableCommand
|

| CreateTableLikeCommand
|

| CreateTempViewUsing
|

| CreateViewCommand
|

| DescribeDatabaseCommand
|

| DescribeFunctionCommand
|

| [[DescribeTableCommand]] DescribeTableCommand
|

| DropDatabaseCommand
|

| DropFunctionCommand
|

| DropTableCommand
|

| ExplainCommand
|

| InsertIntoDataSourceCommand
|

| InsertIntoHadoopFsRelationCommand
|

| InsertIntoHiveTable
|

| ListFilesCommand
|

| ListJarsCommand
|

| LoadDataCommand
|

| RefreshResource
|

| RefreshTable
|

| ResetCommand
|

| [[SaveIntoDataSourceCommand]] `SaveIntoDataSourceCommand`
| When <<run, executed>>, requests `DataSource` to link:spark-sql-DataSource.adoc#write[write a DataFrame to a data source per save mode].

Used exclusively when `DataFrameWriter` is requested to link:spark-sql-DataFrameWriter.adoc#save[save a DataFrame to a data source].

| [[SetCommand]] SetCommand
|

| SetDatabaseCommand
|

| ShowColumnsCommand
|

| ShowCreateTableCommand
|

| ShowDatabasesCommand
|

| ShowFunctionsCommand
|

| ShowPartitionsCommand
|

| ShowTablePropertiesCommand
|

| ShowTablesCommand
|

| StreamingExplainCommand
|

| TruncateTableCommand
|

| UncacheTableCommand
|
|===
