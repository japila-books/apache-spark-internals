== [[AstBuilder]] AstBuilder -- ANTLR-based SQL Parser

`AstBuilder` converts a SQL string into Spark SQL's corresponding entity (i.e. link:spark-sql-DataType.adoc[DataType], link:spark-sql-Expression.adoc[Expression], link:spark-sql-LogicalPlan.adoc[LogicalPlan] or `TableIdentifier`) using <<visit-callbacks, visit callback methods>>.

`AstBuilder` is the link:spark-sql-AbstractSqlParser.adoc#astBuilder[AST builder] of `AbstractSqlParser` (i.e. the base SQL parsing infrastructure in Spark SQL).

[TIP]
====
Spark SQL supports SQL queries as described in https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4[SqlBase.g4]. Using the file can tell you (almost) exactly what Spark SQL supports at any given time.

"Almost" being that although the grammar accepts a SQL statement it can be reported as not allowed by `AstBuilder`, e.g.

```
scala> sql("EXPLAIN FORMATTED SELECT * FROM myTable").show
org.apache.spark.sql.catalyst.parser.ParseException:
Operation not allowed: EXPLAIN FORMATTED(line 1, pos 0)

== SQL ==
EXPLAIN FORMATTED SELECT * FROM myTable
^^^

  at org.apache.spark.sql.catalyst.parser.ParserUtils$.operationNotAllowed(ParserUtils.scala:39)
  at org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitExplain$1.apply(SparkSqlParser.scala:275)
  at org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitExplain$1.apply(SparkSqlParser.scala:273)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:93)
  at org.apache.spark.sql.execution.SparkSqlAstBuilder.visitExplain(SparkSqlParser.scala:273)
  at org.apache.spark.sql.execution.SparkSqlAstBuilder.visitExplain(SparkSqlParser.scala:53)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ExplainContext.accept(SqlBaseParser.java:480)
  at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:42)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:66)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:66)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:93)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:65)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:62)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:61)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:90)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:46)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:61)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  ... 48 elided
```
====

[NOTE]
====
Technically, `AstBuilder` is a ANTLR `AbstractParseTreeVisitor` (as `SqlBaseBaseVisitor`) that is generated from https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4[SqlBase.g4] ANTLR grammar for Spark SQL.

`SqlBaseBaseVisitor` is a ANTLR-specific base class that is auto-generated at build time from a ANTLR grammar in `SqlBase.g4`.

`SqlBaseBaseVisitor` is an ANTLR http://www.antlr.org/api/Java/org/antlr/v4/runtime/tree/AbstractParseTreeVisitor.html[AbstractParseTreeVisitor].
====

[[visit-callbacks]]
.AstBuilder's Visit Callback Methods (in alphabetical order)
[cols="1,1,3",options="header",width="100%"]
|===
| Callback Method
| ANTLR rule / labeled alternative
| Spark SQL Entity

| [[visitDescribeTable]] `visitDescribeTable`
| `describeTable`
a| link:spark-sql-LogicalPlan-RunnableCommand.adoc#DescribeTableCommand[DescribeTableCommand] logical command for `DESCRIBE TABLE`

| [[visitExplain]] `visitExplain`
| `explain`
a| link:spark-sql-LogicalPlan-ExplainCommand.adoc[ExplainCommand]

[NOTE]
====
Can be a `OneRowRelation` for a `EXPLAIN` for unexplainable link:spark-sql-LogicalPlan-RunnableCommand.adoc#DescribeTableCommand[DescribeTableCommand] logical command as created from SQL's <<visitDescribeTable, DESCRIBE TABLE>>.

```
val q = sql("EXPLAIN DESCRIBE TABLE t")
scala> println(q.queryExecution.logical.numberedTreeString)
scala> println(q.queryExecution.logical.numberedTreeString)
00 ExplainCommand OneRowRelation$, false, false, false
```
====

| [[visitFromClause]] `visitFromClause`
| `fromClause`
a| link:spark-sql-LogicalPlan.adoc[LogicalPlan]

Supports multiple comma-separated relations (that all together build an condition-less INNER JOIN) with optional link:spark-sql-Expression-Generator.adoc#lateral-view[LATERAL VIEW].

A relation can be one of the following or a combination thereof:

* Table identifier
* Inline table using `VALUES exprs AS tableIdent`
* Table-valued function (currently only `range` is supported)

| [[visitFunctionCall]] `visitFunctionCall`
| `functionCall` labeled alternative
a|

* `UnresolvedFunction` for a bare function (with no window specification)
* [[visitFunctionCall-UnresolvedWindowExpression]] link:spark-sql-Expression-WindowExpression.adoc#UnresolvedWindowExpression[UnresolvedWindowExpression] for a function evaluated in a windowed context with a `WindowSpecReference`
* link:spark-sql-Expression-WindowExpression.adoc[WindowExpression] for a function over a window

TIP: See the <<function-examples, function examples>> below.

| [[visitNamedExpression]] `visitNamedExpression`
| `namedExpression`
a|

* `Alias` (for a single alias)
* `MultiAlias` (for a parenthesis enclosed alias list
* a bare link:spark-sql-Expression.adoc[Expression]

| [[visitRelation]] `visitRelation`
| `relation`
| link:spark-sql-LogicalPlan.adoc[LogicalPlan] for a `FROM` clause.

| [[visitSingleDataType]] `visitSingleDataType`
| `singleDataType`
| link:spark-sql-DataType.adoc[DataType]

| [[visitSingleExpression]] `visitSingleExpression`
| `singleExpression`
| link:spark-sql-Expression.adoc[Expression]

Takes the named expression and relays to <<visitNamedExpression, visitNamedExpression>>

| [[visitSingleStatement]] `visitSingleStatement`
| `singleStatement`
a| link:spark-sql-LogicalPlan.adoc[LogicalPlan] from a single statement

NOTE: A single statement can be quite involved.

| [[visitSingleTableIdentifier]] `visitSingleTableIdentifier`
| `singleTableIdentifier`
| `TableIdentifier`

| [[visitWindowDef]] `visitWindowDef`
| `windowDef` labeled alternative
a| link:spark-sql-Expression-WindowSpecDefinition.adoc[WindowSpecDefinition]

```
// CLUSTER BY with window frame
'(' CLUSTER BY partition+=expression (',' partition+=expression)*) windowFrame? ')'

// PARTITION BY and ORDER BY with window frame
'(' ((PARTITION \| DISTRIBUTE) BY partition+=expression (',' partition+=expression)*)?
  ((ORDER \| SORT) BY sortItem (',' sortItem)*)?)
  windowFrame? ')'
```

| [[visitQuerySpecification]] `visitQuerySpecification`
| `querySpecification`
a| link:spark-sql-LogicalPlan.adoc[LogicalPlan]

[NOTE]
====
Can be a `OneRowRelation` for a `SELECT` without a `FROM` clause.

```
val q = sql("select 1")
scala> println(q.queryExecution.logical.numberedTreeString)
00 'Project [unresolvedalias(1, None)]
01 +- OneRowRelation$
```
====
|===

[[with-methods]]
.AstBuilder's Parsing Handlers (in alphabetical order)
[cols="1,3",options="header",width="100%"]
|===
| Parsing Handler
| LogicalPlan Added

| [[withAggregation]] `withAggregation`
a|

* link:spark-sql-LogicalPlan-GroupingSets.adoc[GroupingSets] for `GROUP BY &hellip; GROUPING SETS (&hellip;)`

* link:spark-sql-LogicalPlan-Aggregate.adoc[Aggregate] for `GROUP BY &hellip; (WITH CUBE \| WITH ROLLUP)?`

| [[withGenerate]] `withGenerate`
| link:spark-sql-Expression-Generator.adoc[Generate] with link:spark-sql-Expression-Generator.adoc#UnresolvedGenerator[UnresolvedGenerator] and `join` enabled

| [[withHints]] `withHints`
a| link:spark-sql-LogicalPlan-Hint.adoc[Hint] for `/*+ hint */` in `SELECT`.

TIP: Note `+` (plus) between `/\*` and `*/`

`hint` is of the format `name` or `name (params)` with `name` as `BROADCAST`, `BROADCASTJOIN` or `MAPJOIN`.

```
/*+ BROADCAST (table) */
```

| [[withJoinRelations]] `withJoinRelations`
a| link:spark-sql-LogicalPlan-Join.adoc[Join] for a <<visitFromClause, FROM clause>> and <<visitRelation, relation>> alone.

The following join types are supported:

* `INNER` (default)
* `CROSS`
* `LEFT` (with optional `OUTER`)
* `LEFT SEMI`
* `RIGHT` (with optional `OUTER`)
* `FULL` (with optional `OUTER`)
* `ANTI` (optionally prefixed with `LEFT`)

The following join criteria are supported:

* `ON booleanExpression`
* `USING '(' identifier (',' identifier)* ')'`

Joins can be `NATURAL` (with no join criteria).

| [[withQueryResultClauses]] `withQueryResultClauses`
|

| [[withQuerySpecification]] `withQuerySpecification`
|

| [[withWindows]] `withWindows`
a| link:spark-sql-LogicalPlan-WithWindowDefinition.adoc[WithWindowDefinition] for link:spark-sql-functions-windows.adoc[window aggregates] (given `WINDOW` definitions).

Used for <<withQueryResultClauses, withQueryResultClauses>> and <<withQuerySpecification, withQuerySpecification>> with `windows` definition.

```
WINDOW identifier AS windowSpec
  (',' identifier AS windowSpec)*
```

TIP: Consult `windows`, `namedWindow`, `windowSpec`, `windowFrame`, and `frameBound` (with `windowRef` and `windowDef`) ANTLR parsing rules for Spark SQL in link:++https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4#L629++[SqlBase.g4].
|===

NOTE: `AstBuilder` belongs to `org.apache.spark.sql.catalyst.parser` package.

=== [[function-examples]] Function Examples

The examples are handled by <<visitFunctionCall, visitFunctionCall>>.

[source, scala]
----
import spark.sessionState.sqlParser

scala> sqlParser.parseExpression("foo()")
res0: org.apache.spark.sql.catalyst.expressions.Expression = 'foo()

scala> sqlParser.parseExpression("foo() OVER windowSpecRef")
res1: org.apache.spark.sql.catalyst.expressions.Expression = unresolvedwindowexpression('foo(), WindowSpecReference(windowSpecRef))

scala> sqlParser.parseExpression("foo() OVER (CLUSTER BY field)")
res2: org.apache.spark.sql.catalyst.expressions.Expression = 'foo() windowspecdefinition('field, UnspecifiedFrame)
----
