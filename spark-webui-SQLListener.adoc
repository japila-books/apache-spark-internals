== [[SQLListener]] `SQLListener` Spark Listener

`SQLListener` is a custom link:spark-SparkListener.adoc[SparkListener] that collects information about SQL query executions for web UI (to display in link:spark-webui-sql.adoc[SQL tab]). It relies on link:spark-sql-SQLExecution.adoc#spark.sql.execution.id[spark.sql.execution.id] key to distinguish between queries.

Internally, it uses <<SQLExecutionUIData, SQLExecutionUIData>> data structure exclusively to record all the necessary data for a single SQL query execution. `SQLExecutionUIData` is tracked in the internal registries, i.e. `activeExecutions`, `failedExecutions`, and `completedExecutions` as well as lookup tables, i.e. `_executionIdToData`, `_jobIdToExecutionId`, and `_stageIdToStageMetrics`.

`SQLListener` starts recording a query execution by intercepting a <<SparkListenerSQLExecutionStart, SparkListenerSQLExecutionStart>> event (using <<onOtherEvent, onOtherEvent>> callback).

`SQLListener` stops recording information about a SQL query execution when <<SparkListenerSQLExecutionEnd, SparkListenerSQLExecutionEnd>> event arrives.

It defines the other callbacks (from link:spark-SparkListener.adoc[SparkListener] interface):

* <<onJobStart, onJobStart>>
* <<onJobEnd, onJobEnd>>
* onExecutorMetricsUpdate
* onStageSubmitted
* onTaskEnd

=== [[onJobStart]] Registering Job and Stages under Active Execution (onJobStart callback)

[source, scala]
----
onJobStart(jobStart: SparkListenerJobStart): Unit
----

`onJobStart` reads the link:spark-sql-SQLExecution.adoc#spark.sql.execution.id[`spark.sql.execution.id` key], the identifiers of the job and the stages and then updates the <<SQLExecutionUIData, SQLExecutionUIData>> for the execution id in `activeExecutions` internal registry.

NOTE: When `onJobStart` is executed, it is assumed that <<SQLExecutionUIData, SQLExecutionUIData>> has already been created and available in the internal `activeExecutions` registry.

The job in <<SQLExecutionUIData, SQLExecutionUIData>> is marked as running with the stages added (to `stages`). For each stage, a `SQLStageMetrics` is created in the internal `_stageIdToStageMetrics` registry. At the end, the execution id is recorded for the job id in the internal `_jobIdToExecutionId`.

=== [[onOtherEvent]] onOtherEvent

In `onOtherEvent`, `SQLListener` listens to the following link:spark-SparkListener.adoc#SparkListenerEvent[SparkListenerEvent] events:

* <<SparkListenerSQLExecutionStart, SparkListenerSQLExecutionStart>>
* <<SparkListenerSQLExecutionEnd, SparkListenerSQLExecutionEnd>>
* <<SparkListenerDriverAccumUpdates, SparkListenerDriverAccumUpdates>>

==== [[SparkListenerSQLExecutionStart]] Registering Active Execution (SparkListenerSQLExecutionStart Event)

[source, scala]
----
case class SparkListenerSQLExecutionStart(
  executionId: Long,
  description: String,
  details: String,
  physicalPlanDescription: String,
  sparkPlanInfo: SparkPlanInfo,
  time: Long)
extends SparkListenerEvent
----

`SparkListenerSQLExecutionStart` events starts recording information about the `executionId` SQL query execution.

When a `SparkListenerSQLExecutionStart` event arrives, a new <<SQLExecutionUIData, SQLExecutionUIData>> for the `executionId` query execution is created and stored in `activeExecutions` internal registry. It is also stored in `_executionIdToData` lookup table.

==== [[SparkListenerSQLExecutionEnd]] SparkListenerSQLExecutionEnd

[source, scala]
----
case class SparkListenerSQLExecutionEnd(
  executionId: Long,
  time: Long)
extends SparkListenerEvent
----

`SparkListenerSQLExecutionEnd` event stops recording information about the `executionId` SQL query execution (tracked as <<SQLExecutionUIData, SQLExecutionUIData>>). `SQLListener` saves the input `time` as `completionTime`.

If there are no other running jobs (registered in <<SQLExecutionUIData, SQLExecutionUIData>>), the query execution is removed from the `activeExecutions` internal registry and moved to either `completedExecutions` or `failedExecutions` registry.

This is when `SQLListener` checks the number of `SQLExecutionUIData` entires in either registry -- `failedExecutions` or `completedExecutions` -- and removes the excess of the old entries beyond <<spark.sql.ui.retainedExecutions, spark.sql.ui.retainedExecutions>>.

==== [[SparkListenerDriverAccumUpdates]] SparkListenerDriverAccumUpdates

[source, scala]
----
case class SparkListenerDriverAccumUpdates(
  executionId: Long,
  accumUpdates: Seq[(Long, Long)])
extends SparkListenerEvent
----

When `SparkListenerDriverAccumUpdates` comes, <<SQLExecutionUIData, SQLExecutionUIData>> for the input `executionId` is looked up (in `_executionIdToData`) and `SQLExecutionUIData.driverAccumUpdates` is updated with the input `accumUpdates`.

=== [[onJobEnd]] onJobEnd

[source, scala]
----
onJobEnd(jobEnd: SparkListenerJobEnd): Unit
----

When called, `onJobEnd` retrieves the <<SQLExecutionUIData, SQLExecutionUIData>> for the job and records it either successful or failed depending on the job result.

If it is the last job of the query execution (tracked as <<SQLExecutionUIData, SQLExecutionUIData>>), the execution is removed from `activeExecutions` internal registry and moved to either

If the query execution has already been marked as completed (using `completionTime`) and there are no other running jobs (registered in <<SQLExecutionUIData, SQLExecutionUIData>>), the query execution is removed from the `activeExecutions` internal registry and moved to either `completedExecutions` or `failedExecutions` registry.

This is when `SQLListener` checks the number of `SQLExecutionUIData` entires in either registry -- `failedExecutions` or `completedExecutions` -- and removes the excess of the old entries beyond <<spark.sql.ui.retainedExecutions, spark.sql.ui.retainedExecutions>>.

=== [[getExecution]] Getting SQL Execution Data (getExecution method)

[source, scala]
----
getExecution(executionId: Long): Option[SQLExecutionUIData]
----

=== [[getExecutionMetrics]] Getting Execution Metrics (getExecutionMetrics method)

[source, scala]
----
getExecutionMetrics(executionId: Long): Map[Long, String]
----

`getExecutionMetrics` gets the metrics (aka _accumulator updates_) for `executionId` (by which it collects all the tasks that were used for an execution).

It is exclusively used to render the link:spark-webui-sql.adoc#ExecutionPage[ExecutionPage] page in web UI.

=== [[mergeAccumulatorUpdates]] mergeAccumulatorUpdates method

`mergeAccumulatorUpdates` is a `private` helper method for...TK

It is used exclusively in <<getExecutionMetrics, getExecutionMetrics>> method.

=== [[SQLExecutionUIData]] SQLExecutionUIData

`SQLExecutionUIData` is the data abstraction of `SQLListener` to describe SQL query executions. It is a container for jobs, stages, and accumulator updates for a single query execution.

=== [[settings]] Settings

==== [[spark.sql.ui.retainedExecutions]] spark.sql.ui.retainedExecutions

`spark.sql.ui.retainedExecutions` (default: `1000`) is the number of `SQLExecutionUIData` entries to keep in `failedExecutions` and `completedExecutions` internal registries.

When a query execution finishes, the execution is removed from the internal `activeExecutions` registry and stored in `failedExecutions` or `completedExecutions` given the end execution status. It is when `SQLListener` makes sure that the number of `SQLExecutionUIData` entires does not exceed `spark.sql.ui.retainedExecutions` and removes the excess of the old entries.
