== [[Sink]] Streaming Sinks

A *Streaming Sink* represents an external storage to write streaming datasets to. It is modeled as `Sink` trait that can <<contract, process batches>> of data given as link:spark-sql-dataframe.adoc[DataFrames].

The following sinks are currently available in Spark:

* link:spark-sql-streaming-ConsoleSink.adoc[ConsoleSink] for `console` format.
* <<FileStreamSink, FileStreamSink>> for `parquet` format.
* link:spark-sql-streaming-ForeachSink.adoc[ForeachSink] used in link:spark-sql-streaming-DataStreamWriter.adoc#foreach[foreach] operator.
* <<MemorySink, MemorySink>> for `memory` format.

You can create your own streaming format implementing link:spark-sql-streaming-StreamSinkProvider.adoc[StreamSinkProvider].

=== [[contract]] Sink Contract

Sink Contract is described by `Sink` trait. It defines the one and only `addBatch` method to add `data` as `batchId`.

[source, scala]
----
package org.apache.spark.sql.execution.streaming

trait Sink {
  def addBatch(batchId: Long, data: DataFrame): Unit
}
----

=== [[FileStreamSink]] FileStreamSink

`FileStreamSink` is the streaming sink for the `parquet` format.

CAUTION: FIXME

[source, scala]
----
import scala.concurrent.duration._
import org.apache.spark.sql.streaming.{OutputMode, ProcessingTime}
val out = in.writeStream
  .format("parquet")
  .option("path", "parquet-output-dir")
  .option("checkpointLocation", "checkpoint-dir")
  .trigger(ProcessingTime(5.seconds))
  .outputMode(OutputMode.Append)
  .start()
----

`FileStreamSink` supports link:spark-sql-streaming-DataStreamWriter.adoc#outputMode[`Append` output mode] only.

It uses link:spark-sql-SQLConf.adoc#spark.sql.streaming.fileSink.log.deletion[spark.sql.streaming.fileSink.log.deletion] (as `isDeletingExpiredLog`)

=== [[MemorySink]] MemorySink

`MemorySink` is an memory-based `Sink` particularly useful for testing. It stores the results in memory.

It is available as `memory` format that requires a query name (by `queryName` method or `queryName` option).

TIP: See the example in link:spark-sql-streaming-MemoryStream.adoc[MemoryStream].

NOTE: It was introduced in the https://github.com/apache/spark/pull/12119[pull request for [SPARK-14288\][SQL\] Memory Sink for streaming].

Use `toDebugString` to see the batches.

Its aim is to allow users to test streaming applications in the Spark shell or other local tests.

You can set `checkpointLocation` using `option` method or it will be set to link:spark-sql-settings.adoc#spark.sql.streaming.checkpointLocation[spark.sql.streaming.checkpointLocation] setting.

If `spark.sql.streaming.checkpointLocation` is set, the code uses `$location/$queryName` directory.

Finally, when no `spark.sql.streaming.checkpointLocation` is set, a temporary directory `memory.stream` under `java.io.tmpdir` is used with `offsets` subdirectory inside.

NOTE: The directory is cleaned up at shutdown using `ShutdownHookManager.registerShutdownDeleteDir`.

[source, scala]
----
val nums = spark.range(10).withColumnRenamed("id", "num")

scala> val outStream = nums.writeStream
  .format("memory")
  .queryName("memStream")
  .start()
16/04/11 19:37:05 INFO HiveSqlParser: Parsing command: memStream
outStream: org.apache.spark.sql.StreamingQuery = Continuous Query - memStream [state = ACTIVE]
----

It creates `MemorySink` instance based on the schema of the DataFrame it operates on.

It creates a new DataFrame using `MemoryPlan` with `MemorySink` instance created earlier and registers it as a temporary table (using link:spark-sql-dataframe.adoc#registerTempTable[DataFrame.registerTempTable] method).

NOTE: At this point you can query the table as if it were a regular non-streaming table using link:spark-sql-sqlcontext.adoc#sql[sql] method.

A new link:spark-sql-streaming-StreamingQuery.adoc[StreamingQuery] is started (using link:spark-sql-streaming-StreamingQueryManager.adoc#startQuery[StreamingQueryManager.startQuery]) and returned.

CAUTION: FIXME Describe `else` part.
