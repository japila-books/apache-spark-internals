== [[BlockManager]] Block Manager -- Key-Value Store for Blocks

*Block Manager* (`BlockManager`) is a key-value store for blocks of data (simply _blocks_) in Spark. `BlockManager` acts as a local cache that runs on every "node" in a Spark application, i.e. the link:spark-driver.adoc[driver] and link:spark-executor.adoc[executors].

`BlockManager` provides interface for uploading and fetching blocks both locally and remotely using various stores, i.e. <<stores, memory, disk, and off-heap>>.

When <<creating-instance, `BlockManager` is created>>, it creates its own private instances of link:spark-DiskBlockManager.adoc[DiskBlockManager], link:spark-BlockInfoManager.adoc[BlockInfoManager], link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore] (that it immediately wires together, i.e. `BlockInfoManager` with `MemoryStore` and `DiskStore` with `DiskBlockManager`).

The common idiom in Spark to access a `BlockManager` regardless of a location, i.e. the driver or executors, is through  link:spark-sparkenv.adoc#get[SparkEnv]:

[source, scala]
----
SparkEnv.get.blockManager
----

A `BlockManager` is a link:spark-blockdatamanager.adoc[BlockDataManager], i.e. manages the storage for blocks that can represent cached RDD partitions, intermediate shuffle outputs, broadcasts, etc. It is also a <<BlockEvictionHandler, BlockEvictionHandler>> that drops a block from memory and storing it on a disk if applicable.

*Cached blocks* are blocks with non-zero sum of memory and disk sizes.

TIP: Use link:spark-webui.adoc[Web UI], esp. link:spark-webui-storage.adoc[Storage] and link:spark-webui-executors.adoc[Executors] tabs, to monitor the memory used.

TIP: Use link:spark-submit.adoc[spark-submit]'s command-line options, i.e. link:spark-submit.adoc#driver-memory[--driver-memory] for the driver and link:spark-submit.adoc#executor-memory[--executor-memory] for executors or their equivalents as Spark properties, i.e. link:spark-submit.adoc#spark_executor_memory[spark.executor.memory] and link:spark-submit.adoc#spark_driver_memory[spark.driver.memory], to control the memory for storage memory.

A <<creating-instance, `BlockManager` is created>> when a link:spark-sparkenv.adoc#create[Spark application starts] and must be <<initialize, initialized>> before it is fully operable.

When <<externalShuffleServiceEnabled, External Shuffle Service is enabled>>, `BlockManager` uses link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] to read other executors' shuffle files.

[TIP]
====
Enable `INFO`, `DEBUG` or `TRACE` logging level for `org.apache.spark.storage.BlockManager` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.storage.BlockManager=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

[TIP]
====
You may want to shut off WARN messages being printed out about the current state of blocks using the following line to cut the noise:

```
log4j.logger.org.apache.spark.storage.BlockManager=OFF
```
====

=== [[getMatchingBlockIds]] Getting Ids of Existing Blocks (For a Given Filter) -- `getMatchingBlockIds` Method

CAUTION: FIXME

NOTE: `getMatchingBlockIds` is used to handle <<BlockManagerSlaveEndpoint-GetMatchingBlockIds, GetMatchingBlockIds>> messages.

=== [[getLocalValues]] `getLocalValues` Method

[source, scala]
----
getLocalValues(blockId: BlockId): Option[BlockResult]
----

`getLocalValues`...FIXME

Internally, when `getLocalValues` is executed, you should see the following DEBUG message in the logs:

```
DEBUG BlockManager: Getting local block [blockId]
```

`getLocalValues` link:spark-BlockInfoManager.adoc#lockForReading[obtains a read lock for `blockId`].

When no `blockId` block was found, you should see the following DEBUG message in the logs and `getLocalValues` returns "nothing" (i.e. `NONE`).

```
DEBUG Block [blockId] was not found
```

When the `blockId` block was found, you should see the following DEBUG message in the logs:

```
DEBUG Level for block [blockId] is [level]
```

If `blockId` block has memory level and link:spark-MemoryStore.adoc#contains[is registered in `MemoryStore`], `getLocalValues` returns a <<BlockResult, BlockResult>> as `Memory` read method and with a `CompletionIterator` for an interator:

1. link:spark-MemoryStore.adoc#getValues[Values iterator from `MemoryStore` for `blockId`] for "deserialized" persistence levels.
2. Iterator from link:spark-SerializerManager.adoc#dataDeserializeStream[`SerializerManager` after the data stream has been deserialized] for the `blockId` block and link:spark-MemoryStore.adoc#getBytes[the bytes for `blockId` block] for "serialized" persistence levels.

CAUTION: FIXME

=== [[getRemoteValues]] `getRemoteValues` Internal Method

[source, scala]
----
getRemoteValues[T: ClassTag](blockId: BlockId): Option[BlockResult]
----

`getRemoteValues`...FIXME

=== [[get]] Retrieving Block from Local or Remote Block Managers -- `get` Method

[source, scala]
----
get[T](blockId: BlockId): Option[BlockResult]
----

`get` attempts to get the `blockId` block from a local block manager first before querying remote block managers.

Internally, `get` tries to <<getLocalValues, get `blockId` block from the local `BlockManager`>>. If the `blockId` block was found, you should see the following INFO message in the logs and `get` returns the local <<BlockResult, BlockResult>>.

```
INFO Found block [blockId] locally
```

If however the `blockId` block was not found locally, `get` tries to <<getRemoteValues, get the block from remote ``BlockManager``s>>. If the `blockId` block was retrieved from a remote `BlockManager`, you should see the following INFO message in the logs and `get` returns the remote <<BlockResult, BlockResult>>.

```
INFO Found block [blockId] remotely
```

In the end, `get` returns "nothing" (i.e. `NONE`) when the `blockId` block was not found either in the local `BlockManager` or any remote `BlockManager`.

NOTE: `get` is used when `BlockManager` is requested to <<getOrElseUpdate, `getOrElseUpdate` a block>>, <<getSingle, getSingle>> and to link:spark-rdd-blockrdd.adoc#[compute a `BlockRDD`].

=== [[getSingle]] `getSingle` Method

CAUTION: FIXME

=== [[getOrElseUpdate]] `getOrElseUpdate` Method

CAUTION: FIXME

[source, scala]
----
getOrElseUpdate[T](
  blockId: BlockId,
  level: StorageLevel,
  classTag: ClassTag[T],
  makeIterator: () => Iterator[T]): Either[BlockResult, Iterator[T]]
----

`getOrElseUpdate`...FIXME

=== [[getRemoteBytes]] `getRemoteBytes` Method

CAUTION: FIXME

=== [[getBlockData]] `getBlockData` Method

CAUTION: FIXME

=== [[removeBlockInternal]] `removeBlockInternal` Method

CAUTION: FIXME

=== [[externalShuffleServiceEnabled]] Using External Shuffle Service -- `externalShuffleServiceEnabled` Flag

When the link:spark-ExternalShuffleService.adoc[External Shuffle Service] is enabled for a Spark application, `BlockManager` uses link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] to read other executors' shuffle files.

CAUTION: FIXME How is `shuffleClient` used?

=== [[registerTask]] `registerTask` Method

CAUTION: FIXME

=== [[stores]] Stores

A *Store* is the place where blocks are held.

There are the following possible stores:

* link:spark-MemoryStore.adoc[MemoryStore] for memory storage level.
* link:spark-DiskStore.adoc[DiskStore] for disk storage level.
* `ExternalBlockStore` for OFF_HEAP storage level.

=== [[putBlockData]] Storing Block Data -- `putBlockData` Method

CAUTION: FIXME

=== [[putBytes]] Storing Block Bytes -- `putBytes` Method

[source, scala]
----
putBytes(
  blockId: BlockId,
  bytes: ChunkedByteBuffer,
  level: StorageLevel,
  tellMaster: Boolean = true): Boolean
----

`putBytes` stores the `blockId` block (with `bytes` bytes and `level` storage level).

`putBytes` simply passes the call on to the internal <<doPutBytes, doPutBytes>>.

NOTE: `putBytes` is executed when link:spark-executor-taskrunner.adoc#run-result-sent-via-blockmanager[`TaskRunner` sends a task result via `BlockManager`], <<putBlockData, `BlockManager` puts a block locally>> and in link:spark-broadcast.adoc#TorrentBroadcast[TorrentBroadcast].

==== [[doPutBytes]] `doPutBytes` Internal Method

[source, scala]
----
def doPutBytes[T](
  blockId: BlockId,
  bytes: ChunkedByteBuffer,
  level: StorageLevel,
  classTag: ClassTag[T],
  tellMaster: Boolean = true,
  keepReadLock: Boolean = false): Boolean
----

`doPutBytes` is an internal method that calls the internal helper <<doPut, doPut>> with a function that accepts a `BlockInfo` and does the uploading.

If the replication storage level is greater than 1, replication starts in a separate thread (using the internal <<replicate, replicate>> method).

NOTE: Replication is a integral part of link:spark-rdd-caching.adoc#StorageLevel[StorageLevel].

For a memory storage level, depending on whether it is a deserialized one or not, `putIteratorAsValues` or `putBytes` of link:spark-MemoryStore.adoc[MemoryStore] are used, respectively. If the put did not succeed and the storage level is also a disk one, you should see the following WARN message in the logs:

```
WARN BlockManager: Persisting block [blockId] to disk instead.
```

link:spark-DiskStore.adoc#putBytes[DiskStore.putBytes] is called.

NOTE: link:spark-DiskStore.adoc[DiskStore] is only used when link:spark-MemoryStore.adoc[MemoryStore] has failed for memory and disk storage levels.

If the storage level is a disk one only, link:spark-DiskStore.adoc#putBytes[DiskStore.putBytes] is called.

`doPutBytes` requests <<getCurrentBlockStatus, current block status>> and if the block was successfully stored, and the driver should know about it (`tellMaster`), it <<reportBlockStatus, reports current storage status of the block to the driver>>. The link:spark-taskscheduler-taskmetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the updated block status].

Regardless of the block being successfully stored or not, you should see the following DEBUG message in the logs:

```
DEBUG BlockManager: Put block [blockId] locally took [time] ms
```

For replication level greater than `1`, `doPutBytes` waits for the earlier asynchronous replication to finish.

The final result of `doPutBytes` is the result of storing the block successful or not (as computed earlier).

NOTE: `doPutBytes` is called exclusively from <<putBytes, `putBytes` method>>.

=== [[replicate]] `replicate` Method

CAUTION: FIXME

=== [[maybeCacheDiskValuesInMemory]] `maybeCacheDiskValuesInMemory` Method

CAUTION: FIXME

=== [[doPutIterator]] `doPutIterator` Method

CAUTION: FIXME

=== [[doPut]] `doPut` Internal Method

[source, scala]
----
doPut[T](
  blockId: BlockId,
  level: StorageLevel,
  classTag: ClassTag[_],
  tellMaster: Boolean,
  keepReadLock: Boolean)(putBody: BlockInfo => Option[T]): Option[T]
----

`doPut` is an internal helper method for <<doPutBytes, doPutBytes>> and <<doPutIterator, doPutIterator>>.

`doPut` executes the input `putBody` function with a link:spark-BlockInfo.adoc[BlockInfo] being a new `BlockInfo` object (with `level` storage level) that link:spark-BlockInfoManager.adoc#lockNewBlockForWriting[`BlockInfoManager` managed to create a write lock for].

If the block has already been created (and link:spark-BlockInfoManager.adoc#lockNewBlockForWriting[`BlockInfoManager` did not manage to create a write lock for]), the following WARN message is printed out to the logs:

```
WARN Block [blockId] already exists on this machine; not re-adding it
```

`doPut` <<releaseLock, releases the read lock for the block>> when `keepReadLock` flag is disabled and returns `None` immediately.

If however the write lock has been given, `doPut` executes `putBody`.

If the result of `putBody` is `None` the block is considered saved successfully.

For successful save and `keepReadLock` enabled, link:spark-BlockInfoManager.adoc#downgradeLock[`BlockInfoManager` is requested to downgrade an exclusive write lock for `blockId` to a shared read lock].

For successful save and `keepReadLock` disabled, link:spark-BlockInfoManager.adoc#unlock[`BlockInfoManager` is requested to release lock on `blockId`].

For unsuccessful save, <<removeBlockInternal, the block is removed from memory and disk stores>> and the following WARN message is printed out to the logs:

```
WARN Putting block [blockId] failed
```

Ultimately, the following DEBUG message is printed out to the logs:

```
DEBUG Putting block [blockId] [withOrWithout] replication took [usedTime] ms
```

=== [[removeBlock]] Removing Block From Memory and Disk -- `removeBlock` Method

[source, scala]
----
removeBlock(blockId: BlockId, tellMaster: Boolean = true): Unit
----

`removeBlock` removes the `blockId` block from the link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore].

When executed, it prints out the following DEBUG message to the logs:

```
DEBUG Removing block [blockId]
```

It requests link:spark-BlockInfoManager.adoc[BlockInfoManager] for lock for writing for the `blockId` block. If it receives none, it prints out the following WARN message to the logs and quits.

```
WARN Asked to remove block [blockId], which does not exist
```

Otherwise, with a write lock for the block, the block is removed from link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore] (see link:spark-MemoryStore.adoc#remove[Removing Block in `MemoryStore`] and link:spark-DiskStore.adoc#remove[Removing Block in `DiskStore`]).

If both removals fail, it prints out the following WARN message:

```
WARN Block [blockId] could not be removed as it was not found in either the disk, memory, or external block store
```

The block is removed from link:spark-BlockInfoManager.adoc[BlockInfoManager].

It then <<getCurrentBlockStatus, calculates the current block status>> that is used to <<reportBlockStatus, report the block status to the driver>> (if the input `tellMaster` and the info's `tellMaster` are both enabled, i.e. `true`) and the link:spark-taskscheduler-taskmetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change].

NOTE: It is used to <<removeRdd, remove RDDs>> and <<removeBroadcast, broadcast>> as well as in <<BlockManagerSlaveEndpoint-RemoveBlock, BlockManagerSlaveEndpoint while handling `RemoveBlock` messages>>.

=== [[removeRdd]] Removing RDD Blocks -- `removeRdd` Method

[source, scala]
----
removeRdd(rddId: Int): Int
----

`removeRdd` removes all the blocks that belong to the `rddId` RDD.

It prints out the following INFO message to the logs:

```
INFO Removing RDD [rddId]
```

It then requests RDD blocks from link:spark-BlockInfoManager.adoc[BlockInfoManager] and <<removeBlock, removes them (from memory and disk)>> (without informing the driver).

The number of blocks removed is the final result.

NOTE: It is used by <<BlockManagerSlaveEndpoint-RemoveRdd, BlockManagerSlaveEndpoint while handling `RemoveRdd` messages>>.

=== [[removeBroadcast]] Removing Broadcast Blocks -- `removeBroadcast` Method

[source, scala]
----
removeBroadcast(broadcastId: Long, tellMaster: Boolean): Int
----

`removeBroadcast` removes all the blocks of the input `broadcastId` broadcast.

Internally, it starts by printing out the following DEBUG message to the logs:

```
DEBUG Removing broadcast [broadcastId]
```

It then requests all the link:spark-blockdatamanager.adoc#BroadcastBlockId[BroadcastBlockId] objects that belong to the `broadcastId` broadcast from link:spark-BlockInfoManager.adoc[BlockInfoManager] and <<removeBlock, removes them (from memory and disk)>>.

The number of blocks removed is the final result.

NOTE: It is used by <<BlockManagerSlaveEndpoint-RemoveBroadcast, BlockManagerSlaveEndpoint while handling `RemoveBroadcast` messages>>.

=== [[getStatus]] Getting Block Status -- `getStatus` Method

CAUTION: FIXME

=== [[creating-instance]] Creating `BlockManager` Instance

A `BlockManager` needs the following services to be created:

* `executorId` (for the driver and executors)
* link:spark-rpc.adoc[RpcEnv]
* link:spark-BlockManagerMaster.adoc[BlockManagerMaster]
* link:spark-SerializerManager.adoc[SerializerManager]
* link:spark-configuration.adoc[SparkConf]
* link:spark-MemoryManager.adoc[MemoryManager]
* link:spark-service-mapoutputtracker.adoc[MapOutputTracker]
* link:spark-shuffle-manager.adoc[ShuffleManager]
* link:spark-blocktransferservice.adoc[BlockTransferService]
* `SecurityManager`

NOTE: `executorId` is `SparkContext.DRIVER_IDENTIFIER`, i.e. `driver` for the driver and the value of link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc#executor-id[--executor-id] command-line argument for link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc[CoarseGrainedExecutorBackend] executors or link:spark-executor-backends.adoc#MesosExecutorBackend[MesosExecutorBackend].

CAUTION: FIXME Elaborate on the executor backends and executor ids.

When a `BlockManager` instance is created it sets the internal `externalShuffleServiceEnabled` flag to the value of link:spark-ExternalShuffleService.adoc#spark_shuffle_service_enabled[spark.shuffle.service.enabled] setting.

It then creates an instance of link:spark-DiskBlockManager.adoc[DiskBlockManager] (requesting `deleteFilesOnStop` when an external shuffle service is not in use).

It creates an instance of link:spark-BlockInfoManager.adoc[BlockInfoManager] (as `blockInfoManager`).

It creates *block-manager-future* daemon cached thread pool with 128 threads maximum (as `futureExecutionContext`).

It creates a link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore].

link:spark-MemoryManager.adoc[MemoryManager] gets the link:spark-MemoryStore.adoc[MemoryStore] object assigned.

It calculates the maximum memory to use (as `maxMemory`) by requesting the maximum link:spark-MemoryManager.adoc#maxOnHeapStorageMemory[on-heap] and link:spark-MemoryManager.adoc#maxOffHeapStorageMemory[off-heap] storage memory from the assigned `MemoryManager`.

NOTE: link:spark-UnifiedMemoryManager.adoc[UnifiedMemoryManager] is the default `MemoryManager` (as of Spark 1.6).

It calculates the port used by the external shuffle service (as `externalShuffleServicePort`).

NOTE: It is computed specially in Spark on YARN.

CAUTION: FIXME Describe the YARN-specific part.

It creates a client to read other executors' shuffle files (as `shuffleClient`). If the external shuffle service is used an link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] is created or the input link:spark-blocktransferservice.adoc[BlockTransferService] is used.

It sets <<spark.block.failures.beforeLocationRefresh, the maximum number of failures before this block manager refreshes the block locations from the driver>> (as `maxFailuresBeforeLocationRefresh`).

It <<BlockManagerSlaveEndpoint, registers BlockManagerSlaveEndpoint>> with the input link:spark-rpc.adoc[RpcEnv], itself, and link:spark-service-mapoutputtracker.adoc[MapOutputTracker] (as `slaveEndpoint`).

NOTE: A `BlockManager` instance is created while link:spark-sparkenv.adoc#create[SparkEnv is being created].

=== [[shuffleClient]] `shuffleClient`

CAUTION: FIXME

(that is assumed to be a link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient])

=== [[shuffleServerId]] `shuffleServerId`

CAUTION: FIXME

=== [[initialize]] Initializing `BlockManager` -- `initialize` Method

[source, scala]
----
initialize(appId: String): Unit
----

`initialize` initializes a `BlockManager` on the driver and executors (see link:spark-sparkcontext.adoc#creating-instance[Creating SparkContext Instance] and link:spark-executor.adoc#creating-instance[Creating Executor Instance], respectively).

NOTE: The method must be called before a `BlockManager` can be considered fully operable.

`initialize` does the following in order:

1. Initializes link:spark-blocktransferservice.adoc#init[BlockTransferService]
2. Initializes the internal shuffle client, be it link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] or link:spark-blocktransferservice.adoc[BlockTransferService].
3. link:spark-BlockManagerMaster.adoc#registerBlockManager[Registers itself with the driver's `BlockManagerMaster`] (using the `id`, `maxMemory` and its `slaveEndpoint`).
+
The `BlockManagerMaster` reference is passed in when the <<creating-instance, `BlockManager` is created>> on the driver and executors.
4. Sets <<shuffleServerId, shuffleServerId>> to an instance of <<BlockManagerId, BlockManagerId>> given an executor id, host name and port for link:spark-blocktransferservice.adoc[BlockTransferService].
5. It creates the address of the server that serves this executor's shuffle files (using <<shuffleServerId, shuffleServerId>>)

CAUTION: FIXME Review the initialize procedure again

CAUTION: FIXME Describe `shuffleServerId`. Where is it used?

If the <<externalShuffleServiceEnabled, External Shuffle Service is used>>, the following INFO appears in the logs:

```
INFO external shuffle service port = [externalShuffleServicePort]
```

It link:spark-BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's BlockManagerMaster] passing the <<BlockManagerId, BlockManagerId>>, the maximum memory (as `maxMemory`), and the <<BlockManagerSlaveEndpoint, BlockManagerSlaveEndpoint>>.

Ultimately, if the initialization happens on an executor and the <<externalShuffleServiceEnabled, External Shuffle Service is used>>, it <<registerWithExternalShuffleServer, registers to the shuffle service>>.

NOTE: `initialize` is called when the link:spark-sparkcontext-creating-instance-internals.adoc#BlockManager-initialization[driver is launched (and `SparkContext` is created)] and when an link:spark-executor.adoc#creating-instance[`Executor` is created] (for link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc#RegisteredExecutor[CoarseGrainedExecutorBackend] and link:spark-executor-backends.adoc#MesosExecutorBackend[MesosExecutorBackend]).

==== [[registerWithExternalShuffleServer]] Registering Executor's BlockManager with External Shuffle Server -- `registerWithExternalShuffleServer` Method

[source, scala]
----
registerWithExternalShuffleServer(): Unit
----

`registerWithExternalShuffleServer` is an internal helper method to register the `BlockManager` for an executor with an link:spark-ExternalShuffleService.adoc[external shuffle server].

NOTE: It is executed when a <<initialize, `BlockManager` is initialized on an executor and an external shuffle service is used>>.

When executed, you should see the following INFO message in the logs:

```
INFO Registering executor with local external shuffle service.
```

It uses <<shuffleClient, shuffleClient>> to link:spark-shuffleclient.adoc#ExternalShuffleClient-registerWithShuffleServer[register the block manager] using <<shuffleServerId, shuffleServerId>> (i.e. the host, the port and the executorId) and a `ExecutorShuffleInfo`.

NOTE: The `ExecutorShuffleInfo` uses `localDirs` and `subDirsPerLocalDir` from link:spark-DiskBlockManager.adoc[DiskBlockManager] and the class name of the constructor link:spark-shuffle-manager.adoc[ShuffleManager].

It tries to register at most 3 times with 5-second sleeps in-between.

NOTE: The maximum number of attempts and the sleep time in-between are hard-coded, i.e. they are not configured.

Any issues while connecting to the external shuffle service are reported as ERROR messages in the logs:

```
ERROR Failed to connect to external shuffle server, will retry [#attempts] more times after waiting 5 seconds...
```

=== [[reregister]] Re-registering Blocks to Driver -- `reregister` Method

[source, scala]
----
reregister(): Unit
----

When called, `reregister` prints the following INFO message to the logs:

```
INFO BlockManager: BlockManager [blockManagerId] re-registering with master
```

`reregister` then link:spark-BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's `BlockManagerMaster`] (just as it was when <<initialize, BlockManager was initializing>>). It passes the <<BlockManagerId, BlockManagerId>>, the maximum memory (as `maxMemory`), and the <<BlockManagerSlaveEndpoint, BlockManagerSlaveEndpoint>>.

`reregister` will then report all the local blocks to the link:spark-BlockManagerMaster.adoc[BlockManagerMaster].

You should see the following INFO message in the logs:

```
INFO BlockManager: Reporting [blockInfoManager.size] blocks to the master.
```

For each block metadata (in link:spark-BlockInfoManager.adoc[BlockInfoManager]) it <<getCurrentBlockStatus, gets block current status>> and <<tryToReportBlockStatus, tries to send it to the BlockManagerMaster>>.

If there is an issue communicating to the link:spark-BlockManagerMaster.adoc[BlockManagerMaster], you should see the following ERROR message in the logs:

```
ERROR BlockManager: Failed to report [blockId] to master; giving up.
```

After the ERROR message, `reregister` stops reporting.

NOTE: `reregister` is called when a link:spark-executor.adoc#heartbeats-and-active-task-metrics[`Executor` was informed to re-register while sending heartbeats].

=== [[getCurrentBlockStatus]] Calculate Current Block Status -- `getCurrentBlockStatus` Method

[source, scala]
----
getCurrentBlockStatus(blockId: BlockId, info: BlockInfo): BlockStatus
----

`getCurrentBlockStatus` returns the current `BlockStatus` of the `BlockId` block (with the block's current link:spark-rdd-caching.adoc#StorageLevel[StorageLevel], memory and disk sizes). It uses link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore] for size and other information.

NOTE: Most of the information to build `BlockStatus` is already in `BlockInfo` except that it may not necessarily reflect the current state per link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore].

Internally, it uses the input link:spark-BlockInfo.adoc[BlockInfo] to know about the block's storage level. If the storage level is not set (i.e. `null`), the returned `BlockStatus` assumes the link:spark-rdd-caching.adoc#StorageLevel[default NONE storage level] and the memory and disk sizes being `0`.

If however the storage level is set, `getCurrentBlockStatus` uses link:spark-MemoryStore.adoc[MemoryStore] or link:spark-DiskStore.adoc[DiskStore] to check whether the block is stored in the storages or not and request for their sizes in the storages respectively (using their `getSize` or assume `0`).

NOTE: It is acceptable that the `BlockInfo` says to use memory or disk yet the block is not in the storages (yet or anymore). The method will give current status.

NOTE: `getCurrentBlockStatus` is used when <<reregister, executor's BlockManager is requested to report the current status of the local blocks to the master>>, <<doPutBytes, saving a block to a storage>> or <<dropFromMemory, removing a block from memory only>> or <<removeBlock, both, i.e. from memory and disk>>.

=== [[dropFromMemory]] Removing Blocks From Memory Only -- `dropFromMemory` Method

[source, scala]
----
dropFromMemory(
  blockId: BlockId,
  data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel
----

When `dropFromMemory` is executed, you should see the following INFO message in the logs:

```
INFO BlockManager: Dropping block [blockId] from memory
```

It then asserts that the `blockId` block is link:spark-BlockInfoManager.adoc#assertBlockIsLockedForWriting[locked for writing].

If the block's StorageLevel uses disks and the internal link:spark-DiskStore.adoc[DiskStore] object (`diskStore`) does not contain the block, it is saved then. You should see the following INFO message in the logs:

```
INFO BlockManager: Writing block [blockId] to disk
```

CAUTION: FIXME Describe the case with saving a block to disk.

The block's memory size is fetched and recorded (using `MemoryStore.getSize`).

The block is link:spark-MemoryStore.adoc#remove[removed from memory] if exists. If not, you should see the following WARN message in the logs:

```
WARN BlockManager: Block [blockId] could not be dropped from memory as it does not exist
```

It then <<getCurrentBlockStatus, calculates the current storage status of the block>> and <<reportBlockStatus, reports it to the driver>>. It only happens when `info.tellMaster`.

CAUTION: FIXME When would `info.tellMaster` be `true`?

A block is considered updated when it was written to disk or removed from memory or both. If either happened, the link:spark-taskscheduler-taskmetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change].

Ultimately, `dropFromMemory` returns the current storage level of the block.

NOTE: `dropFromMemory` is part of the single-method <<BlockEvictionHandler, BlockEvictionHandler>> interface.

=== [[reportAllBlocks]] `reportAllBlocks` Method

CAUTION: FIXME

NOTE: `reportAllBlocks` is called when `BlockManager` is requested to <<reregister, re-register all blocks to the driver>>.

=== [[reportBlockStatus]] Reporting Current Storage Status of Block to Driver -- `reportBlockStatus` Method

[source, scala]
----
reportBlockStatus(
  blockId: BlockId,
  info: BlockInfo,
  status: BlockStatus,
  droppedMemorySize: Long = 0L): Unit
----

`reportBlockStatus` is an internal method for <<tryToReportBlockStatus, reporting a block status to the driver>> and if told to re-register it prints out the following INFO message to the logs:

```
INFO BlockManager: Got told to re-register updating block [blockId]
```

It does asynchronous reregistration (using `asyncReregister`).

In either case, it prints out the following DEBUG message to the logs:

```
DEBUG BlockManager: Told master about block [blockId]
```

NOTE: `reportBlockStatus` is called by <<getBlockData, getBlockData>>, <<doPutBytes, doPutBytes>>, <<doPutIterator, doPutIterator>>, <<dropFromMemory, dropFromMemory>> and <<removeBlockInternal, removeBlockInternal>>.

=== [[tryToReportBlockStatus]] Reporting Block Status to Driver -- `tryToReportBlockStatus` Method

[source, scala]
----
def tryToReportBlockStatus(
  blockId: BlockId,
  info: BlockInfo,
  status: BlockStatus,
  droppedMemorySize: Long = 0L): Boolean
----

`tryToReportBlockStatus` is an internal method to link:spark-BlockManagerMaster.adoc#updateBlockInfo[report block status update to `BlockManagerMaster`] and returns its response.

NOTE: `tryToReportBlockStatus` is executed in <<reportAllBlocks, reportAllBlocks>> or <<reportBlockStatus, reportBlockStatus>>.

=== [[BlockEvictionHandler]] BlockEvictionHandler

`BlockEvictionHandler` is a `private[storage]` Scala trait with a single method <<BlockEvictionHandler-dropFromMemory, dropFromMemory>>.

[source, scala]
----
dropFromMemory(
  blockId: BlockId,
  data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel
----

NOTE: A `BlockManager` is a `BlockEvictionHandler`.

NOTE: `dropFromMemory` is called when  link:spark-MemoryStore.adoc#evictBlocksToFreeSpace[`MemoryStore` evicts blocks from memory to free space].

=== [[BlockManagerSlaveEndpoint]] BlockManagerSlaveEndpoint

`BlockManagerSlaveEndpoint` is a link:spark-rpc.adoc#ThreadSafeRpcEndpoint[thread-safe RPC endpoint] for remote communication between executors and the driver.

CAUTION: FIXME the intro needs more love.

While a <<creating-instance, BlockManager is being created>> so is the `BlockManagerSlaveEndpoint` RPC endpoint with the name *BlockManagerEndpoint[randomId]* to handle <<BlockManagerSlaveEndpoint-messages, RPC messages>>.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.storage.BlockManagerSlaveEndpoint` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.storage.BlockManagerSlaveEndpoint=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

==== [[BlockManagerSlaveEndpoint-RemoveBlock]] RemoveBlock Message

[source, scala]
----
RemoveBlock(blockId: BlockId)
----

When a `RemoveBlock` message comes in, you should see the following DEBUG message in the logs:

```
DEBUG BlockManagerSlaveEndpoint: removing block [blockId]
```

It then calls <<removeBlock, BlockManager to remove `blockId` block>>.

NOTE: Handling `RemoveBlock` messages happens on a separate thread. See <<BlockManagerSlaveEndpoint-asyncThreadPool, BlockManagerSlaveEndpoint Thread Pool>>.

When the computation is successful, you should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Done removing block [blockId], response is [response]
```

And `true` response is sent back. You should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Sent response: true to [senderAddress]
```

In case of failure, you should see the following ERROR in the logs and the stack trace.

```
ERROR BlockManagerSlaveEndpoint: Error in removing block [blockId]
```

==== [[BlockManagerSlaveEndpoint-RemoveRdd]] RemoveRdd Message

[source, scala]
----
RemoveRdd(rddId: Int)
----

When a `RemoveRdd` message comes in, you should see the following DEBUG message in the logs:

```
DEBUG BlockManagerSlaveEndpoint: removing RDD [rddId]
```

It then calls <<removeRdd, BlockManager to remove `rddId` RDD>>.

NOTE: Handling `RemoveRdd` messages happens on a separate thread. See <<BlockManagerSlaveEndpoint-asyncThreadPool, BlockManagerSlaveEndpoint Thread Pool>>.

When the computation is successful, you should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Done removing RDD [rddId], response is [response]
```

And the number of blocks removed is sent back. You should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Sent response: [#blocks] to [senderAddress]
```

In case of failure, you should see the following ERROR in the logs and the stack trace.

```
ERROR BlockManagerSlaveEndpoint: Error in removing RDD [rddId]
```

==== [[BlockManagerSlaveEndpoint-RemoveShuffle]] RemoveShuffle Message

[source, scala]
----
RemoveShuffle(shuffleId: Int)
----

When a `RemoveShuffle` message comes in, you should see the following DEBUG message in the logs:

```
DEBUG BlockManagerSlaveEndpoint: removing shuffle [shuffleId]
```

If link:spark-service-mapoutputtracker.adoc[MapOutputTracker] was given (when the RPC endpoint was created), it calls link:spark-service-mapoutputtracker.adoc#unregisterShuffle[MapOutputTracker to unregister the `shuffleId` shuffle].

It then calls link:spark-shuffle-manager.adoc#unregisterShuffle[ShuffleManager to unregister the `shuffleId` shuffle].

NOTE: Handling `RemoveShuffle` messages happens on a separate thread. See <<BlockManagerSlaveEndpoint-asyncThreadPool, BlockManagerSlaveEndpoint Thread Pool>>.

When the computation is successful, you should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Done removing shuffle [shuffleId], response is [response]
```

And the result is sent back. You should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Sent response: [response] to [senderAddress]
```

In case of failure, you should see the following ERROR in the logs and the stack trace.

```
ERROR BlockManagerSlaveEndpoint: Error in removing shuffle [shuffleId]
```

==== [[BlockManagerSlaveEndpoint-RemoveBroadcast]] RemoveBroadcast Message

[source, scala]
----
RemoveBroadcast(broadcastId: Long)
----

When a `RemoveBroadcast` message comes in, you should see the following DEBUG message in the logs:

```
DEBUG BlockManagerSlaveEndpoint: removing broadcast [broadcastId]
```

It then calls <<removeBroadcast, BlockManager to remove the `broadcastId` broadcast>>.

NOTE: Handling `RemoveBroadcast` messages happens on a separate thread. See <<BlockManagerSlaveEndpoint-asyncThreadPool, BlockManagerSlaveEndpoint Thread Pool>>.

When the computation is successful, you should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Done removing broadcast [broadcastId], response is [response]
```

And the result is sent back. You should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Sent response: [response] to [senderAddress]
```

In case of failure, you should see the following ERROR in the logs and the stack trace.

```
ERROR BlockManagerSlaveEndpoint: Error in removing broadcast [broadcastId]
```

==== [[BlockManagerSlaveEndpoint-GetBlockStatus]] GetBlockStatus Message

[source, scala]
----
GetBlockStatus(blockId: BlockId)
----

When a `GetBlockStatus` message comes in, it responds with the result of <<getStatus, calling BlockManager about the status of `blockId`>>.

==== [[BlockManagerSlaveEndpoint-GetMatchingBlockIds]] `GetMatchingBlockIds` Message

[source, scala]
----
GetMatchingBlockIds(filter: BlockId => Boolean, askSlaves: Boolean = true)
----

`GetMatchingBlockIds` triggers a computation of <<getMatchingBlockIds, the memory and disk blocks matching `filter`>> and sends it back.

==== [[BlockManagerSlaveEndpoint-TriggerThreadDump]] TriggerThreadDump Message

When a `TriggerThreadDump` message comes in, a thread dump is generated and sent back.

==== [[BlockManagerSlaveEndpoint-asyncThreadPool]] BlockManagerSlaveEndpoint Thread Pool

`BlockManagerSlaveEndpoint` uses *block-manager-slave-async-thread-pool* daemon thread pool (`asyncThreadPool`) for some messages to talk to other Spark services, i.e. `BlockManager`, link:spark-service-mapoutputtracker.adoc[MapOutputTracker], link:spark-shuffle-manager.adoc[ShuffleManager] in a non-blocking, asynchronous way.

The reason for the async thread pool is that the block-related operations might take quite some time and to release the main RPC thread other threads are spawned to talk to the external services and pass responses on to the clients.

NOTE: `BlockManagerSlaveEndpoint` uses Java's https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor].

=== [[broadcast]] Broadcast Values

When a new broadcast value is created, link:spark-broadcast.adoc#TorrentBroadcast[TorrentBroadcast] blocks are put in the block manager.

You should see the following `TRACE` message:

```
TRACE Put for block [blockId] took [startTimeMs] to get into synchronized block
```

It puts the data in the memory first and drop to disk if the memory store can't hold it.

```
DEBUG Put block [blockId] locally took [startTimeMs]
```

=== [[BlockManagerId]] BlockManagerId

FIXME

=== [[execution-context]] Execution Context

*block-manager-future* is the execution context for...FIXME

=== [[metrics]] Metrics

Block Manager uses link:spark-metrics.adoc[Spark Metrics System] (via `BlockManagerSource`) to report metrics about internal status.

The name of the source is *BlockManager*.

It emits the following numbers:

* memory / maxMem_MB - the maximum memory configured
* memory / remainingMem_MB - the remaining memory
* memory / memUsed_MB - the memory used
* memory / diskSpaceUsed_MB - the disk used

=== Misc

The underlying abstraction for blocks in Spark is a `ByteBuffer` that limits the size of a block to 2GB (`Integer.MAX_VALUE` - see http://stackoverflow.com/q/8076472/1305344[Why does FileChannel.map take up to Integer.MAX_VALUE of data?] and https://issues.apache.org/jira/browse/SPARK-1476[SPARK-1476 2GB limit in spark for blocks]). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for `long`), ser-deser via byte array-backed output streams.

When a non-local executor starts, it initializes a `BlockManager` object for the `spark.app.id` id.

=== [[BlockResult]] BlockResult

`BlockResult` is a description of a fetched block with the `readMethod` and `bytes`.
