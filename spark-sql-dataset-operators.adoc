== Dataset Operators

You can group the set of all operators to use with `Datasets` per their target, i.e. the part of a `Dataset` they are applied to.

1. link:spark-sql-columns.adoc[Column Operators]
2. link:spark-sql-functions.adoc[Standard Functions -- `functions` object]
3. link:spark-sql-udfs.adoc[User-Defined Functions (UDFs)]
4. link:spark-sql-aggregation.adoc[Aggregation -- Typed and Untyped Grouping]
5. link:spark-sql-UserDefinedAggregateFunction.adoc[`UserDefinedAggregateFunction` -- User-Defined Aggregate Functions (UDAFs)]
6. link:spark-sql-windows.adoc[Window Aggregate Operators -- Windows]
7. link:spark-sql-joins.adoc[Joins]
8. link:spark-sql-caching.adoc[Caching]

Beside the above operators, there are the following ones working with a `Dataset` as a whole.

.Dataset Operators
[cols="1,3",options="header",width="100%"]
|===
| Operator | Description
| <<as, as>> | Converting a `Dataset` to a `Dataset`
| <<coalesce, coalesce>> | Repartitioning a `Dataset` with shuffle disabled.
| <<createGlobalTempView, createGlobalTempView>> |
| <<createOrReplaceTempView, createOrReplaceTempView>> |
| <<createTempView, createTempView>> |
| <<explain, explain>> | Explain logical and physical plans of a `Dataset`
| <<filter, filter>> |
| <<flatMap, flatMap>> |
| <<foreachPartition, foreachPartition>> |
| <<isLocal, isLocal>> |
| <<isStreaming, isStreaming>> |
| <<mapPartition, mapPartition>> |
| <<randomSplit, randomSplit>> | Randomly split a `Dataset` into two ``Dataset``s
| <<rdd, rdd>> |
| <<repartition, repartition>> | Repartitioning a `Dataset` with shuffle enabled.
| <<schema, schema>> |
| <<select, select>> |
| <<selectExpr, selectExpr>> |
| <<show, show>> |
| <<take, take>> |
| <<toDF, toDF>> | Converts a `Dataset` to a `DataFrame`
| <<toJSON, toJSON>> |
| <<transform, transform>> | Transforms a `Dataset`
| <<where, where>> |
| <<write, write>> |
| <<writeStream, writeStream>> |
|===

=== [[createTempViewCommand]] `createTempViewCommand` Internal Operator

CAUTION: FIXME

=== [[createGlobalTempView]] `createGlobalTempView` Operator

CAUTION: FIXME

=== [[createOrReplaceTempView]] `createOrReplaceTempView` Operator

CAUTION: FIXME

=== [[createTempView]] `createTempView` Operator

CAUTION: FIXME

=== [[transform]] Transforming Datasets -- `transform` Operator

[source, scala]
----
transform[U](t: Dataset[T] => Dataset[U]): Dataset[U]
----

`transform` applies `t` function to the source `Dataset[T]` to produce a result `Dataset[U]`. It is for chaining custom transformations.

[source, scala]
----
val dataset = spark.range(5)

// Transformation t
import org.apache.spark.sql.Dataset
def withDoubled(longs: Dataset[java.lang.Long]) = longs.withColumn("doubled", 'id * 2)

scala> dataset.transform(withDoubled).show
+---+-------+
| id|doubled|
+---+-------+
|  0|      0|
|  1|      2|
|  2|      4|
|  3|      6|
|  4|      8|
+---+-------+
----

Internally, `transform` executes `t` function on the current `Dataset[T]`.

=== [[toDF]] Converting to `DataFrame` -- `toDF` Methods

[source, scala]
----
toDF(): DataFrame
toDF(colNames: String*): DataFrame
----

`toDF` converts a link:spark-sql-dataset.adoc[Dataset] into a link:spark-sql-dataframe.adoc[DataFrame].

Internally, the empty-argument `toDF` creates a `Dataset[Row]` using the ``Dataset``'s link:spark-sql-sparksession.adoc[SparkSession] and link:spark-sql-query-execution.adoc[QueryExecution] with the encoder being link:spark-sql-RowEncoder.adoc[RowEncoder].

CAUTION: FIXME Describe `toDF(colNames: String*)`

=== [[as]] Converting to `Dataset` -- `as` Method

CAUTION: FIXME

=== [[write]] Accessing `DataFrameWriter` -- `write` Method

[source, scala]
----
write: DataFrameWriter[T]
----

`write` method returns link:spark-sql-dataframewriter.adoc[DataFrameWriter] for records of type `T`.

[source, scala]
----
import org.apache.spark.sql.{DataFrameWriter, Dataset}
val ints: Dataset[Int] = (0 to 5).toDS

val writer: DataFrameWriter[Int] = ints.write
----

=== [[writeStream]] Accessing `DataStreamWriter` -- `writeStream` Method

[source, scala]
----
writeStream: DataStreamWriter[T]
----

`writeStream` method returns link:spark-sql-streaming-DataStreamWriter.adoc[DataStreamWriter] for records of type `T`.

[source, scala]
----
val papers = spark.readStream.text("papers").as[String]

import org.apache.spark.sql.streaming.DataStreamWriter
val writer: DataStreamWriter[String] = papers.writeStream
----

=== [[show]] Display Records -- `show` Methods

[source, scala]
----
show(): Unit
show(numRows: Int): Unit
show(truncate: Boolean): Unit
show(numRows: Int, truncate: Boolean): Unit
show(numRows: Int, truncate: Int): Unit
----

CAUTION: FIXME

Internally, `show` relays to a private `showString` to do the formatting. It turns the `Dataset` into a `DataFrame` (by calling `toDF()`) and <<take, takes first `n` records>>.

=== [[take]] Taking First n Records -- `take` Action

[source, scala]
----
take(n: Int): Array[T]
----

`take` is an action on a `Dataset` that returns a collection of `n` records.

WARNING: `take` loads all the data into the memory of the Spark application's driver process and for a large `n` could result in `OutOfMemoryError`.

Internally, `take` creates a new `Dataset` with `Limit` logical plan for `Literal` expression and the current `LogicalPlan`. It then runs the link:spark-sql-SparkPlan.adoc[SparkPlan] that produces a `Array[InternalRow]` that is in turn decoded to `Array[T]` using a bounded link:spark-sql-Encoder.adoc[encoder].

=== [[foreachPartition]] `foreachPartition` Action

[source, scala]
----
foreachPartition(f: Iterator[T] => Unit): Unit
----

`foreachPartition` applies the `f` function to each partition of the `Dataset`.

[source, scala]
----
case class Record(id: Int, city: String)
val ds = Seq(Record(0, "Warsaw"), Record(1, "London")).toDS

ds.foreachPartition { iter: Iterator[Record] => iter.foreach(println) }
----

NOTE: `foreachPartition` is used to link:spark-sql-dataframewriter.adoc#jdbc[save a `DataFrame` to a JDBC table] (indirectly through `JdbcUtils.saveTable`) and link:spark-sql-streaming-ForeachSink.adoc[ForeachSink].

=== [[mapPartitions]] `mapPartitions` Operator

[source, scala]
----
mapPartitions[U: Encoder](func: Iterator[T] => Iterator[U]): Dataset[U]
----

`mapPartitions` returns a new `Dataset` (of type `U`) with the function `func` applied to each partition.

CAUTION: FIXME Example

=== [[flatMap]] Creating Zero or More Records -- `flatMap` Operator

[source, scala]
----
flatMap[U: Encoder](func: T => TraversableOnce[U]): Dataset[U]
----

`flatMap` returns a new `Dataset` (of type `U`) with all records (of type `T`) mapped over using the function `func` and then flattening the results.

NOTE: `flatMap` can create new records. It deprecated `explode`.

[source, scala]
----
final case class Sentence(id: Long, text: String)
val sentences = Seq(Sentence(0, "hello world"), Sentence(1, "witaj swiecie")).toDS

scala> sentences.flatMap(s => s.text.split("\\s+")).show
+-------+
|  value|
+-------+
|  hello|
|  world|
|  witaj|
|swiecie|
+-------+
----

Internally, `flatMap` calls <<mapPartitions, mapPartitions>> with the partitions `flatMap(ped)`.

=== [[coalesce]] Repartitioning Dataset with Shuffle Disabled -- `coalesce` Operator

[source, scala]
----
coalesce(numPartitions: Int): Dataset[T]
----

`coalesce` operator repartitions the `Dataset` to exactly `numPartitions` partitions.

Internally, `coalesce` creates a `Repartition` logical operator with `shuffle` disabled (which is marked as `false` in the below ``explain``'s output).

[source, scala]
----
scala> spark.range(5).coalesce(1).explain(extended = true)
== Parsed Logical Plan ==
Repartition 1, false
+- Range (0, 5, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint
Repartition 1, false
+- Range (0, 5, step=1, splits=Some(8))

== Optimized Logical Plan ==
Repartition 1, false
+- Range (0, 5, step=1, splits=Some(8))

== Physical Plan ==
Coalesce 1
+- *Range (0, 5, step=1, splits=Some(8))
----

=== [[repartition]] Repartitioning Dataset with Shuffle Enabled -- `repartition` Operators

[source, scala]
----
repartition(numPartitions: Int): Dataset[T]
repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T]
repartition(partitionExprs: Column*): Dataset[T]
----

`repartition` operators repartition the `Dataset` to exactly `numPartitions` partitions or using `partitionExprs` expressions.

Internally, `repartition` creates a `Repartition` or `RepartitionByExpression` logical operators with `shuffle` enabled, respectively (which is marked as `true` in the below ``explain``'s output).

[source, scala]
----
scala> spark.range(5).repartition(1).explain(extended = true)
== Parsed Logical Plan ==
Repartition 1, true
+- Range (0, 5, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint
Repartition 1, true
+- Range (0, 5, step=1, splits=Some(8))

== Optimized Logical Plan ==
Repartition 1, true
+- Range (0, 5, step=1, splits=Some(8))

== Physical Plan ==
Exchange RoundRobinPartitioning(1)
+- *Range (0, 5, step=1, splits=Some(8))
----

NOTE: `repartition` methods correspond to SQL's `DISTRIBUTE BY` or `CLUSTER BY`.

=== [[select]] Projecting Columns -- `select` Operators

[source, scala]
----
select[U1: Encoder](c1: TypedColumn[T, U1]): Dataset[U1]
select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)]
select[U1, U2, U3](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)]
select[U1, U2, U3, U4](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3],
  c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)]
select[U1, U2, U3, U4, U5](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3],
  c4: TypedColumn[T, U4],
  c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)]
----

CAUTION: FIXME

=== [[filter]] `filter` Operator

CAUTION: FIXME

=== [[where]] `where` Operators

[source, scala]
----
where(condition: Column): Dataset[T]
where(conditionExpr: String): Dataset[T]
----

`where` is a synonym for <<filter, filter>> operator, i.e. it simply passes the parameters on to `filter`.

=== [[selectExpr]] Projecting Columns using Expressions -- `selectExpr` Operator

[source, scala]
----
selectExpr(exprs: String*): DataFrame
----

`selectExpr` is like `select`, but accepts SQL expressions `exprs`.

[source, scala]
----
val ds = spark.range(5)

scala> ds.selectExpr("rand() as random").show
16/04/14 23:16:06 INFO HiveSqlParser: Parsing command: rand() as random
+-------------------+
|             random|
+-------------------+
|  0.887675894185651|
|0.36766085091074086|
| 0.2700020856675186|
| 0.1489033635529543|
| 0.5862990791950973|
+-------------------+
----

Internally, it executes `select` with every expression in `exprs` mapped to link:spark-sql-columns.adoc[Column] (using link:spark-sql-sql-parsers.adoc[SparkSqlParser.parseExpression]).

[source, scala]
----
scala> ds.select(expr("rand() as random")).show
+------------------+
|            random|
+------------------+
|0.5514319279894851|
|0.2876221510433741|
|0.4599999092045741|
|0.5708558868374893|
|0.6223314406247136|
+------------------+
----

NOTE: A new feature in Spark **2.0.0**.

=== [[randomSplit]] Randomly Split Dataset -- `randomSplit` Operators

[source, scala]
----
randomSplit(weights: Array[Double]): Array[Dataset[T]]
randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]]
----

`randomSplit` randomly splits the `Dataset` per `weights`.

`weights` doubles should sum up to `1` and will be normalized if they do not.

You can define `seed` and if you don't, a random `seed` will be used.

NOTE: It is used in link:spark-mllib/spark-mllib-estimators.adoc#TrainValidationSplit[TrainValidationSplit] to split dataset into training and validation datasets.

[source, scala]
----
val ds = spark.range(10)
scala> ds.randomSplit(Array[Double](2, 3)).foreach(_.show)
+---+
| id|
+---+
|  0|
|  1|
|  2|
+---+

+---+
| id|
+---+
|  3|
|  4|
|  5|
|  6|
|  7|
|  8|
|  9|
+---+
----

NOTE: A new feature in Spark **2.0.0**.

=== [[explain]] Explaining Logical and Physical Plans -- `explain` Operator

[source, scala]
----
explain(): Unit
explain(extended: Boolean): Unit
----

`explain` prints the link:spark-sql-LogicalPlan.adoc[logical] and (with `extended` enabled) link:spark-sql-SparkPlan.adoc[physical] plans to the console. Use it to review the structured queries and optimizations applied.

TIP: If you are serious about query debugging you could also use the link:spark-sql-debugging-execution.adoc[Debugging Query Execution facility].

Internally, `explain` link:spark-sql-SparkPlan.adoc#executeCollect[executes] a link:spark-sql-ExplainCommand.adoc[ExplainCommand] logical command.

[source, scala]
----
scala> spark.range(10).explain(extended = true)
== Parsed Logical Plan ==
Range (0, 10, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint
Range (0, 10, step=1, splits=Some(8))

== Optimized Logical Plan ==
Range (0, 10, step=1, splits=Some(8))

== Physical Plan ==
*Range (0, 10, step=1, splits=Some(8))
----

=== [[toJSON]] `toJSON` method

`toJSON` maps the content of `Dataset` to a `Dataset` of JSON strings.

NOTE: A new feature in Spark **2.0.0**.

[source, scala]
----
scala> val ds = Seq("hello", "world", "foo bar").toDS
ds: org.apache.spark.sql.Dataset[String] = [value: string]

scala> ds.toJSON.show
+-------------------+
|              value|
+-------------------+
|  {"value":"hello"}|
|  {"value":"world"}|
|{"value":"foo bar"}|
+-------------------+
----

Internally, `toJSON` grabs the `RDD[InternalRow]` (of the link:spark-sql-query-execution.adoc#toRdd[QueryExecution] of the `Dataset`) and link:spark-rdd-transformations.adoc#mapPartitions[maps the records (per RDD partition)] into JSON.

NOTE: `toJSON` uses Jackson's JSON parser -- https://github.com/FasterXML/jackson-module-scala[jackson-module-scala].

=== [[schema]] Accessing Schema -- `schema` Method

A `Dataset` has a *schema*.

[source, scala]
----
schema: StructType
----

[TIP]
====
You may also use the following methods to learn about the schema:

* `printSchema(): Unit`
* <<explain, explain>>
====

=== [[rdd]] Converting Dataset into RDD -- `rdd` Attribute

[source, scala]
----
rdd: RDD[T]
----

Whenever you are in need to convert a `Dataset` into a `RDD`, executing `rdd` method gives you the RDD of the proper input object type (not link:spark-sql-dataframe.adoc#features[Row as in DataFrames]) that sits behind the `Dataset`.

[source, scala]
----
scala> val rdd = tokens.rdd
rdd: org.apache.spark.rdd.RDD[Token] = MapPartitionsRDD[11] at rdd at <console>:30
----

Internally, it looks link:spark-sql-Encoder.adoc#ExpressionEncoder[ExpressionEncoder] (for the `Dataset`) up and accesses the `deserializer` expression. That gives the link:spark-sql-DataType.adoc[DataType] of the result of evaluating the expression.

NOTE: A deserializer expression is used to decode an link:spark-sql-InternalRow.adoc[InternalRow] to an object of type `T`. See link:spark-sql-Encoder.adoc#ExpressionEncoder[ExpressionEncoder].

It then executes a link:spark-sql-logical-plan-DeserializeToObject.adoc[`DeserializeToObject` logical operator] that will produce a `RDD[InternalRow]` that is converted into the proper `RDD[T]` using the `DataType` and `T`.

NOTE: It is a lazy operation that "produces" a `RDD[T]`.

=== [[isStreaming]] `isStreaming` Method

`isStreaming` returns `true` when `Dataset` contains link:spark-sql-streaming-streamingrelation.adoc[StreamingRelation] or link:spark-sql-streaming-streamingrelation.adoc#StreamingExecutionRelation[StreamingExecutionRelation] *streaming sources*.

NOTE: Streaming datasets are created using link:spark-sql-dataframereader.adoc#stream[DataFrameReader.stream] method (for link:spark-sql-streaming-streamingrelation.adoc[StreamingRelation]) and contain link:spark-sql-streaming-streamingrelation.adoc#StreamingExecutionRelation[StreamingExecutionRelation] after link:spark-sql-streaming-DataStreamWriter.adoc#start[DataStreamWriter.start].

[source, scala]
----
val reader = spark.read
val helloStream = reader.stream("hello")

scala> helloStream.isStreaming
res9: Boolean = true
----

NOTE: A new feature in Spark **2.0.0**.

=== [[isLocal]] Is Dataset Local -- `isLocal` method

[source, scala]
----
isLocal: Boolean
----

`isLocal` is a flag that says whether operators like `collect` or `take` could be run locally, i.e. without using executors.

Internally, `isLocal` checks whether the logical query plan of a `Dataset` is link:spark-sql-logical-plan-LocalRelation.adoc[LocalRelation].
