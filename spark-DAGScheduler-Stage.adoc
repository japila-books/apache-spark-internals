== [[Stage]] Stage -- Physical Unit Of Execution

A *stage* is a physical unit of execution. It is a step in a physical execution plan.

A stage is a set of parallel tasks -- one task per partition (of an RDD that computes partial results of a function executed as part of a Spark job).

.Stage, tasks and submitting a job
image::diagrams/stage-tasks.png[align="center"]

In other words, a Spark job is a computation with that computation sliced into stages.

A stage is uniquely identified by `id`. When a stage is created, link:spark-dagscheduler.adoc[DAGScheduler] increments internal counter `nextStageId` to track the number of link:spark-dagscheduler.adoc#submitStage[stage submissions].

[[rdd]]
A stage can only work on the partitions of a single RDD (identified by `rdd`), but can be associated with many other dependent parent stages (via internal field `parents`), with the boundary of a stage marked by shuffle dependencies.

Submitting a stage can therefore trigger execution of a series of dependent parent stages (refer to link:spark-dagscheduler.adoc#runJob[RDDs, Job Execution, Stages, and Partitions]).

.Submitting a job triggers execution of the stage and its parent stages
image::diagrams/job-stage.png[align="center"]

Finally, every stage has a `firstJobId` that is the id of the job that submitted the stage.

There are two types of stages:

* link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] is an intermediate stage (in the execution DAG) that produces data for other stage(s). It writes *map output files* for a shuffle. It can also be the final stage in a job in link:spark-dagscheduler.adoc#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling].
* link:spark-dagscheduler-ResultStage.adoc[ResultStage] is the final stage that executes link:spark-rdd.adoc#actions[a Spark action] in a user program by running a function on an RDD.

When a job is submitted, a new stage is created with the parent link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] linked -- they can be created from scratch or linked to, i.e. shared, if other jobs use them already.

.DAGScheduler and Stages for a job
image::diagrams/scheduler-job-shuffles-result-stages.png[align="center"]

A stage tracks the jobs (their ids) it belongs to (using the internal `jobIds` registry).

DAGScheduler splits up a job into a collection of stages. Each stage contains a sequence of link:spark-rdd.adoc[narrow transformations] that can be completed without link:spark-rdd-shuffle.adoc[shuffling] the entire data set, separated at *shuffle boundaries*, i.e. where shuffle occurs. Stages are thus a result of breaking the RDD graph at shuffle boundaries.

.Graph of Stages
image::images/dagscheduler-stages.png[align="center"]

Shuffle boundaries introduce a barrier where stages/tasks must wait for the previous stage to finish before they fetch map outputs.

.DAGScheduler splits a job into stages
image::diagrams/scheduler-job-splits-into-stages.png[align="center"]

RDD operations with link:spark-rdd.adoc[narrow dependencies], like `map()` and `filter()`, are pipelined together into one set of tasks in each stage, but operations with shuffle dependencies require multiple stages, i.e. one to write a set of map output files, and another to read those files after a barrier.

In the end, every stage will have only shuffle dependencies on other stages, and may compute multiple operations inside it. The actual pipelining of these operations happens in the `RDD.compute()` functions of various RDDs, e.g. `MappedRDD`, `FilteredRDD`, etc.

At some point of time in a stage's life, every partition of the stage gets transformed into a task - link:spark-taskscheduler-ShuffleMapTask.adoc[ShuffleMapTask] or link:spark-taskscheduler-ResultTask.adoc[ResultTask] for link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] and link:spark-dagscheduler-ResultStage.adoc[ResultStage], respectively.

Partitions are computed in jobs, and result stages may not always need to compute all partitions in their target RDD, e.g. for actions like `first()` and `lookup()`.

`DAGScheduler` prints the following INFO message when there are tasks to submit:

```
INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (ShuffledRDD[86] at reduceByKey at <console>:24)
```

There is also the following DEBUG message with pending partitions:

```
DEBUG DAGScheduler: New pending partitions: Set(0)
```

Tasks are later submitted to link:spark-TaskScheduler.adoc[Task Scheduler] (via `taskScheduler.submitTasks`).

When no tasks in a stage can be submitted, the following DEBUG message shows in the logs:

```
FIXME
```

[[internal-registries]]
.Stage's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[details]] `details`
| Long description of the stage

Used when...FIXME

| [[fetchFailedAttemptIds]] `fetchFailedAttemptIds`
| FIXME

Used when...FIXME

| [[jobIds]] `jobIds`
| Set of link:spark-dagscheduler-jobs.adoc[jobs] the stage belongs to.

Used when...FIXME

| [[name]] `name`
| Name of the stage

Used when...FIXME

| [[nextAttemptId]] `nextAttemptId`
| The ID for the next attempt of the stage.

Used when...FIXME

| [[numPartitions]] `numPartitions`
| Number of partitions

Used when...FIXME

| [[pendingPartitions]] `pendingPartitions`
| Set of pending link:spark-rdd-partitions.adoc[partitions]

Used when...FIXME

| [[_latestInfo]] `_latestInfo`
| Internal cache with...FIXME

Used when...FIXME
|===

=== [[contract]] Stage Contract

[source, scala]
----
abstract class Stage {
  def findMissingPartitions(): Seq[Int]
}
----

NOTE: `Stage` is a `private[scheduler] abstract` contract.

.Stage Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[findMissingPartitions]] `findMissingPartitions`
| Used when...
|===

=== [[findMissingPartitions]] `findMissingPartitions` Method

`Stage.findMissingPartitions()` calculates the ids of the missing partitions, i.e. partitions for which the ActiveJob knows they are not finished (and so they are missing).

A link:spark-dagscheduler-ResultStage.adoc[ResultStage] stage knows it by querying the active job about partition ids (`numPartitions`) that are not finished (using `ActiveJob.finished` array of booleans).

.ResultStage.findMissingPartitions and ActiveJob
image::images/resultstage-findMissingPartitions.png[align="center"]

In the above figure, partitions 1 and 2 are not finished (`F` is false while `T` is true).

=== [[failedOnFetchAndShouldAbort]] `failedOnFetchAndShouldAbort` Method

`Stage.failedOnFetchAndShouldAbort(stageAttemptId: Int): Boolean` checks whether the number of fetch failed attempts (using `fetchFailedAttemptIds`) exceeds the number of consecutive failures allowed for a given stage (that should then be aborted)

NOTE: The number of consecutive failures for a stage is not configurable.

=== [[latestInfo]] Getting StageInfo For Most Recent Attempt -- `latestInfo` Method

[source, scala]
----
latestInfo: StageInfo
----

`latestInfo` simply returns the <<_latestInfo, most recent `StageInfo`>> (i.e. makes it accessible).

=== [[makeNewStageAttempt]] Creating New Stage Attempt (as StageInfo) -- `makeNewStageAttempt` Method

[source, scala]
----
makeNewStageAttempt(
  numPartitionsToCompute: Int,
  taskLocalityPreferences: Seq[Seq[TaskLocation]] = Seq.empty): Unit
----

`makeNewStageAttempt` link:spark-taskscheduler-taskmetrics.adoc[creates a new `TaskMetrics`] and link:spark-taskscheduler-taskmetrics.adoc#register[registers the internal accumulators (using the RDD's `SparkContext`)].

NOTE: `makeNewStageAttempt` uses <<rdd, rdd>> that was defined when <<creating-instance, `Stage` was created>>.

`makeNewStageAttempt` sets <<_latestInfo, _latestInfo>> to be a link:spark-dagscheduler-StageInfo.adoc#fromStage[`StageInfo` from the current stage] (with <<nextAttemptId, nextAttemptId>>, `numPartitionsToCompute`, and `taskLocalityPreferences`).

`makeNewStageAttempt` increments <<nextAttemptId, nextAttemptId>> counter.

NOTE: `makeNewStageAttempt` is used exclusively when link:spark-dagscheduler.adoc#submitMissingTasks[`DAGScheduler` submits missing tasks for a stage].
