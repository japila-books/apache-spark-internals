== [[ShuffleMapStage]] `ShuffleMapStage` -- Intermediate Stage in Job

A *ShuffleMapStage* (aka *shuffle map stage*, or simply *map stage*) is an intermediate stage in the execution DAG that produces data for link:spark-rdd-shuffle.adoc[shuffle operation]. It is an input for the other following stages in the DAG of stages. That is why it is also called a *shuffle dependency's map side*.

TIP: Read about link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency].

A `ShuffleMapStage` may contain multiple *pipelined operations*, e.g. `map` and `filter`, before shuffle operation.

CAUTION: FIXME: Show the example and the logs + figures

A `ShuffleMapStage` can be part of many jobs -- refer to the section <<stage-sharing, `ShuffleMapStage` sharing>>.

A `ShuffleMapStage` is a stage with a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] -- the shuffle that it is part of and `outputLocs` and `numAvailableOutputs` track how many map outputs are ready.

NOTE: ``ShuffleMapStage``s can also be submitted independently as jobs with `DAGScheduler.submitMapStage` for link:spark-dagscheduler.adoc#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling].

[[isAvailable]]
When executed, a `ShuffleMapStage` saves *map output files* that can later be fetched by reduce tasks. When all map outputs are available, the `ShuffleMapStage` is considered *available* (or *ready*).

CAUTION: FIXME Figure with ShuffleMapStages saving files

The output locations (`outputLocs`) of a `ShuffleMapStage` are the same as used by its link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]. Output locations can be missing, i.e. partitions have not been cached or are lost.

A `ShuffleMapStage` is registered to DAGScheduler that tracks the mapping of shuffles (by their ids from SparkContext) to corresponding ShuffleMapStages that compute them, stored in `shuffleToMapStage`.

A `ShuffleMapStage` is created from an input link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and a job's id (in `DAGScheduler#newOrUsedShuffleStage`).

CAUTION: FIXME Where's `shuffleToMapStage` used?

* getShuffleMapStage - see <<stage-sharing, Stage sharing>>
* getAncestorShuffleDependencies

When there is no `ShuffleMapStage` for a shuffle id (of a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]), one is created with the ancestor shuffle dependencies of the RDD (of a `ShuffleDependency`) that are registered to link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster].

FIXME Where is `ShuffleMapStage` used?

* newShuffleMapStage - the proper way to create shuffle map stages (with the additional setup steps)
* `getShuffleMapStage` - see <<stage-sharing, Stage sharing>>

[CAUTION]
====
FIXME

* What's `ShuffleMapStage.outputLocs`?
* `newShuffleMapStage`
====

=== [[addActiveJob]] `addActiveJob` Method

CAUTION: FIXME

=== [[creating-instance]] Creating `ShuffleMapStage` Instance

CAUTION: FIXME

=== [[mapStageJobs]] `mapStageJobs` Method

CAUTION: FIXME

=== [[addOutputLoc]] Registering `MapStatus` Result For Partition -- `addOutputLoc` Method

[source, scala]
----
addOutputLoc(partition: Int, status: MapStatus): Unit
----

CAUTION: FIXME

=== [[shuffleDep]] `shuffleDep` Property

CAUTION: FIXME

=== [[removeActiveJob]] `removeActiveJob` Method

CAUTION: FIXME

=== [[stage-sharing]] ShuffleMapStage Sharing

A `ShuffleMapStage` can be shared across multiple jobs, if these jobs reuse the same RDDs.

When a `ShuffleMapStage` is submitted to DAGScheduler to execute, `getShuffleMapStage` is called.

[source, scala]
----
scala> val rdd = sc.parallelize(0 to 5).map((_,1)).sortByKey()  // <1>

scala> rdd.count  // <2>

scala> rdd.count  // <3>
----
<1> Shuffle at `sortByKey()`
<2> Submits a job with two stages with two being executed
<3> Intentionally repeat the last action that submits a new job with two stages with one being shared as already-being-computed

.Skipped Stages are already-computed ShuffleMapStages
image::images/dagscheduler-webui-skipped-stages.png[align="center"]
