== [[RDD]] RDD

[[T]]
`RDD` is a description of a distributed computation over dataset of records of type `T`.

[[id]]
`RDD` is identified by a *unique identifier* (aka *RDD ID*) that is unique among all RDDs in a link:spark-SparkContext.adoc[SparkContext].

[source, scala]
----
id: Int
----

[[storageLevel]]
`RDD` has a link:spark-rdd-StorageLevel.adoc[storage level] that...FIXME

[source, scala]
----
storageLevel: StorageLevel
----

The storage level of an RDD is link:spark-rdd-StorageLevel.adoc#NONE[StorageLevel.NONE] by default which is...FIXME

=== [[getOrCompute]] Getting Or Computing RDD Partition -- `getOrCompute` Method

[source, scala]
----
getOrCompute(partition: Partition, context: TaskContext): Iterator[T]
----

`getOrCompute` creates a link:spark-BlockDataManager.adoc#RDDBlockId[RDDBlockId] for the <<id, RDD id>> and the link:spark-rdd-Partition.adoc#index[partition index].

`getOrCompute` requests the `BlockManager` to link:spark-BlockManager.adoc#getOrElseUpdate[getOrElseUpdate] for the block ID (with the <<storageLevel, storage level>> and the `makeIterator` function).

NOTE: `getOrCompute` uses link:spark-SparkEnv.adoc#get[SparkEnv] to access the current link:spark-SparkEnv.adoc#blockManager[BlockManager].

[[getOrCompute-readCachedBlock]]
`getOrCompute` records whether...FIXME (readCachedBlock)

`getOrCompute` branches off per the response from the link:spark-BlockManager.adoc#getOrElseUpdate[BlockManager] and whether the internal `readCachedBlock` flag is now on or still off. In either case, `getOrCompute` creates an link:spark-InterruptibleIterator.adoc[InterruptibleIterator].

NOTE: link:spark-InterruptibleIterator.adoc[InterruptibleIterator] simply delegates to a wrapped internal `Iterator`, but allows for link:spark-TaskContext.adoc#isInterrupted[task killing functionality].

For a `BlockResult` available and `readCachedBlock` flag on, `getOrCompute`...FIXME

For a `BlockResult` available and `readCachedBlock` flag off, `getOrCompute`...FIXME

NOTE: The `BlockResult` could be found in a local block manager or fetched from a remote block manager. It may also have been stored (persisted) just now. In either case, the `BlockResult` is available (and link:spark-BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a `Left` value with the `BlockResult`).

For `Right(iter)` (regardless of the value of `readCachedBlock` flag since...FIXME), `getOrCompute`...FIXME

NOTE: link:spark-BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a `Right(iter)` value to indicate an error with a block.

NOTE: `getOrCompute` is used on Spark executors.

NOTE: `getOrCompute` is a `private[spark]` method that is exclusively used when <<iterator, iterating over partition when a RDD is cached>>.

=== [[compute]] Computing Partition (in TaskContext) -- `compute` Method

[source, scala]
----
compute(split: Partition, context: TaskContext): Iterator[T]
----

The abstract `compute` method computes the input `split` link:spark-rdd-partitions.adoc[partition] in the link:spark-TaskContext.adoc[TaskContext] to produce a collection of values (of type `T`).

`compute` is implemented by any type of RDD in Spark and is called every time the records are requested unless RDD is link:spark-rdd-caching.adoc[cached] or link:spark-rdd-checkpointing.adoc[checkpointed] (and the records can be read from an external storage, but this time closer to the compute node).

When an RDD is link:spark-rdd-caching.adoc[cached], for specified link:spark-rdd-StorageLevel.adoc[storage levels] (i.e. all but `NONE`) link:spark-cachemanager.adoc[`CacheManager` is requested to get or compute partitions].

NOTE: `compute` method runs on the link:spark-driver.adoc[driver].
