== [[QueryExecution]] QueryExecution -- Query Execution of Dataset

`QueryExecution` represents the <<execution-pipeline, structured query execution pipeline>> of a link:spark-sql-Dataset.adoc[Dataset].

NOTE: When an action of a `Dataset` is executed, that triggers an execution of `QueryExecution` (in the form of calling <<toRdd, toRdd>>) which will morph itself into a `RDD` of link:spark-sql-InternalRow.adoc[binary rows], i.e. `RDD[InternalRow]`.

You can access the `QueryExecution` of a `Dataset` using link:spark-sql-Dataset.adoc#queryExecution[queryExecution] attribute.

[source, scala]
----
val ds: Dataset[Long] = ...
val queryExec = ds.queryExecution
----

`QueryExecution` is the result of link:spark-sql-SessionState.adoc#executePlan[executing a `LogicalPlan` in a `SparkSession`] (and so you could create a `Dataset` from a link:spark-sql-LogicalPlan.adoc[logical operator] or use the `QueryExecution` after executing a logical operator).

[[execution-pipeline]]
[[attributes]]
.QueryExecution's (Lazily-Initialized) Attributes (aka Structured Query Execution Pipeline)
[cols="1,3",options="header",width="100%"]
|===
| Attribute / Phase
| Description

| [[analyzed]] `analyzed`
a| Analyzed <<logical, logical plan>> that has passed link:spark-sql-Analyzer.adoc#execute[Analyzer]'s check rules.

[source, scala]
----
val schema = queryExecution.analyzed.output
----

TIP: Use Dataset's link:spark-sql-dataset-operators.adoc#explain[explain(extended = true)] or SQL's `EXPLAIN EXTENDED` to see the analyzed logical plan of a structured query.

| [[withCachedData]] `withCachedData`
| `LogicalPlan` that is the `analyzed` plan after being analyzed, checked (for unsupported operations) and replaced with cached segments.

| [[optimizedPlan]] `optimizedPlan`
| Optimized link:spark-sql-LogicalPlan.adoc[logical plan] being the result of executing the session-owned link:spark-sql-SessionState.adoc#optimizer[Catalyst Query Optimizer] to <<withCachedData, withCachedData>>.

| [[sparkPlan]] `sparkPlan`
a| link:spark-sql-SparkPlan.adoc[Physical plan] (after link:spark-sql-SparkPlanner.adoc[SparkPlanner] has planned the <<optimizedPlan, optimized logical plan>>).

NOTE: `sparkPlan` is the first physical plan from the collection of all possible physical plans.

NOTE: It is guaranteed that Catalyst's `QueryPlanner` (which `SparkPlanner` extends) link:spark-sql-catalyst-QueryPlanner.adoc#plan[will always generate at least one physical plan].

| [[executedPlan]] `executedPlan`
a| link:spark-sql-SparkPlan.adoc[Physical plan] ready for execution (i.e. <<sparkPlan, sparkPlan>> after <<prepareForExecution, physical optimization rules applied>>).

NOTE: `executedPlan` is the phase when link:spark-sql-CollapseCodegenStages.adoc[CollapseCodegenStages] physical preparation rule is executed to collapse physical operators that support code generation together as a link:spark-sql-SparkPlan-WholeStageCodegenExec.adoc[WholeStageCodegenExec] operator.

| [[toRdd]] `toRdd`
a| `RDD` of link:spark-sql-InternalRow.adoc[binary rows] (after link:spark-sql-SparkPlan.adoc#execute[executing] the <<executedPlan, executedPlan>>).

[NOTE]
====
`toRdd` is a "boundary" between two Spark modules: Spark SQL and Spark Core.

After you have executed `toRdd` (directly or not), you basically "leave" Spark SQL's Datasets and "enter" Spark Core's RDD space.
====
|===

You can access the lazy attributes as follows:

[source, scala]
----
val dataset: Dataset[Long] = ...
dataset.queryExecution.executedPlan
----

[[properties]]
.QueryExecution's Properties (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[planner]] `planner`
| link:spark-sql-SparkPlanner.adoc[SparkPlanner]
|===

`QueryExecution` uses the input `SparkSession` to access the current link:spark-sql-SparkPlanner.adoc[SparkPlanner] (through link:spark-sql-SessionState.adoc[SessionState]) when <<creating-instance, it is created>>. It then computes a link:spark-sql-SparkPlan.adoc[SparkPlan] (a `PhysicalPlan` exactly) using the planner. It is available as the <<sparkPlan, `sparkPlan` attribute>>.

A streaming variant of `QueryExecution` is <<IncrementalExecution, IncrementalExecution>>.

TIP: Use link:spark-sql-dataset-operators.adoc#explain[explain] operator to know about the logical and physical plans of a `Dataset`.

[source, scala]
----
val ds = spark.range(5)
scala> ds.queryExecution
res17: org.apache.spark.sql.execution.QueryExecution =
== Parsed Logical Plan ==
Range 0, 5, 1, 8, [id#39L]

== Analyzed Logical Plan ==
id: bigint
Range 0, 5, 1, 8, [id#39L]

== Optimized Logical Plan ==
Range 0, 5, 1, 8, [id#39L]

== Physical Plan ==
WholeStageCodegen
:  +- Range 0, 1, 8, 5, [id#39L]
----

NOTE: `QueryExecution` belongs to `org.apache.spark.sql.execution` package.

NOTE: `QueryExecution` is a transient feature of a link:spark-sql-Dataset.adoc[Dataset], i.e. it is not preserved across serializations.

=== [[simpleString]] `simpleString` Method

CAUTION: FIXME

=== [[debug]] debug Object

CAUTION: FIXME

=== [[completeString]] Building Complete Text Representation -- `completeString` Internal Method

CAUTION: FIXME

=== [[creating-instance]] Creating QueryExecution Instance

`QueryExecution` takes the following when created:

* [[sparkSession]] link:spark-sql-SparkSession.adoc[SparkSession]
* [[logical]] link:spark-sql-LogicalPlan.adoc[LogicalPlan]

=== [[preparations]] Physical Plan Preparation Rules -- `preparations` Method

`preparations` is a sequence of link:spark-sql-SparkPlan.adoc[physical plan] preparation rules (i.e. `Rule[SparkPlan]`).

TIP: A `SparkPlan` preparation rule transforms a link:spark-sql-SparkPlan.adoc[physical plan] to another (possibly more efficient).

`preparations` is one of the final phases of query execution that Spark developers could use for further query optimizations.

The current list of `SparkPlan` transformations in `preparations` is as follows:

1. `ExtractPythonUDFs`
1. `PlanSubqueries`
1. link:spark-sql-EnsureRequirements.adoc[EnsureRequirements]
1. link:spark-sql-CollapseCodegenStages.adoc[CollapseCodegenStages]
1. `ReuseExchange`
1. `ReuseSubquery`

NOTE: The physical preparation rules are applied sequentially in order to the physical plan before execution, i.e. they generate a `SparkPlan` when <<executedPlan, executedPlan>> lazy value is first accessed (and is cached afterwards).

=== [[prepareForExecution]] Executing preparations Physical Plan Rules -- `prepareForExecution` Method

[source, scala]
----
prepareForExecution(plan: SparkPlan): SparkPlan
----

`prepareForExecution` takes <<preparations, preparations>> rules and applies them one by one to the input `plan`.

NOTE: `prepareForExecution` is used exclusively when `QueryExecution` <<executedPlan, prepares physical plan for execution>>.

=== [[IncrementalExecution]] IncrementalExecution

`IncrementalExecution` is a custom `QueryExecution` with `OutputMode`, `checkpointLocation`, and `currentBatchId`.

It lives in `org.apache.spark.sql.execution.streaming` package.

CAUTION: FIXME What is `stateStrategy`?

Stateful operators in the query plan are numbered using `operatorId` that starts with `0`.

`IncrementalExecution` adds one `Rule[SparkPlan]` called `state` to <<preparations, preparations>> sequence of rules as the first element.

CAUTION: FIXME What does `IncrementalExecution` do? Where is it used?

=== [[assertAnalyzed]] Creating Analyzed Logical Plan and Checking Correctness -- `assertAnalyzed` Method

[source, scala]
----
assertAnalyzed(): Unit
----

`assertAnalyzed` triggers initialization of <<analyzed, analyzed>> (which is almost like executing it).

NOTE: `assertAnalyzed` executes <<analyzed, analyzed>> by accessing it and throwing the result away. Since `analyzed` is a lazy value in Scala, it will then get initialized for the first time and stays so forever.

`assertAnalyzed` then requests `Analyzer` to link:spark-sql-Analyzer-CheckAnalysis.adoc#checkAnalysis[check the correctness of the analysis of the LogicalPlan] (i.e. `analyzed`).

[NOTE]
====
`assertAnalyzed` uses <<sparkSession, SparkSession>> to link:spark-sql-SparkSession.adoc#sessionState[access the current `SessionState`] that it then uses to link:spark-sql-SessionState.adoc#analyzer[access the `Analyzer`].

In Scala the access path looks as follows.

[source, scala]
----
sparkSession.sessionState.analyzer
----
====

In case of any `AnalysisException`, `assertAnalyzed` creates a new `AnalysisException` to make sure that it holds <<analyzed, analyzed>> and reports it.

[NOTE]
====
`assertAnalyzed` is used when:

* `Dataset` link:spark-sql-Dataset.adoc#creating-instance[is created]
* `QueryExecution` <<withCachedData, is requested for `LogicalPlan` with cached data>>
* link:spark-sql-LogicalPlan-CreateViewCommand.adoc#run[CreateViewCommand] and link:spark-sql-LogicalPlan-AlterViewAsCommand.adoc#run[AlterViewAsCommand] are executed
====

=== [[toString]] Building Extended Text Representation with Logical and Physical Plans -- `toString` Method

[source, scala]
----
toString: String
----

`toString` is a mere alias for <<completeString, completeString>> with `appendStats` flag disabled.

NOTE: `toString` is on the "other" side of <<toStringWithStats, toStringWithStats>> which has `appendStats` flag enabled.

NOTE: `toString` is used when...FIXME

=== [[toStringWithStats]] Building Text Representation with Cost Stats -- `toStringWithStats` Method

[source, scala]
----
toStringWithStats: String
----

`toStringWithStats` is a mere alias for <<completeString, completeString>> with `appendStats` flag enabled.

NOTE: `toStringWithStats` is a custom <<toString, toString>> with cost statistics.

[source, scala]
----
// test dataset
val dataset = spark.range(20).limit(2)

// toStringWithStats in action - note Optimized Logical Plan section with Statistics
scala> dataset.queryExecution.toStringWithStats
res6: String =
== Parsed Logical Plan ==
GlobalLimit 2
+- LocalLimit 2
   +- Range (0, 20, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint
GlobalLimit 2
+- LocalLimit 2
   +- Range (0, 20, step=1, splits=Some(8))

== Optimized Logical Plan ==
GlobalLimit 2, Statistics(sizeInBytes=32.0 B, rowCount=2, isBroadcastable=false)
+- LocalLimit 2, Statistics(sizeInBytes=160.0 B, isBroadcastable=false)
   +- Range (0, 20, step=1, splits=Some(8)), Statistics(sizeInBytes=160.0 B, isBroadcastable=false)

== Physical Plan ==
CollectLimit 2
+- *Range (0, 20, step=1, splits=Some(8))
----

NOTE: `toStringWithStats` is used exclusively when `ExplainCommand` link:spark-sql-LogicalPlan-ExplainCommand.adoc#run[is executed] (only when `cost` attribute is enabled).

=== [[hiveResultString]] Transforming SparkPlan Execution Result to Hive-Compatible Output Format -- `hiveResultString` Method

[source, scala]
----
hiveResultString(): Seq[String]
----

`hiveResultString` returns the result as a Hive-compatible output format.

[source, scala]
----
scala> spark.range(5).queryExecution.hiveResultString
res0: Seq[String] = ArrayBuffer(0, 1, 2, 3, 4)

scala> spark.read.csv("people.csv").queryExecution.hiveResultString
res4: Seq[String] = ArrayBuffer(id	name	age, 0	Jacek	42)
----

Internally, `hiveResultString` <<hiveResultString-transformations, transformation>> the <<executedPlan, SparkPlan>>.

[[hiveResultString-transformations]]
.hiveResultString's SparkPlan Transformations (in execution order)
[width="100%",cols="1,2",options="header"]
|===
| SparkPlan
| Description

| link:spark-sql-SparkPlan-ExecutedCommandExec.adoc[ExecutedCommandExec] for `DescribeTableCommand`
| Executes `DescribeTableCommand` and transforms every link:spark-sql-Row.adoc[Row] to a Hive-compatible output format.

| link:spark-sql-SparkPlan-ExecutedCommandExec.adoc[ExecutedCommandExec] for `ShowTablesCommand`
| Executes `ExecutedCommandExec` and transforms the result to a collection of table names.

| Any other link:spark-sql-SparkPlan.adoc[SparkPlan]
| Executes `SparkPlan` and transforms the result to a Hive-compatible output format.
|===

NOTE: `hiveResultString` is used exclusively when `SparkSQLDriver` (of ThriftServer) runs a command.
