== [[TaskContext]] TaskContext

`TaskContext` is the <<contract, base>> for <<implementations, contextual information>> about a task.

You can access the active `TaskContext` instance using <<get, TaskContext.get>> method.

[source, scala]
----
import org.apache.spark.TaskContext
val ctx = TaskContext.get
----

`TaskContext` allows for <<registering-task-listeners, registering task listeners>> and <<getLocalProperty, accessing local properties>> that were set on the driver.

NOTE: `TaskContext` is serializable.

[[contract]]
[source, scala]
----
package org.apache.spark

abstract class TaskContext extends Serializable {
  // only required methods that have no implementation
  // the others follow
  def isCompleted(): Boolean
  def isInterrupted(): Boolean
  def isRunningLocally(): Boolean
  def addTaskCompletionListener(listener: TaskCompletionListener): TaskContext
  def addTaskFailureListener(listener: TaskFailureListener): TaskContext
  def stageId(): Int
  def stageAttemptNumber(): Int
  def partitionId(): Int
  def attemptNumber(): Int
  def taskAttemptId(): Long
  def getLocalProperty(key: String): String
  def taskMetrics(): TaskMetrics
  def getMetricsSources(sourceName: String): Seq[Source]
  private[spark] def killTaskIfInterrupted(): Unit
  private[spark] def getKillReason(): Option[String]
  private[spark] def taskMemoryManager(): TaskMemoryManager
  private[spark] def registerAccumulator(a: AccumulatorV2[_, _]): Unit
  private[spark] def setFetchFailed(fetchFailed: FetchFailedException): Unit
}
----

.(Subset of) TaskContext Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| `addTaskCompletionListener`
| [[addTaskCompletionListener]] Registers a `TaskCompletionListener`

Used when...

| `addTaskFailureListener`
| [[addTaskFailureListener]] Registers a `TaskFailureListener`

Used when...

| `attemptNumber`
| [[attemptNumber]] Specifies how many times the task has been attempted (starting from 0).

Used when...

| `getLocalProperty`
| [[getLocalProperty]] Used when...

Accesses local properties set by the driver using link:spark-sparkcontext-local-properties.adoc#setLocalProperty[SparkContext.setLocalProperty].

| `getMetricsSources`
| [[getMetricsSources]] Gives all the metrics sources by `sourceName` which are associated with the instance that runs the task.

| `isCompleted`
| [[isCompleted]] Used when...

| `isInterrupted`
| [[isInterrupted]] A flag that is enabled when a task was killed.

Used when...

| `killTaskIfInterrupted`
a| [[killTaskIfInterrupted]] If the task was marked for interruption, i.e. cancellation, `killTaskIfInterrupted` (is supposed to) throws a `TaskKilledException` with the reason for the interrupt (that in turn kills the task).

Used (to break a task execution) when:

* `InterruptibleIterator` is requested to link:spark-InterruptibleIterator.adoc#hasNext[hasNext]

* `TaskRunner` is requested to link:spark-Executor-TaskRunner.adoc#run[run]

* `SortedIterator` and `UnsafeSorterSpillReader` are requested to `loadNext`

* Spark SQL's `FileScanRDD` is requested to `compute`

| `partitionId`
| [[partitionId]] Id of the link:spark-rdd-Partition.adoc[Partition] computed by the task.

Used when...

| `registerAccumulator`
| [[registerAccumulator]] Used when...

| `stageId`
| [[stageId]] Id of the link:spark-scheduler-Stage.adoc[Stage] the task belongs to.

Used when...

| `taskAttemptId`
| [[taskAttemptId]] Id of the attempt of the task.

Used when...

| `taskMemoryManager`
| [[taskMemoryManager]] Used when...

| `taskMetrics`
| [[taskMetrics]] link:spark-executor-TaskMetrics.adoc[TaskMetrics] of the active link:spark-scheduler-Task.adoc[Task].

Used when...
|===

[[implementations]]
NOTE: link:spark-TaskContextImpl.adoc[TaskContextImpl] is the one and only known implementation of <<contract, TaskContext Contract>> in Apache Spark.

=== [[unset]] `unset` Method

CAUTION: FIXME

=== [[setTaskContext]] `setTaskContext` Method

CAUTION: FIXME

=== [[get]] Accessing Active TaskContext -- `get` Method

[source, scala]
----
get(): TaskContext
----

`get` method returns the `TaskContext` instance for an active task (as a link:spark-TaskContextImpl.adoc[TaskContextImpl]). There can only be one instance and tasks can use the object to access contextual information about themselves.

[source, scala]
----
val rdd = sc.range(0, 3, numSlices = 3)

scala> rdd.partitions.size
res0: Int = 3

rdd.foreach { n =>
  import org.apache.spark.TaskContext
  val tc = TaskContext.get
  val msg = s"""|-------------------
                |partitionId:   ${tc.partitionId}
                |stageId:       ${tc.stageId}
                |attemptNum:    ${tc.attemptNumber}
                |taskAttemptId: ${tc.taskAttemptId}
                |-------------------""".stripMargin
  println(msg)
}
----

NOTE: `TaskContext` object uses https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[ThreadLocal] to keep it thread-local, i.e. to associate state with the thread of a task.

=== [[registering-task-listeners]] Registering Task Listeners

Using `TaskContext` object you can register task listeners for <<addTaskCompletionListener, task completion regardless of the final state>> and <<addTaskFailureListener, task failures only>>.

==== [[addTaskCompletionListener]] `addTaskCompletionListener` Method

[source, scala]
----
addTaskCompletionListener(listener: TaskCompletionListener): TaskContext
addTaskCompletionListener(f: (TaskContext) => Unit): TaskContext
----

`addTaskCompletionListener` methods register a `TaskCompletionListener` listener to be executed on task completion.

NOTE: It will be executed regardless of the final state of a task - success, failure, or cancellation.

[source, scala]
----
val rdd = sc.range(0, 5, numSlices = 1)

import org.apache.spark.TaskContext
val printTaskInfo = (tc: TaskContext) => {
  val msg = s"""|-------------------
                |partitionId:   ${tc.partitionId}
                |stageId:       ${tc.stageId}
                |attemptNum:    ${tc.attemptNumber}
                |taskAttemptId: ${tc.taskAttemptId}
                |-------------------""".stripMargin
  println(msg)
}

rdd.foreachPartition { _ =>
  val tc = TaskContext.get
  tc.addTaskCompletionListener(printTaskInfo)
}
----

==== [[addTaskFailureListener]] `addTaskFailureListener` Method

[source, scala]
----
addTaskFailureListener(listener: TaskFailureListener): TaskContext
addTaskFailureListener(f: (TaskContext, Throwable) => Unit): TaskContext
----

`addTaskFailureListener` methods register a `TaskFailureListener` listener to be executed on task failure only. It can be executed multiple times since a task can be re-attempted when it fails.

[source, scala]
----
val rdd = sc.range(0, 2, numSlices = 2)

import org.apache.spark.TaskContext
val printTaskErrorInfo = (tc: TaskContext, error: Throwable) => {
  val msg = s"""|-------------------
                |partitionId:   ${tc.partitionId}
                |stageId:       ${tc.stageId}
                |attemptNum:    ${tc.attemptNumber}
                |taskAttemptId: ${tc.taskAttemptId}
                |error:         ${error.toString}
                |-------------------""".stripMargin
  println(msg)
}

val throwExceptionForOddNumber = (n: Long) => {
  if (n % 2 == 1) {
    throw new Exception(s"No way it will pass for odd number: $n")
  }
}

// FIXME It won't work.
rdd.map(throwExceptionForOddNumber).foreachPartition { _ =>
  val tc = TaskContext.get
  tc.addTaskFailureListener(printTaskErrorInfo)
}

// Listener registration matters.
rdd.mapPartitions { (it: Iterator[Long]) =>
  val tc = TaskContext.get
  tc.addTaskFailureListener(printTaskErrorInfo)
  it
}.map(throwExceptionForOddNumber).count
----

=== [[getPartitionId]] (Unused) Accessing Partition Id -- `getPartitionId` Method

[source, scala]
----
getPartitionId(): Int
----

`getPartitionId` <<get, gets the active `TaskContext`>> and returns <<partitionId, partitionId>> or `0` (if `TaskContext` not available).

NOTE: `getPartitionId` is not used.
