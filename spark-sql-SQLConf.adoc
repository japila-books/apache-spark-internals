== [[SQLConf]] SQLConf

`SQLConf` is an internal key-value configuration store for <<parameters, parameters and hints>> used in Spark SQL.

[NOTE]
====
`SQLConf` is not meant to be used directly and _is_ available through the user-facing interface `RuntimeConfig` that you can access using link:spark-sql-SparkSession.adoc#conf[SparkSession].

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...

scala> spark.conf
res0: org.apache.spark.sql.RuntimeConfig = org.apache.spark.sql.RuntimeConfig@6b58a0f9
----
====

`SQLConf` offers methods to <<get, get>>, <<set, set>>, <<unset, unset>> or <<clear, clear>> their values, but has also the <<accessor-methods, accessor methods>> to read the current value of a parameter or hint.

You can access a session-specific `SQLConf` using link:spark-sql-SparkSession.adoc#sessionState[SessionState]:

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...

import spark.sessionState.conf

// accessing properties through accessor methods
scala> conf.numShufflePartitions
res0: Int = 200

// setting properties using aliases
import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS
conf.setConf(SHUFFLE_PARTITIONS, 2)
scala> conf.numShufflePartitions
res2: Int = 2

// unset aka reset properties to the default value
conf.unsetConf(SHUFFLE_PARTITIONS)
scala> conf.numShufflePartitions
res4: Int = 200
----

[[accessor-methods]]
.SQLConf's Accessor Methods (in alphabetical order)
[cols="1,1,1",options="header",width="100%"]
|===
| Name
| Parameter / Hint
| Description

| [[adaptiveExecutionEnabled]] `adaptiveExecutionEnabled`
| <<spark.sql.adaptive.enabled, spark.sql.adaptive.enabled>>
| Used exclusively for `EnsureRequirements` link:spark-sql-EnsureRequirements.adoc#withExchangeCoordinator[to add ExchangeCoordinator] (when adaptive query execution is enabled)

| [[autoBroadcastJoinThreshold]] `autoBroadcastJoinThreshold`
| <<spark.sql.autoBroadcastJoinThreshold, spark.sql.autoBroadcastJoinThreshold>>
| Used exclusively in link:spark-sql-SparkStrategy-JoinSelection.adoc[JoinSelection] execution planning strategy

| [[broadcastTimeout]] `broadcastTimeout`
| <<spark.sql.broadcastTimeout, spark.sql.broadcastTimeout>>
| Used exclusively in link:spark-sql-SparkPlan-BroadcastExchangeExec.adoc[BroadcastExchangeExec] (for broadcasting a table to executors).

| [[columnBatchSize]] `columnBatchSize`
| <<spark.sql.inMemoryColumnarStorage.batchSize, spark.sql.inMemoryColumnarStorage.batchSize>>
| Used...FIXME

| [[dataFramePivotMaxValues]] `dataFramePivotMaxValues`
| <<spark.sql.pivotMaxValues, spark.sql.pivotMaxValues>>
| Used exclusively in link:spark-sql-RelationalGroupedDataset.adoc#pivot[pivot] operator.

| [[dataFrameRetainGroupColumns]] `dataFrameRetainGroupColumns`
| <<spark.sql.retainGroupColumns, spark.sql.retainGroupColumns>>
| Used exclusively in link:spark-sql-RelationalGroupedDataset.adoc[RelationalGroupedDataset] when creating the result `Dataset` (after `agg`, `count`, `mean`, `max`, `avg`, `min`, and `sum` operators).

| [[defaultSizeInBytes]] `defaultSizeInBytes`
| <<spark.sql.defaultSizeInBytes, spark.sql.defaultSizeInBytes>>
| Used...FIXME

| [[numShufflePartitions]] `numShufflePartitions`
| <<spark.sql.shuffle.partitions, spark.sql.shuffle.partitions>>
a|

Used in:

* Dataset's link:spark-sql-dataset-operators.adoc#repartition[repartition] operator (for a link:spark-sql-LogicalPlan-Repartition-RepartitionByExpression.adoc#RepartitionByExpression[RepartitionByExpression] logical operator)
* link:spark-sql-SparkSqlAstBuilder.adoc#withRepartitionByExpression[SparkSqlAstBuilder] (for a link:spark-sql-LogicalPlan-Repartition-RepartitionByExpression.adoc#RepartitionByExpression[RepartitionByExpression] logical operator)
* link:spark-sql-SparkStrategy-JoinSelection.adoc#canBuildLocalHashMap[JoinSelection] execution planning strategy
* link:spark-sql-LogicalPlan-RunnableCommand.adoc#SetCommand[SetCommand] logical command
* link:spark-sql-EnsureRequirements.adoc#defaultNumPreShufflePartitions[EnsureRequirements] physical plan optimization

| [[joinReorderEnabled]] `joinReorderEnabled`
| <<spark.sql.cbo.joinReorder.enabled, spark.sql.cbo.joinReorder.enabled>>
| Used exclusively in link:spark-sql-Optimizer-CostBasedJoinReorder.adoc[CostBasedJoinReorder] logical plan optimization

| [[limitScaleUpFactor]] `limitScaleUpFactor`
| <<spark.sql.limit.scaleUpFactor, spark.sql.limit.scaleUpFactor>>
| Used exclusively when a physical operator is requested link:spark-sql-SparkPlan.adoc#executeTake[the first n rows as an array].

| [[preferSortMergeJoin]] `preferSortMergeJoin`
| <<spark.sql.join.preferSortMergeJoin, spark.sql.join.preferSortMergeJoin>>
| Used exclusively in link:spark-sql-SparkStrategy-JoinSelection.adoc[JoinSelection] execution planning strategy to prefer sort merge join over shuffle hash join.

| [[starSchemaDetection]] `starSchemaDetection`
| <<spark.sql.cbo.starSchemaDetection, spark.sql.cbo.starSchemaDetection>>
| Used exclusively in link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization (and indirectly in `StarSchemaDetection`)

| [[useCompression]] `useCompression`
| <<spark.sql.inMemoryColumnarStorage.compressed, spark.sql.inMemoryColumnarStorage.compressed>>
| Used...FIXME

| [[wholeStageEnabled]] `wholeStageEnabled`
| <<spark.sql.codegen.wholeStage, spark.sql.codegen.wholeStage>>
a| Used in:

* link:spark-sql-CollapseCodegenStages.adoc[CollapseCodegenStages] to control codegen
* `ParquetFileFormat` to control row batch reading

| [[wholeStageFallback]] `wholeStageFallback`
| <<spark.sql.codegen.fallback, spark.sql.codegen.fallback>>
| Used exclusively when `WholeStageCodegenExec` is link:spark-sql-SparkPlan-WholeStageCodegenExec.adoc#doExecute[executed].

| [[wholeStageMaxNumFields]] `wholeStageMaxNumFields`
| <<spark.sql.codegen.maxFields, spark.sql.codegen.maxFields>>
a|

Used in:

* link:spark-sql-CollapseCodegenStages.adoc[CollapseCodegenStages] to control codegen
* `ParquetFileFormat` to control row batch reading

| [[windowExecBufferSpillThreshold]] `windowExecBufferSpillThreshold`
| <<spark.sql.windowExec.buffer.spill.threshold, spark.sql.windowExec.buffer.spill.threshold>>
| Used exclusively when `WindowExec` unary physical operator is link:spark-sql-SparkPlan-WindowExec.adoc#doExecute[executed].

| [[useObjectHashAggregation]] `useObjectHashAggregation`
| <<spark.sql.execution.useObjectHashAggregateExec, spark.sql.execution.useObjectHashAggregateExec>>
| Used exclusively in `Aggregation` execution planning strategy when link:spark-sql-SparkStrategy-Aggregation.adoc#AggUtils-createAggregate[selecting a physical plan].
|===

[[parameters]]
.Parameters and Hints (in alphabetical order)
[cols=",1,2",options="header",width="100%"]
|===
| Name
| Default Value
| Description

| [[spark.sql.adaptive.enabled]] `spark.sql.adaptive.enabled`
| `false`
a| Enables adaptive query execution

NOTE: Adaptive query execution is not supported for streaming Datasets and is disabled at execution.

Use <<adaptiveExecutionEnabled, adaptiveExecutionEnabled>> method to access the current value.

| [[spark.sql.autoBroadcastJoinThreshold]] `spark.sql.autoBroadcastJoinThreshold`
| `10L * 1024 * 1024`
| Maximum size (in bytes) for a table that will be broadcast to all worker nodes when performing a join.

If the size of the statistics of the logical plan of a table is at most the setting, the DataFrame is broadcast for join.

Negative values or `0` disable broadcasting.

Use <<autoBroadcastJoinThreshold, autoBroadcastJoinThreshold>> method to access the current value.

| [[spark.sql.broadcastTimeout]] `spark.sql.broadcastTimeout`
| `5 * 60`
| Timeout in seconds for the broadcast wait time in broadcast joins.

When negative, it is assumed infinite (i.e. `Duration.Inf`)

Used through <<broadcastTimeout, SQLConf.broadcastTimeout>>.

| [[spark.sql.cbo.enabled]] `spark.sql.cbo.enabled`
| `false`
a| Enables cost-based optimization (CBO) for estimation of plan statistics when enabled (i.e. `true`).

Used (through `SQLConf.cboEnabled` method) in:

* link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization (and indirectly in `StarSchemaDetection` for `reorderStarJoins`)
* link:spark-sql-Optimizer-CostBasedJoinReorder.adoc[CostBasedJoinReorder] logical plan optimization
* For link:spark-sql-LogicalPlan.adoc#computeStats[statistics estimates] in `Project`, `Filter`, link:spark-sql-LogicalPlan-Join.adoc[Join], and link:spark-sql-LogicalPlan-Aggregate.adoc[Aggregate] logical plans

| [[spark.sql.cbo.joinReorder.enabled]] `spark.sql.cbo.joinReorder.enabled`
| `false`
a| Enables join reorder for cost-based optimization (CBO).

Use <<joinReorderEnabled, joinReorderEnabled>> method to access the current value.

| [[spark.sql.cbo.starSchemaDetection]] `spark.sql.cbo.starSchemaDetection`
| `false`
a| Enables *join reordering* based on star schema detection for cost-based optimization (CBO) in link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization.

Use <<starSchemaDetection, starSchemaDetection>> method to access the current value.

| [[spark.sql.codegen.fallback]] `spark.sql.codegen.fallback`
| `true`
| *(internal)* Whether the whole stage codegen could be temporary disabled for the part of a query that has failed to compile generated code (`true`) or not (`false`).

Use <<wholeStageFallback, wholeStageFallback>> method to access the current value.

| [[spark.sql.codegen.maxFields]] `spark.sql.codegen.maxFields`
| `100`
| *(internal)* Maximum number of output fields (including nested fields) that whole-stage codegen supports. Going above the number deactivates whole-stage codegen.

Use <<wholeStageMaxNumFields, wholeStageMaxNumFields>> method to access the current value.

| [[spark.sql.codegen.wholeStage]] `spark.sql.codegen.wholeStage`
| `true`
| *(internal)* Whether the whole stage (of multiple physical operators) will be compiled into a single Java method (`true`) or not (`false`).

Use <<wholeStageEnabled, wholeStageEnabled>> method to access the current value.

| [[spark.sql.defaultSizeInBytes]] `spark.sql.defaultSizeInBytes`
| Java's `Long.MaxValue`
| *(internal)* Table size used in query planning.

It is by default set to Java's `Long.MaxValue` which is larger than <<spark.sql.autoBroadcastJoinThreshold, spark.sql.autoBroadcastJoinThreshold>> to be more conservative. That is to say by default the optimizer will not choose to broadcast a table unless it knows for sure its size is small enough.

Use <<useObjectHashAggregation, useObjectHashAggregation>> method to access the current value.

| [[spark.sql.execution.useObjectHashAggregateExec]] `spark.sql.execution.useObjectHashAggregateExec`
| `true`
| Flag to enable link:spark-sql-SparkPlan-ObjectHashAggregateExec.adoc[ObjectHashAggregateExec] in link:spark-sql-SparkStrategy-Aggregation.adoc#AggUtils-createAggregate[Aggregation] execution planning strategy.

Use <<useObjectHashAggregation, useObjectHashAggregation>> method to access the current value.

| [[spark.sql.inMemoryColumnarStorage.batchSize]] `spark.sql.inMemoryColumnarStorage.batchSize`
| `10000`
| *(internal)* Controls...FIXME

Use <<columnBatchSize, columnBatchSize>> method to access the current value.

| [[spark.sql.inMemoryColumnarStorage.compressed]] `spark.sql.inMemoryColumnarStorage.compressed`
| `true`
| *(internal)* Controls...FIXME

Use <<useCompression, useCompression>> method to access the current value.

| [[spark.sql.join.preferSortMergeJoin]] `spark.sql.join.preferSortMergeJoin`
| `true`
| *(internal)* Controls link:spark-sql-SparkStrategy-JoinSelection.adoc[JoinSelection] execution planning strategy to prefer sort merge join over shuffle hash join.

Use <<preferSortMergeJoin, preferSortMergeJoin>> method to access the current value.

| [[spark.sql.limit.scaleUpFactor]] `spark.sql.limit.scaleUpFactor`
| `4`
| *(internal)* Minimal increase rate in the number of partitions between attempts when executing `take` operator on a structured query. Higher values lead to more partitions read. Lower values might lead to longer execution times as more jobs will be run.

Use <<limitScaleUpFactor, limitScaleUpFactor>> method to access the current value.

| [[spark.sql.optimizer.maxIterations]] `spark.sql.optimizer.maxIterations`
| `100`
| Maximum number of iterations for link:spark-sql-Analyzer.adoc#fixedPoint[Analyzer] and  link:spark-sql-Optimizer.adoc#fixedPoint[Optimizer].

| [[spark.sql.pivotMaxValues]] `spark.sql.pivotMaxValues`
| `10000`
| Maximum number of (distinct) values that will be collected without error (when doing a link:spark-sql-RelationalGroupedDataset.adoc#pivot[pivot] without specifying the values for the pivot column)

Use <<dataFramePivotMaxValues, dataFramePivotMaxValues>> method to access the current value.

| [[spark.sql.retainGroupColumns]] `spark.sql.retainGroupColumns`
| `true`
| Controls whether to retain columns used for aggregation or not (in link:spark-sql-RelationalGroupedDataset.adoc[RelationalGroupedDataset] operators).

Use <<dataFrameRetainGroupColumns, dataFrameRetainGroupColumns>> method to access the current value.

| [[spark.sql.selfJoinAutoResolveAmbiguity]] `spark.sql.selfJoinAutoResolveAmbiguity`
| `true`
| Control whether to resolve ambiguity in join conditions for link:spark-sql-joins.adoc#join[self-joins] automatically.

| [[spark.sql.shuffle.partitions]] `spark.sql.shuffle.partitions`
| `200`
| Default number of partitions to use when shuffling data for joins or aggregations.

Corresponds to Apache Hive's https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-mapred.reduce.tasks[mapred.reduce.tasks] property that Spark considers deprecated.

Use <<numShufflePartitions, numShufflePartitions>> method to access the current value.

| [[spark.sql.streaming.fileSink.log.deletion]] `spark.sql.streaming.fileSink.log.deletion`
| `true`
| Controls whether to delete the expired log files in link:spark-sql-streaming-sink.adoc#FileStreamSink[file stream sink].

| [[spark.sql.streaming.fileSink.log.cleanupDelay]] `spark.sql.streaming.fileSink.log.cleanupDelay`
| FIXME
| FIXME

| [[spark.sql.streaming.schemaInference]] `spark.sql.streaming.schemaInference`
| FIXME
| FIXME

| [[spark.sql.streaming.fileSink.log.compactInterval]] `spark.sql.streaming.fileSink.log.compactInterval`
| FIXME
| FIXME

| [[spark.sql.windowExec.buffer.spill.threshold]] `spark.sql.windowExec.buffer.spill.threshold`
| `4096`
| *(internal)* Threshold for number of rows buffered in link:spark-sql-SparkPlan-WindowExec.adoc[window operator]

Use <<windowExecBufferSpillThreshold, windowExecBufferSpillThreshold>> method to access the current value.
|===

NOTE: `SQLConf` is a `private[sql]` serializable class in `org.apache.spark.sql.internal` package.

=== [[get]] Getting Parameters and Hints

You can get the current parameters and hints using the following family of `get` methods.

[source, scala]
----
getConfString(key: String): String
getConf[T](entry: ConfigEntry[T], defaultValue: T): T
getConf[T](entry: ConfigEntry[T]): T
getConf[T](entry: OptionalConfigEntry[T]): Option[T]
getConfString(key: String, defaultValue: String): String
getAllConfs: immutable.Map[String, String]
getAllDefinedConfs: Seq[(String, String, String)]
----

=== [[set]] Setting Parameters and Hints

You can set parameters and hints using the following family of `set` methods.

[source, scala]
----
setConf(props: Properties): Unit
setConfString(key: String, value: String): Unit
setConf[T](entry: ConfigEntry[T], value: T): Unit
----

=== [[unset]] Unsetting Parameters and Hints

You can unset parameters and hints using the following family of `unset` methods.

[source, scala]
----
unsetConf(key: String): Unit
unsetConf(entry: ConfigEntry[_]): Unit
----

=== [[clear]] Clearing All Parameters and Hints

[source, scala]
----
clear(): Unit
----

You can use `clear` to remove all the parameters and hints in `SQLConf`.
