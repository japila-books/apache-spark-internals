# Management Scripts for Standalone Master

You can start a [Spark Standalone](index.md) *master* (aka _standalone Master_) using <<start-script, sbin/start-master.sh>> and stop it using <<stop-script, sbin/stop-master.sh>>.

## sbin/start-master.sh { #start-script }

`sbin/start-master.sh` script starts a Spark master on the machine the script is executed on.

```bash
./sbin/start-master.sh
```

The script prepares the command line to start the class `org.apache.spark.deploy.master.Master` and by default runs as follows:

```bash
org.apache.spark.deploy.master.Master \
  --ip japila.local --port 7077 --webui-port 8080
```

!!! note
    The command sets `SPARK_PRINT_LAUNCH_COMMAND` environment variable to print out the launch command to standard error output.
    
    Learn more in [Print Launch Command of Spark Scripts](../tips-and-tricks/index.md#SPARK_PRINT_LAUNCH_COMMAND).

It has support for starting Tachyon using `--with-tachyon` command line option. It assumes `tachyon/bin/tachyon` command be available in Spark's home directory.

The script uses the following helper scripts:

* `sbin/spark-config.sh`
* `bin/load-spark-env.sh`
* `conf/spark-env.sh` contains environment variables of a Spark executable.

Ultimately, the script calls `sbin/spark-daemon.sh start` to kick off `org.apache.spark.deploy.master.Master` with parameter `1` and `--ip`, `--port`, and `--webui-port` <<start-options, command-line options>>.

## Command-line Options { #start-options }

You can use the following command-line options:

* `--host` or `-h` the hostname to listen on; overrides <<spark-standalone.md#environment-variables, SPARK_MASTER_HOST>>.
* `--ip` or `-i` (deprecated) the IP to listen on
* `--port` or `-p` - command-line version of <<spark-standalone.md#environment-variables, SPARK_MASTER_PORT>> that overrides it.
* `--webui-port` - command-line version of <<spark-standalone.md#environment-variables, SPARK_MASTER_WEBUI_PORT>> that overrides it.
* `--properties-file` (default: `$SPARK_HOME/conf/spark-defaults.conf`) - the path to a custom Spark properties file. Refer to link:spark-properties.md#spark-defaults-conf[spark-defaults.conf].
* `--help` - prints out help

## sbin/stop-master.sh { #stop-script }

You can stop a Spark Standalone master using `sbin/stop-master.sh` script.

```text
./sbin/stop-master.sh
```

It effectively sends SIGTERM to the master's process.

You should see the ERROR in master's logs:

```text
ERROR Master: RECEIVED SIGNAL 15: SIGTERM
```
