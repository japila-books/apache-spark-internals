= DiskStore

*DiskStore* manages data blocks on disk for xref:storage:BlockManager.adoc#diskStore[BlockManager].

.DiskStore and BlockManager
image::DiskStore-BlockManager.png[align="center"]

== [[creating-instance]] Creating Instance

DiskStore takes the following to be created:

* [[conf]] xref:ROOT:SparkConf.adoc[]
* [[diskManager]] xref:storage:DiskBlockManager.adoc[]
* [[securityManager]] SecurityManager

== [[blockSizes]] blockSizes Internal Registry

[source, scala]
----
blockSizes: ConcurrentHashMap[BlockId, Long]
----

blockSizes is a Java {java-javadoc-url}/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap] that DiskStore uses to track xref:storage:BlockId.adoc[]s by their size on disk.

== [[contains]] Checking if Block File Exists

[source, scala]
----
contains(
  blockId: BlockId): Boolean
----

`contains` requests the <<diskManager, DiskBlockManager>> for the xref:storage:DiskBlockManager.adoc#getFile[block file] by (the name of) the input xref:storage:BlockId.adoc[] and check whether the file actually exists or not.

`contains` is used when:

* BlockManager is requested to xref:storage:BlockManager.adoc#getStatus[getStatus], xref:storage:BlockManager.adoc#getCurrentBlockStatus[getCurrentBlockStatus], xref:storage:BlockManager.adoc#getLocalValues[getLocalValues], xref:storage:BlockManager.adoc#doGetLocalBytes[doGetLocalBytes], xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory]

* DiskStore is requested to <<put, put>>

== [[put]] Writing Block to Disk

[source, scala]
----
put(
  blockId: BlockId)(
  writeFunc: WritableByteChannel => Unit): Unit
----

`put` prints out the following DEBUG message to the logs:

```
Attempting to put block [blockId]
```

`put` requests the <<diskManager, DiskBlockManager>> for the xref:storage:DiskBlockManager.adoc#getFile[block file] for the input xref:storage:BlockId.adoc[].

`put` <<openForWrite, opens the block file for writing>> (wrapped into a CountingWritableChannel to count the bytes written).

`put` executes the given writeFunc function with the WritableByteChannel of the block file and registers the bytes written to the <<blockSizes, blockSizes>> internal registry.

In the end, `put` prints out the following DEBUG message to the logs:

```
Block [fileName] stored as [size] file on disk in [time] ms
```

In case of any exception, `put` <<remove, deletes the block file>>.

`put` throws an `IllegalStateException` when the BlockId is already <<contains, is already present in the disk store>>:

```
Block [blockId] is already present in the disk store
```

`put` is used when:

* BlockManager is requested to xref:storage:BlockManager.adoc#doPutIterator[doPutIterator] and xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory]

* DiskStore is requested to <<putBytes, putBytes>>

== [[putBytes]] putBytes Method

[source, scala]
----
putBytes(
  blockId: BlockId,
  bytes: ChunkedByteBuffer): Unit
----

`putBytes`...FIXME

`putBytes` is used when BlockManager is requested to xref:storage:BlockManager.adoc#doPutBytes[doPutBytes] and xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory].

== [[remove]] Removing Block

[source, scala]
----
remove(
  blockId: BlockId): Boolean
----

`remove`...FIXME

`remove` is used when:

* BlockManager is requested to xref:storage:BlockManager.adoc#removeBlockInternal[removeBlockInternal]

* DiskStore is requested to <<put, put>> (when an exception was thrown)

== [[openForWrite]] Opening Block File For Writing

[source, scala]
----
openForWrite(
  file: File): WritableByteChannel
----

`openForWrite`...FIXME

`openForWrite` is used when DiskStore is requested to <<put, write a block to disk>>.

== [[logging]] Logging

Enable `ALL` logging level for `org.apache.spark.storage.DiskStore` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

[source]
----
log4j.logger.org.apache.spark.storage.DiskStore=ALL
----

Refer to xref:ROOT:spark-logging.adoc[Logging].
