= [[DiskBlockManager]] DiskBlockManager

*DiskBlockManager* creates and maintains the logical mapping between logical blocks and physical on-disk locations.

By default, one block is mapped to one file with a name given by its `BlockId`. It is however possible to have a block map to only a segment of a file.

Block files are hashed among the <<getConfiguredLocalDirs, local directories>>.

DiskBlockManager is <<creating-instance, created>> for a xref:storage:BlockManager.adoc#diskBlockManager[BlockManager]

DiskBlockManager is used to create a xref:DiskStore.adoc[DiskStore].

== [[creating-instance]] Creating Instance

* [[conf]] xref:ROOT:spark-SparkConf.adoc[SparkConf]
* [[deleteFilesOnStop]] `deleteFilesOnStop` flag

When created, DiskBlockManager uses xref:ROOT:spark-configuration-properties.adoc#spark.diskStore.subDirectories[spark.diskStore.subDirectories] configuration property to set <<subDirsPerLocalDir, subDirsPerLocalDir>>.

DiskBlockManager <<createLocalDirs, creates one or many local directories to store block data>> (as <<localDirs, localDirs>>). When not successful, you should see the following ERROR message in the logs and DiskBlockManager exits with error code `53`.

```
Failed to create any local dir.
```

DiskBlockManager initializes the internal <<subDirs, subDirs>> collection of locks for every local directory to store block data with an array of `subDirsPerLocalDir` size for files.

In the end, DiskBlockManager <<addShutdownHook, registers a shutdown hook>> to clean up the local directories for blocks.

== [[getFile]] Finding Block File

[source, scala]
----
getFile(
  blockId: BlockId): File // <1>
getFile(
  filename: String): File
----
<1> Uses the name of the given `BlockId`

`getFile`...FIXME

`getFile` is used when:

* DiskBlockManager is requested to <<containsBlock, containsBlock>>, <<createTempLocalBlock, createTempLocalBlock>>, <<createTempShuffleBlock, createTempShuffleBlock>>

* DiskStore is requested to xref:DiskStore.adoc#getBytes[getBytes], xref:DiskStore.adoc#remove[remove], xref:DiskStore.adoc#contains[contains], and xref:DiskStore.adoc#put[put]

* IndexShuffleBlockResolver is requested to xref:shuffle:IndexShuffleBlockResolver.adoc#getDataFile[getDataFile] and xref:shuffle:IndexShuffleBlockResolver.adoc#getIndexFile[getIndexFile]

== [[createTempShuffleBlock]] `createTempShuffleBlock` Method

[source, scala]
----
createTempShuffleBlock(): (TempShuffleBlockId, File)
----

`createTempShuffleBlock` creates a temporary `TempShuffleBlockId` block.

CAUTION: FIXME

== [[getAllFiles]] `getAllFiles` Method

[source, scala]
----
getAllFiles(): Seq[File]
----

`getAllFiles`...FIXME

NOTE: `getAllFiles` is used exclusively when DiskBlockManager is requested to <<getAllBlocks, getAllBlocks>>.

== [[addShutdownHook]] Registering Shutdown Hook -- `addShutdownHook` Internal Method

[source, scala]
----
addShutdownHook(): AnyRef
----

`addShutdownHook` registers a shutdown hook to execute <<doStop, doStop>> at shutdown.

When executed, you should see the following DEBUG message in the logs:

```
DEBUG DiskBlockManager: Adding shutdown hook
```

`addShutdownHook` adds the shutdown hook so it prints the following INFO message and executes <<doStop, doStop>>.

```
INFO DiskBlockManager: Shutdown hook called
```

== [[doStop]] Stopping DiskBlockManager (Removing Local Directories for Blocks) -- `doStop` Internal Method

[source, scala]
----
doStop(): Unit
----

`doStop` deletes the local directories recursively (only when the constructor's `deleteFilesOnStop` is enabled and the parent directories are not registered to be removed at shutdown).

NOTE: `doStop` is used when DiskBlockManager is requested to <<addShutdownHook, shut down>> or <<stop, stop>>.

== [[getConfiguredLocalDirs]] Getting Local Directories for Spark to Write Files -- `Utils.getConfiguredLocalDirs` Internal Method

[source, scala]
----
getConfiguredLocalDirs(conf: SparkConf): Array[String]
----

`getConfiguredLocalDirs` returns the local directories where Spark can write files.

Internally, `getConfiguredLocalDirs` uses `conf` link:spark-SparkConf.adoc[SparkConf] to know if link:spark-ExternalShuffleService.adoc[External Shuffle Service] is enabled (using link:spark-ExternalShuffleService.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled]).

`getConfiguredLocalDirs` checks if <<isRunningInYarnContainer, Spark runs on YARN>> and if so, returns <<getYarnLocalDirs, ``LOCAL_DIRS``-controlled local directories>>.

In non-YARN mode (or for the driver in yarn-client mode), `getConfiguredLocalDirs` checks the following environment variables (in the order) and returns the value of the first met:

1. `SPARK_EXECUTOR_DIRS` environment variable
2. `SPARK_LOCAL_DIRS` environment variable
3. `MESOS_DIRECTORY` environment variable (only when External Shuffle Service is not used)

In the end, when no earlier environment variables were found, `getConfiguredLocalDirs` uses link:spark-properties.adoc#spark.local.dir[spark.local.dir] Spark property or falls back on `java.io.tmpdir` System property.

[NOTE]
====
`getConfiguredLocalDirs` is used when:

* DiskBlockManager is requested to <<createLocalDirs, createLocalDirs>>

* `Utils` helper is requested to link:spark-Utils.adoc#getLocalDir[getLocalDir] and link:spark-Utils.adoc#getOrCreateLocalRootDirsImpl[getOrCreateLocalRootDirsImpl]
====

== [[getYarnLocalDirs]] Getting Writable Directories in YARN -- `getYarnLocalDirs` Internal Method

[source, scala]
----
getYarnLocalDirs(conf: SparkConf): String
----

`getYarnLocalDirs` uses `conf` link:spark-SparkConf.adoc[SparkConf] to read `LOCAL_DIRS` environment variable with comma-separated local directories (that have already been created and secured so that only the user has access to them).

`getYarnLocalDirs` throws an `Exception` with the message `Yarn Local dirs can't be empty` if `LOCAL_DIRS` environment variable was not set.

== [[isRunningInYarnContainer]] Checking If Spark Runs on YARN -- `isRunningInYarnContainer` Internal Method

[source, scala]
----
isRunningInYarnContainer(conf: SparkConf): Boolean
----

`isRunningInYarnContainer` uses `conf` link:spark-SparkConf.adoc[SparkConf] to read Hadoop YARN's link:http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-api/apidocs/org/apache/hadoop/yarn/api/ApplicationConstants.Environment.html#CONTAINER_ID[`CONTAINER_ID` environment variable] to find out if Spark runs in a YARN container.

NOTE: `CONTAINER_ID` environment variable is exported by YARN NodeManager.

== [[getAllBlocks]] Getting All Blocks Stored On Disk -- `getAllBlocks` Method

[source, scala]
----
getAllBlocks(): Seq[BlockId]
----

`getAllBlocks` gets all the blocks stored on disk.

Internally, `getAllBlocks` takes the <<getAllFiles, block files>> and returns their names (as `BlockId`).

NOTE: `getAllBlocks` is used exclusively when `BlockManager` is requested to xref:ROOT:BlockManager.adoc#getMatchingBlockIds[find IDs of existing blocks for a given filter].

== [[createLocalDirs]] Creating Local Directories for Storing Block Data -- `createLocalDirs` Internal Method

[source, scala]
----
createLocalDirs(conf: SparkConf): Array[File]
----

`createLocalDirs` creates `blockmgr-[random UUID]` directory under local directories to store block data.

Internally, `createLocalDirs` reads <<getConfiguredLocalDirs, local writable directories>> and creates a subdirectory `blockmgr-[random UUID]` under every configured parent directory.

If successful, you should see the following INFO message in the logs:

```
INFO DiskBlockManager: Created local directory at [localDir]
```

When failed to create a local directory, you should see the following ERROR message in the logs:

```
ERROR DiskBlockManager: Failed to create local dir in [rootDir]. Ignoring this directory.
```

NOTE: `createLocalDirs` is used exclusively when <<localDirs, localDirs>> is initialized.

== [[stop]] `stop` Internal Method

[source, scala]
----
stop(): Unit
----

`stop`...FIXME

NOTE: `stop` is used exclusively when `BlockManager` is requested to xref:ROOT:BlockManager.adoc#stop[stop].

== [[subDirs]] File Locks for Local Block Store Directories -- `subDirs` Internal Property

[source, scala]
----
subDirs: Array[Array[File]]
----

`subDirs` is a collection of <<subDirsPerLocalDir, subDirsPerLocalDir>> file locks for every <<createLocalDirs, local block store directory>> where DiskBlockManager stores block data (with the columns being the number of local directories and the rows as collection of `subDirsPerLocalDir` size).

NOTE: `subDirs(n)` is to access ``n``-th local directory.

NOTE: `subDirs` is used when DiskBlockManager is requested to <<getFile, getFile>> or <<getAllFiles, getAllFiles>>.

== [[logging]] Logging

Enable `ALL` logging level for `org.apache.spark.storage.DiskBlockManager` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

[source]
----
log4j.logger.org.apache.spark.storage.DiskBlockManager=ALL
----

Refer to xref:ROOT:spark-logging.adoc[Logging].

== [[internal-registries]] Internal Properties

.DiskBlockManager's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,3",options="header",width="100%"]
|===
| Name
| Description

| localDirs
a| [[localDirs]] Local directories for block data

`localDirs` is initialized using <<createLocalDirs, createLocalDirs>>.

There has to be at least one local directory or DiskBlockManager cannot be <<creating-instance, created>>.

Used when:

* DiskBlockManager is requested to <<getFile, getFile>>, initialize <<subDirs, subDirs>> and <<doStop, stop>>

* `BlockManager` is requested to xref:ROOT:BlockManager.adoc#registerWithExternalShuffleServer[register the executor's BlockManager with an external shuffle server]

* PySpark's `BasePythonRunner` is requested to `compute`

| subDirsPerLocalDir
a| [[subDirsPerLocalDir]] <<spark-configuration-properties.adoc#spark.diskStore.subDirectories, spark.diskStore.subDirectories>> configuration property (default: `64`)

Used when:

* DiskBlockManager is <<subDirs, created>> and is requested to <<getFile, getFile>>

* `BlockManager` is requested to xref:ROOT:BlockManager.adoc#registerWithExternalShuffleServer[register the executor's BlockManager with an external shuffle server]
|===
