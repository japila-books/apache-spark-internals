= BlockManager

*BlockManager* is a block storage of blocks of data identified by a block ID.

BlockManager runs on every node in a Spark application, i.e. the xref:ROOT:spark-driver.adoc[driver] and xref:executor:Executor.adoc[executors].

[[BlockDataManager]]
BlockManager is a <<spark-BlockDataManager.adoc#, BlockDataManager>> and manages the storage for blocks that with cached RDD partitions, intermediate shuffle outputs, broadcasts, etc.

BlockManager is a local cache.

BlockManager provides interface for uploading and fetching blocks both locally and remotely using various stores, i.e. <<stores, memory, disk, and off-heap>>.

BlockManager is <<creating-instance, created>> exclusively when `SparkEnv` is xref:core:SparkEnv.adoc#create-BlockManager[created] (for the driver and executors). BlockManager gets a <<diskBlockManager, DiskBlockManager>>, a <<blockInfoManager, BlockInfoManager>>, a <<memoryStore, MemoryStore>> and a <<diskStore, DiskStore>> (and wires them up together).

[[futureExecutionContext]]
BlockManager uses a Scala https://www.scala-lang.org/api/current/scala/concurrent/ExecutionContextExecutorService.html[ExecutionContextExecutorService] to execute *FIXME* asynchronously (on a thread pool with *block-manager-future* prefix and maximum of 128 threads).

[[BlockEvictionHandler]]
BlockManager is a link:spark-BlockEvictionHandler.adoc[BlockEvictionHandler] that can <<dropFromMemory, drop a block from memory and store it on a disk>> if required.

*Cached blocks* are blocks with non-zero sum of memory and disk sizes.

TIP: Use xref:webui:index.adoc[Web UI], esp. xref:webui:spark-webui-storage.adoc[Storage] and xref:webui:spark-webui-executors.adoc[Executors] tabs, to monitor the memory used.

TIP: Use xref:tools:spark-submit.adoc[spark-submit]'s command-line options, i.e. xref:tools:spark-submit.adoc#driver-memory[--driver-memory] for the driver and xref:tools:spark-submit.adoc#executor-memory[--executor-memory] for executors or their equivalents as Spark properties, i.e. xref:tools:spark-submit.adoc#spark.executor.memory[spark.executor.memory] and xref:tools:spark-submit.adoc#spark_driver_memory[spark.driver.memory], to control the memory for storage memory.

A <<creating-instance, BlockManager is created>> when a xref:core:SparkEnv.adoc#create[Spark application starts] and must be <<initialize, initialized>> before it is fully operable.

When <<externalShuffleServiceEnabled, External Shuffle Service is enabled>>, BlockManager uses xref:core:ExternalShuffleClient.adoc[ExternalShuffleClient] to read other executors' shuffle files.

== [[creating-instance]] Creating Instance

BlockManager takes the following to be created:

* [[executorId]] Executor ID
* [[rpcEnv]] xref:ROOT:index.adoc[RpcEnv]
* [[master]] xref:BlockManagerMaster.adoc[BlockManagerMaster]
* [[serializerManager]] xref:serializer:SerializerManager.adoc[SerializerManager]
* [[conf]] xref:ROOT:SparkConf.adoc[SparkConf]
* <<memoryManager, MemoryManager>>
* [[mapOutputTracker]] xref:scheduler:MapOutputTracker.adoc[MapOutputTracker]
* <<shuffleManager, ShuffleManager>>
* <<blockTransferService, BlockTransferService>>
* [[securityManager]] SecurityManager
* [[numUsableCores]] CPU cores

NOTE: `executorId` is `SparkContext.DRIVER_IDENTIFIER`, i.e. `driver` for the driver, and the value of xref:executor:CoarseGrainedExecutorBackend.adoc#executor-id[--executor-id] command-line argument for xref:executor:CoarseGrainedExecutorBackend.adoc[] executors (or xref:spark-on-mesos:spark-executor-backends-MesosExecutorBackend.adoc[MesosExecutorBackend]).

When created, BlockManager sets <<externalShuffleServiceEnabled, externalShuffleServiceEnabled>> internal flag based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property.

BlockManager then creates an instance of xref:DiskBlockManager.adoc[DiskBlockManager] (requesting `deleteFilesOnStop` when an external shuffle service is not in use).

BlockManager creates an instance of link:spark-BlockInfoManager.adoc[BlockInfoManager] (as `blockInfoManager`).

BlockManager creates *block-manager-future* daemon cached thread pool with 128 threads maximum (as `futureExecutionContext`).

BlockManager calculates the maximum memory to use (as `maxMemory`) by requesting the maximum xref:memory:MemoryManager.adoc#maxOnHeapStorageMemory[on-heap] and xref:memory:MemoryManager.adoc#maxOffHeapStorageMemory[off-heap] storage memory from the assigned `MemoryManager`.

BlockManager calculates the port used by the external shuffle service (as `externalShuffleServicePort`).

NOTE: It is computed specially in Spark on YARN.

BlockManager creates a client to read other executors' shuffle files (as `shuffleClient`). If the external shuffle service is used an xref:core:ExternalShuffleClient.adoc[ExternalShuffleClient] is created or the input xref:core:BlockTransferService.adoc[BlockTransferService] is used.

BlockManager sets the xref:ROOT:configuration-properties.adoc#spark.block.failures.beforeLocationRefresh[maximum number of failures] before this block manager refreshes the block locations from the driver (as `maxFailuresBeforeLocationRefresh`).

BlockManager registers link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc[BlockManagerSlaveEndpoint] with the input xref:ROOT:index.adoc[RpcEnv], itself, and xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] (as `slaveEndpoint`).

== [[slaveEndpoint]] BlockManagerEndpoint RPC Endpoint

When created, BlockManager requests the <<rpcEnv, RpcEnv>> to xref:rpc:RpcEnv.adoc#setupEndpoint[register BlockManagerEndpoint RPC endpoint].

== [[SparkEnv]] Accessing BlockManager Using SparkEnv

BlockManager is available using xref:core:SparkEnv.adoc#blockManager[SparkEnv] on the driver and executors.

[source, scala]
----
import org.apache.spark.SparkEnv
SparkEnv.get.blockManager
----

== [[blockTransferService]] BlockTransferService

BlockManager is given a xref:core:BlockTransferService.adoc[BlockTransferService] to be created.

BlockTransferService is used as the <<shuffleClient, ShuffleClient>> when BlockManager is configured to use no external shuffle service (based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property).

BlockTransferService is xref:core:BlockTransferService.adoc#init[initialized] when BlockManager <<initialize, is>>.

BlockTransferService is xref:core:BlockTransferService.adoc#close[closed] when BlockManager is requested to <<stop, stop>>.

BlockTransferService is used when BlockManager is requested to <<getRemoteBytes, fetching a block from>> or <<replicate, replicate a block to>> remote block managers.

== [[memoryManager]] MemoryManager

BlockManager is given a xref:memory:MemoryManager.adoc[MemoryManager] to be <<creating-instance, created>>.

BlockManager uses the MemoryManager for the following:

* Create the <<memoryStore, MemoryStore>> (that is then assigned to xref:memory:MemoryManager.adoc#setMemoryStore[MemoryManager] as a "circular dependency")

* Initialize <<maxOnHeapMemory, maxOnHeapMemory>> and <<maxOffHeapMemory, maxOffHeapMemory>> (for reporting)

== [[shuffleManager]] ShuffleManager

BlockManager is given a xref:shuffle:ShuffleManager.adoc[ShuffleManager] when <<creating-instance, created>>.

BlockManager uses the ShuffleManager for the following:

* <<getBlockData, Retrieving a block data>> (for shuffle blocks)

* <<getLocalBytes, Retrieving a non-shuffle block data>> (for shuffle blocks anyway)

* <<registerWithExternalShuffleServer, Registering an executor with a local external shuffle service>> (when <<initialize, initialized>> on an executor with <<externalShuffleServiceEnabled, externalShuffleServiceEnabled>>)

== [[diskBlockManager]] DiskBlockManager

When <<creating-instance, created>>, BlockManager creates a xref:DiskBlockManager.adoc[DiskBlockManager].

.DiskBlockManager and BlockManager
image::DiskBlockManager-BlockManager.png[align="center"]

BlockManager uses the BlockManager for the following:

* Creating a <<diskStore, DiskStore>>

* <<registerWithExternalShuffleServer, Registering an executor with a local external shuffle service>> (when <<initialize, initialized>> on an executor with <<externalShuffleServiceEnabled, externalShuffleServiceEnabled>>)

The BlockManager is available as `diskBlockManager` reference to other Spark systems.

[source, scala]
----
import org.apache.spark.SparkEnv
SparkEnv.get.blockManager.diskBlockManager
----

== [[memoryStore]] MemoryStore

When <<creating-instance, created>>, BlockManager creates a xref:MemoryStore.adoc[MemoryStore] (with the <<blockInfoManager, BlockInfoManager>>, the <<serializerManager, SerializerManager>>, the <<memoryManager, MemoryManager>> and itself as a xref:spark-BlockEvictionHandler.adoc[BlockEvictionHandler]).

.MemoryStore and BlockManager
image::MemoryStore-BlockManager.png[align="center"]

BlockManager uses the MemoryStore when requested to <<doGetLocalBytes, doGetLocalBytes>>.

The MemoryStore is available as `memoryStore` reference to other Spark systems.

[source, scala]
----
import org.apache.spark.SparkEnv
SparkEnv.get.blockManager.memoryStore
----

== [[diskStore]] DiskStore

When <<creating-instance, created>>, BlockManager creates a xref:DiskStore.adoc[DiskStore] (with the <<diskBlockManager, DiskBlockManager>>).

.DiskStore and BlockManager
image::DiskStore-BlockManager.png[align="center"]

BlockManager uses the DiskStore when requested to <<getStatus, getStatus>>, <<getCurrentBlockStatus, getCurrentBlockStatus>>, <<getLocalValues, getLocalValues>>, <<doGetLocalBytes, doGetLocalBytes>>, <<doPutBytes, doPutBytes>>, <<doPutIterator, doPutIterator>>, <<dropFromMemory, dropFromMemory>>, <<removeBlockInternal, removeBlockInternal>>.

== [[metrics]] Performance Metrics

BlockManager uses link:spark-BlockManager-BlockManagerSource.adoc[BlockManagerSource] to report metrics under the name *BlockManager*.

== [[getLocations]] `getLocations` Method

CAUTION: FIXME

== [[blockIdsToHosts]] `blockIdsToHosts` Method

CAUTION: FIXME

== [[getLocationBlockIds]] `getLocationBlockIds` Method

CAUTION: FIXME

== [[getPeers]] `getPeers` Method

CAUTION: FIXME

== [[releaseAllLocksForTask]] `releaseAllLocksForTask` Method

CAUTION: FIXME

== [[stop]] Stopping BlockManager

[source, scala]
----
stop(): Unit
----

`stop`...FIXME

NOTE: `stop` is used exclusively when `SparkEnv` is requested to xref:core:SparkEnv.adoc#stop[stop].

== [[getMatchingBlockIds]] Getting IDs of Existing Blocks (For a Given Filter)

[source, scala]
----
getMatchingBlockIds(filter: BlockId => Boolean): Seq[BlockId]
----

`getMatchingBlockIds`...FIXME

NOTE: `getMatchingBlockIds` is used exclusively when `BlockManagerSlaveEndpoint` is requested to link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc#GetMatchingBlockIds[handle a GetMatchingBlockIds message].

== [[getLocalValues]] getLocalValues Method

[source, scala]
----
getLocalValues(
  blockId: BlockId): Option[BlockResult]
----

`getLocalValues` prints out the following DEBUG message to the logs:

```
Getting local block [blockId]
```

`getLocalValues` link:spark-BlockInfoManager.adoc#lockForReading[obtains a read lock for `blockId`].

When no `blockId` block was found, you should see the following DEBUG message in the logs and `getLocalValues` returns "nothing" (i.e. `NONE`).

```
Block [blockId] was not found
```

When the `blockId` block was found, you should see the following DEBUG message in the logs:

```
Level for block [blockId] is [level]
```

If `blockId` block has memory level and xref:storage:MemoryStore.adoc#contains[is registered in `MemoryStore`], `getLocalValues` returns a <<BlockResult, BlockResult>> as `Memory` read method and with a `CompletionIterator` for an interator:

1. xref:storage:MemoryStore.adoc#getValues[Values iterator from `MemoryStore` for `blockId`] for "deserialized" persistence levels.
2. Iterator from xref:serializer:SerializerManager.adoc#dataDeserializeStream[`SerializerManager` after the data stream has been deserialized] for the `blockId` block and xref:storage:MemoryStore.adoc#getBytes[the bytes for `blockId` block] for "serialized" persistence levels.

NOTE: `getLocalValues` is used when xref:core:TorrentBroadcast.adoc#readBroadcastBlock[`TorrentBroadcast` reads the blocks of a broadcast variable and stores them in a local BlockManager].

CAUTION: FIXME

== [[getRemoteValues]] `getRemoteValues` Internal Method

[source, scala]
----
getRemoteValues[T: ClassTag](blockId: BlockId): Option[BlockResult]
----

`getRemoteValues`...FIXME

== [[get]] Retrieving Block from Local or Remote Block Managers

[source, scala]
----
get[T: ClassTag](blockId: BlockId): Option[BlockResult]
----

`get` attempts to get the `blockId` block from a local block manager first before requesting it from remote block managers.

Internally, `get` tries to <<getLocalValues, get the block from the local BlockManager>>. If the block was found, you should see the following INFO message in the logs and `get` returns the local <<BlockResult, BlockResult>>.

```
INFO Found block [blockId] locally
```

If however the block was not found locally, `get` tries to <<getRemoteValues, get the block from remote block managers>>. If retrieved from a remote block manager, you should see the following INFO message in the logs and `get` returns the remote <<BlockResult, BlockResult>>.

```
INFO Found block [blockId] remotely
```

In the end, `get` returns "nothing" (i.e. `NONE`) when the `blockId` block was not found either in the local BlockManager or any remote BlockManager.

[NOTE]
====
`get` is used when:

* BlockManager is requested to <<getOrElseUpdate, getOrElseUpdate>> and <<getSingle, getSingle>>
====

== [[getBlockData]] Retrieving Block Data

[source, scala]
----
getBlockData(
  blockId: BlockId): ManagedBuffer
----

NOTE: `getBlockData` is part of the xref:spark-BlockDataManager.adoc#getBlockData[BlockDataManager] contract.

For a xref:spark-BlockId.adoc[BlockId] of a shuffle (a ShuffleBlockId), getBlockData requests the <<shuffleManager, ShuffleManager>> for the xref:shuffle:ShuffleManager.adoc#shuffleBlockResolver[ShuffleBlockResolver] that is then requested for xref:shuffle:ShuffleBlockResolver.adoc#getBlockData[getBlockData].

Otherwise, getBlockData <<getLocalBytes, getLocalBytes>> for the given BlockId.

If found, getBlockData creates a new BlockManagerManagedBuffer (with the <<blockInfoManager, BlockInfoManager>>, the input BlockId, the retrieved BlockData and the dispose flag enabled).

If not found, getBlockData <<reportBlockStatus, informs the BlockManagerMaster>> that the block could not be found (and that the master should no longer assume the block is available on this executor) and throws a BlockNotFoundException.

NOTE: `getBlockData` is executed for shuffle blocks or local blocks that the BlockManagerMaster knows this executor really has (unless BlockManagerMaster is outdated).

== [[getLocalBytes]] Retrieving Non-Shuffle Local Block Data

[source, scala]
----
getLocalBytes(
  blockId: BlockId): Option[BlockData]
----

`getLocalBytes`...FIXME

[NOTE]
====
`getLocalBytes` is used when:

* TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#readBlocks[readBlocks]

* BlockManager is requested for the <<getBlockData, block data>> (of a non-shuffle block)
====

== [[removeBlockInternal]] `removeBlockInternal` Method

CAUTION: FIXME

== [[externalShuffleServiceEnabled]] Is External Shuffle Service Enabled?

When the xref:ROOT:ExternalShuffleService.adoc[External Shuffle Service] is enabled for a Spark application, BlockManager uses xref:core:ExternalShuffleClient.adoc[ExternalShuffleClient] to read other executors' shuffle files.

CAUTION: FIXME How is `shuffleClient` used?

== [[stores]] Stores

A *Store* is the place where blocks are held.

There are the following possible stores:

* xref:storage:MemoryStore.adoc[MemoryStore] for memory storage level.
* xref:DiskStore.adoc[DiskStore] for disk storage level.
* `ExternalBlockStore` for OFF_HEAP storage level.

== [[putBlockData]] Storing Block Data Locally

[source, scala]
----
putBlockData(
  blockId: BlockId,
  data: ManagedBuffer,
  level: StorageLevel,
  classTag: ClassTag[_]): Boolean
----

`putBlockData` simply <<putBytes, stores `blockId` locally>> (given the given storage `level`).

NOTE: `putBlockData` is part of the link:spark-BlockDataManager.adoc#putBlockData[BlockDataManager Contract].

Internally, `putBlockData` wraps `ChunkedByteBuffer` around `data` buffer's NIO `ByteBuffer` and calls <<putBytes, putBytes>>.

== [[putBytes]] Storing Block Bytes Locally

[source, scala]
----
putBytes(
  blockId: BlockId,
  bytes: ChunkedByteBuffer,
  level: StorageLevel,
  tellMaster: Boolean = true): Boolean
----

`putBytes` makes sure that the `bytes` are not `null` and <<doPutBytes, doPutBytes>>.

[NOTE]
====
`putBytes` is used when:

* BlockManager is requested to <<putBlockData, puts a block data locally>>

* `TaskRunner` is requested to xref:executor:TaskRunner.adoc#run-result-sent-via-blockmanager[run] (and the result size is above xref:executor:Executor.adoc#maxDirectResultSize[maxDirectResultSize])

* `TorrentBroadcast` is requested to xref:core:TorrentBroadcast.adoc#writeBlocks[writeBlocks] and xref:core:TorrentBroadcast.adoc#readBlocks[readBlocks]
====

=== [[doPutBytes]] `doPutBytes` Internal Method

[source, scala]
----
doPutBytes[T](
  blockId: BlockId,
  bytes: ChunkedByteBuffer,
  level: StorageLevel,
  classTag: ClassTag[T],
  tellMaster: Boolean = true,
  keepReadLock: Boolean = false): Boolean
----

`doPutBytes` calls the internal helper <<doPut, doPut>> with a function that accepts a `BlockInfo` and does the uploading.

Inside the function, if the xref:storage:StorageLevel.adoc[storage `level`]'s replication is greater than 1, it immediately starts <<replicate, replication>> of the `blockId` block on a separate thread (from `futureExecutionContext` thread pool). The replication uses the input `bytes` and `level` storage level.

For a memory storage level, the function checks whether the storage `level` is deserialized or not. For a deserialized storage `level`, ``BlockManager``'s xref:serializer:SerializerManager.adoc#dataDeserializeStream[`SerializerManager` deserializes `bytes` into an iterator of values] that xref:storage:MemoryStore.adoc#putIteratorAsValues[`MemoryStore` stores]. If however the storage `level` is not deserialized, the function requests xref:storage:MemoryStore.adoc#putBytes[`MemoryStore` to store the bytes]

If the put did not succeed and the storage level is to use disk, you should see the following WARN message in the logs:

```
WARN BlockManager: Persisting block [blockId] to disk instead.
```

And xref:DiskStore.adoc#putBytes[`DiskStore` stores the bytes].

NOTE: xref:DiskStore.adoc[DiskStore] is requested to store the bytes of a block with memory and disk storage level only when xref:storage:MemoryStore.adoc[MemoryStore] has failed.

If the storage level is to use disk only, xref:DiskStore.adoc#putBytes[`DiskStore` stores the bytes].

`doPutBytes` requests <<getCurrentBlockStatus, current block status>> and if the block was successfully stored, and the driver should know about it (`tellMaster`), the function <<reportBlockStatus, reports the current storage status of the block to the driver>>. The xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[current `TaskContext` metrics are updated with the updated block status] (only when executed inside a task where `TaskContext` is available).

You should see the following DEBUG message in the logs:

```
DEBUG BlockManager: Put block [blockId] locally took [time] ms
```

The function waits till the earlier asynchronous replication finishes for a block with replication level greater than `1`.

The final result of `doPutBytes` is the result of storing the block successful or not (as computed earlier).

NOTE: `doPutBytes` is used exclusively when BlockManager is requested to <<putBytes, putBytes>>.

== [[maybeCacheDiskValuesInMemory]] `maybeCacheDiskValuesInMemory` Method

CAUTION: FIXME

== [[doPut]] `doPut` Internal Method

[source, scala]
----
doPut[T](
  blockId: BlockId,
  level: StorageLevel,
  classTag: ClassTag[_],
  tellMaster: Boolean,
  keepReadLock: Boolean)(putBody: BlockInfo => Option[T]): Option[T]
----

`doPut` is an internal helper method for <<doPutBytes, doPutBytes>> and <<doPutIterator, doPutIterator>>.

`doPut` executes the input `putBody` function with a link:spark-BlockInfo.adoc[BlockInfo] being a new `BlockInfo` object (with `level` storage level) that link:spark-BlockInfoManager.adoc#lockNewBlockForWriting[`BlockInfoManager` managed to create a write lock for].

If the block has already been created (and link:spark-BlockInfoManager.adoc#lockNewBlockForWriting[`BlockInfoManager` did not manage to create a write lock for]), the following WARN message is printed out to the logs:

```
WARN Block [blockId] already exists on this machine; not re-adding it
```

`doPut` <<releaseLock, releases the read lock for the block>> when `keepReadLock` flag is disabled and returns `None` immediately.

If however the write lock has been given, `doPut` executes `putBody`.

If the result of `putBody` is `None` the block is considered saved successfully.

For successful save and `keepReadLock` enabled, link:spark-BlockInfoManager.adoc#downgradeLock[`BlockInfoManager` is requested to downgrade an exclusive write lock for `blockId` to a shared read lock].

For successful save and `keepReadLock` disabled, link:spark-BlockInfoManager.adoc#unlock[`BlockInfoManager` is requested to release lock on `blockId`].

For unsuccessful save, <<removeBlockInternal, the block is removed from memory and disk stores>> and the following WARN message is printed out to the logs:

```
WARN Putting block [blockId] failed
```

Ultimately, the following DEBUG message is printed out to the logs:

```
DEBUG Putting block [blockId] [withOrWithout] replication took [usedTime] ms
```

== [[removeBlock]] Removing Block From Memory and Disk

[source, scala]
----
removeBlock(blockId: BlockId, tellMaster: Boolean = true): Unit
----

`removeBlock` removes the `blockId` block from the xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore].

When executed, it prints out the following DEBUG message to the logs:

```
DEBUG Removing block [blockId]
```

It requests link:spark-BlockInfoManager.adoc[BlockInfoManager] for lock for writing for the `blockId` block. If it receives none, it prints out the following WARN message to the logs and quits.

```
WARN Asked to remove block [blockId], which does not exist
```

Otherwise, with a write lock for the block, the block is removed from xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore] (see xref:storage:MemoryStore.adoc#remove[Removing Block in `MemoryStore`] and xref:DiskStore.adoc#remove[Removing Block in `DiskStore`]).

If both removals fail, it prints out the following WARN message:

```
WARN Block [blockId] could not be removed as it was not found in either the disk, memory, or external block store
```

The block is removed from link:spark-BlockInfoManager.adoc[BlockInfoManager].

It then <<getCurrentBlockStatus, calculates the current block status>> that is used to <<reportBlockStatus, report the block status to the driver>> (if the input `tellMaster` and the info's `tellMaster` are both enabled, i.e. `true`) and the xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change].

NOTE: It is used to <<removeRdd, remove RDDs>> and <<removeBroadcast, broadcast>> as well as in link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc#RemoveBlock[`BlockManagerSlaveEndpoint` while handling `RemoveBlock` messages].

== [[removeRdd]] Removing RDD Blocks

[source, scala]
----
removeRdd(rddId: Int): Int
----

`removeRdd` removes all the blocks that belong to the `rddId` RDD.

It prints out the following INFO message to the logs:

```
INFO Removing RDD [rddId]
```

It then requests RDD blocks from link:spark-BlockInfoManager.adoc[BlockInfoManager] and <<removeBlock, removes them (from memory and disk)>> (without informing the driver).

The number of blocks removed is the final result.

NOTE: It is used by link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc#RemoveRdd[`BlockManagerSlaveEndpoint` while handling `RemoveRdd` messages].

== [[removeBroadcast]] Removing All Blocks of Broadcast Variable

[source, scala]
----
removeBroadcast(broadcastId: Long, tellMaster: Boolean): Int
----

`removeBroadcast` removes all the blocks of the input `broadcastId` broadcast.

Internally, it starts by printing out the following DEBUG message to the logs:

```
DEBUG Removing broadcast [broadcastId]
```

It then requests all the link:spark-BlockDataManager.adoc#BroadcastBlockId[BroadcastBlockId] objects that belong to the `broadcastId` broadcast from link:spark-BlockInfoManager.adoc[BlockInfoManager] and <<removeBlock, removes them (from memory and disk)>>.

The number of blocks removed is the final result.

NOTE: It is used by link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc#RemoveBroadcast[`BlockManagerSlaveEndpoint` while handling `RemoveBroadcast` messages].

== [[getStatus]] Getting Block Status

CAUTION: FIXME

== [[shuffleServerId]] `shuffleServerId`

CAUTION: FIXME

== [[initialize]] Initializing BlockManager

[source, scala]
----
initialize(
  appId: String): Unit
----

initialize initializes a BlockManager on the driver and executors (see xref:ROOT:SparkContext.adoc#creating-instance[Creating SparkContext Instance] and xref:executor:Executor.adoc#creating-instance[Creating Executor Instance], respectively).

NOTE: The method must be called before a BlockManager can be considered fully operable.

initialize does the following in order:

1. Initializes xref:core:BlockTransferService.adoc#init[BlockTransferService]
2. Initializes the internal shuffle client, be it xref:core:ExternalShuffleClient.adoc[ExternalShuffleClient] or xref:core:BlockTransferService.adoc[BlockTransferService].
3. xref:BlockManagerMaster.adoc#registerBlockManager[Registers itself with the driver's `BlockManagerMaster`] (using the `id`, `maxMemory` and its `slaveEndpoint`).
+
The `BlockManagerMaster` reference is passed in when the <<creating-instance, BlockManager is created>> on the driver and executors.
4. Sets <<shuffleServerId, shuffleServerId>> to an instance of <<BlockManagerId, BlockManagerId>> given an executor id, host name and port for xref:core:BlockTransferService.adoc[BlockTransferService].
5. It creates the address of the server that serves this executor's shuffle files (using <<shuffleServerId, shuffleServerId>>)

CAUTION: FIXME Review the initialize procedure again

CAUTION: FIXME Describe `shuffleServerId`. Where is it used?

If the <<externalShuffleServiceEnabled, External Shuffle Service is used>>, the following INFO appears in the logs:

```
INFO external shuffle service port = [externalShuffleServicePort]
```

It xref:BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's BlockManagerMaster] passing the <<BlockManagerId, BlockManagerId>>, the maximum memory (as `maxMemory`), and the link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc[BlockManagerSlaveEndpoint].

Ultimately, if the initialization happens on an executor and the <<externalShuffleServiceEnabled, External Shuffle Service is used>>, it <<registerWithExternalShuffleServer, registers to the shuffle service>>.

NOTE: initialize is called when the link:spark-SparkContext-creating-instance-internals.adoc#BlockManager-initialization[driver is launched (and `SparkContext` is created)] and when an xref:executor:Executor.adoc#creating-instance[`Executor` is created] (for xref:executor:CoarseGrainedExecutorBackend.adoc#RegisteredExecutor[CoarseGrainedExecutorBackend] and xref:spark-on-mesos:spark-executor-backends-MesosExecutorBackend.adoc[MesosExecutorBackend]).

== [[registerWithExternalShuffleServer]] Registering Executor's BlockManager with External Shuffle Server

[source, scala]
----
registerWithExternalShuffleServer(): Unit
----

`registerWithExternalShuffleServer` is an internal helper method to register the BlockManager for an executor with an xref:deploy:ExternalShuffleService.adoc[external shuffle server].

NOTE: It is executed when a <<initialize, BlockManager is initialized on an executor and an external shuffle service is used>>.

When executed, you should see the following INFO message in the logs:

```
INFO Registering executor with local external shuffle service.
```

It uses <<shuffleClient, shuffleClient>> to xref:core:ExternalShuffleClient.adoc#registerWithShuffleServer[register the block manager] using <<shuffleServerId, shuffleServerId>> (i.e. the host, the port and the executorId) and a `ExecutorShuffleInfo`.

NOTE: The `ExecutorShuffleInfo` uses `localDirs` and `subDirsPerLocalDir` from xref:DiskBlockManager.adoc[DiskBlockManager] and the class name of the constructor xref:shuffle:ShuffleManager.adoc[ShuffleManager].

It tries to register at most 3 times with 5-second sleeps in-between.

NOTE: The maximum number of attempts and the sleep time in-between are hard-coded, i.e. they are not configured.

Any issues while connecting to the external shuffle service are reported as ERROR messages in the logs:

```
ERROR Failed to connect to external shuffle server, will retry [#attempts] more times after waiting 5 seconds...
```

NOTE: registerWithExternalShuffleServer is used when BlockManager is requested to <<initialize, initialize>> (when executed on an executor with <<externalShuffleServiceEnabled, externalShuffleServiceEnabled>>).

== [[reregister]] Re-registering BlockManager with Driver and Reporting Blocks

[source, scala]
----
reregister(): Unit
----

When executed, `reregister` prints the following INFO message to the logs:

```
INFO BlockManager: BlockManager [blockManagerId] re-registering with master
```

`reregister` then xref:BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's `BlockManagerMaster`] (just as it was when <<initialize, BlockManager was initializing>>). It passes the <<BlockManagerId, BlockManagerId>>, the maximum memory (as `maxMemory`), and the link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc[BlockManagerSlaveEndpoint].

`reregister` will then report all the local blocks to the xref:BlockManagerMaster.adoc[BlockManagerMaster].

You should see the following INFO message in the logs:

```
INFO BlockManager: Reporting [blockInfoManager.size] blocks to the master.
```

For each block metadata (in link:spark-BlockInfoManager.adoc[BlockInfoManager]) it <<getCurrentBlockStatus, gets block current status>> and <<tryToReportBlockStatus, tries to send it to the BlockManagerMaster>>.

If there is an issue communicating to the xref:BlockManagerMaster.adoc[BlockManagerMaster], you should see the following ERROR message in the logs:

```
ERROR BlockManager: Failed to report [blockId] to master; giving up.
```

After the ERROR message, `reregister` stops reporting.

NOTE: `reregister` is called when a xref:executor:Executor.adoc#heartbeats-and-active-task-metrics[`Executor` was informed to re-register while sending heartbeats].

== [[getCurrentBlockStatus]] Calculate Current Block Status

[source, scala]
----
getCurrentBlockStatus(blockId: BlockId, info: BlockInfo): BlockStatus
----

`getCurrentBlockStatus` returns the current `BlockStatus` of the `BlockId` block (with the block's current xref:storage:StorageLevel.adoc[StorageLevel], memory and disk sizes). It uses xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore] for size and other information.

NOTE: Most of the information to build `BlockStatus` is already in `BlockInfo` except that it may not necessarily reflect the current state per xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore].

Internally, it uses the input link:spark-BlockInfo.adoc[BlockInfo] to know about the block's storage level. If the storage level is not set (i.e. `null`), the returned `BlockStatus` assumes the xref:storage:StorageLevel.adoc[default `NONE` storage level] and the memory and disk sizes being `0`.

If however the storage level is set, `getCurrentBlockStatus` uses xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore] to check whether the block is stored in the storages or not and request for their sizes in the storages respectively (using their `getSize` or assume `0`).

NOTE: It is acceptable that the `BlockInfo` says to use memory or disk yet the block is not in the storages (yet or anymore). The method will give current status.

NOTE: `getCurrentBlockStatus` is used when <<reregister, executor's BlockManager is requested to report the current status of the local blocks to the master>>, <<doPutBytes, saving a block to a storage>> or <<dropFromMemory, removing a block from memory only>> or <<removeBlock, both, i.e. from memory and disk>>.

== [[reportAllBlocks]] `reportAllBlocks` Internal Method

[source, scala]
----
reportAllBlocks(): Unit
----

`reportAllBlocks`...FIXME

NOTE: `reportAllBlocks` is used when BlockManager is requested to <<reregister, re-register all blocks to the driver>>.

== [[reportBlockStatus]] Reporting Current Storage Status of Block to Driver

[source, scala]
----
reportBlockStatus(
  blockId: BlockId,
  info: BlockInfo,
  status: BlockStatus,
  droppedMemorySize: Long = 0L): Unit
----

`reportBlockStatus` is an internal method for <<tryToReportBlockStatus, reporting a block status to the driver>> and if told to re-register it prints out the following INFO message to the logs:

```
INFO BlockManager: Got told to re-register updating block [blockId]
```

It does asynchronous reregistration (using `asyncReregister`).

In either case, it prints out the following DEBUG message to the logs:

```
DEBUG BlockManager: Told master about block [blockId]
```

NOTE: `reportBlockStatus` is used when BlockManager is requested to <<getBlockData, getBlockData>>, <<doPutBytes, doPutBytes>>, <<doPutIterator, doPutIterator>>, <<dropFromMemory, dropFromMemory>> and <<removeBlockInternal, removeBlockInternal>>.

== [[tryToReportBlockStatus]] Reporting Block Status Update to Driver

[source, scala]
----
def tryToReportBlockStatus(
  blockId: BlockId,
  info: BlockInfo,
  status: BlockStatus,
  droppedMemorySize: Long = 0L): Boolean
----

`tryToReportBlockStatus` xref:BlockManagerMaster.adoc#updateBlockInfo[reports block status update] to <<master, BlockManagerMaster>> and returns its response.

NOTE: `tryToReportBlockStatus` is used when BlockManager is requested to <<reportAllBlocks, reportAllBlocks>> or <<reportBlockStatus, reportBlockStatus>>.

== [[broadcast]] Broadcast Values

When a new broadcast value is created, xref:core:TorrentBroadcast.adoc[TorrentBroadcast] blocks are put in the block manager.

You should see the following `TRACE` message:

```
TRACE Put for block [blockId] took [startTimeMs] to get into synchronized block
```

It puts the data in the memory first and drop to disk if the memory store can't hold it.

```
DEBUG Put block [blockId] locally took [startTimeMs]
```

== [[BlockManagerId]] BlockManagerId

FIXME

== [[execution-context]] Execution Context

*block-manager-future* is the execution context for...FIXME

== Misc

The underlying abstraction for blocks in Spark is a `ByteBuffer` that limits the size of a block to 2GB (`Integer.MAX_VALUE` - see http://stackoverflow.com/q/8076472/1305344[Why does FileChannel.map take up to Integer.MAX_VALUE of data?] and https://issues.apache.org/jira/browse/SPARK-1476[SPARK-1476 2GB limit in spark for blocks]). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for `long`), ser-deser via byte array-backed output streams.

When a non-local executor starts, it initializes a BlockManager object using xref:ROOT:SparkConf.adoc#spark.app.id[spark.app.id] Spark property for the id.

== [[BlockResult]] BlockResult

`BlockResult` is a description of a fetched block with the `readMethod` and `bytes`.

== [[registerTask]] Registering Task with BlockInfoManager

[source, scala]
----
registerTask(taskAttemptId: Long): Unit
----

`registerTask` link:spark-BlockInfoManager.adoc#registerTask[registers the input `taskAttemptId` with `BlockInfoManager`].

NOTE: `registerTask` is used exclusively when xref:scheduler:Task.adoc#run[`Task` runs].

== [[getDiskWriter]] Creating DiskBlockObjectWriter

[source, scala]
----
getDiskWriter(
  blockId: BlockId,
  file: File,
  serializerInstance: SerializerInstance,
  bufferSize: Int,
  writeMetrics: ShuffleWriteMetrics): DiskBlockObjectWriter
----

getDiskWriter creates a xref:storage:DiskBlockObjectWriter.adoc[DiskBlockObjectWriter] (with xref:ROOT:configuration-properties.adoc#spark.shuffle.sync[spark.shuffle.sync] configuration property for syncWrites argument).

getDiskWriter uses the <<serializerManager, SerializerManager>> of the BlockManager.

getDiskWriter is used when:

* BypassMergeSortShuffleWriter is requested to xref:shuffle:BypassMergeSortShuffleWriter.adoc#write[write records (of a partition)]

* ShuffleExternalSorter is requested to xref:shuffle:ShuffleExternalSorter.adoc#writeSortedFile[writeSortedFile]

* ExternalAppendOnlyMap is requested to xref:shuffle:ExternalAppendOnlyMap.adoc#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk]

* ExternalSorter is requested to xref:shuffle:ExternalSorter.adoc#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] and xref:shuffle:ExternalSorter.adoc#writePartitionedFile[writePartitionedFile]

* xref:memory:UnsafeSorterSpillWriter.adoc[UnsafeSorterSpillWriter] is created

== [[addUpdatedBlockStatusToTaskMetrics]] Recording Updated BlockStatus In Current Task's TaskMetrics

[source, scala]
----
addUpdatedBlockStatusToTaskMetrics(blockId: BlockId, status: BlockStatus): Unit
----

`addUpdatedBlockStatusToTaskMetrics` link:spark-TaskContext.adoc#get[takes an active `TaskContext`] (if available) and xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[records updated `BlockStatus` for `Block`] (in the link:spark-TaskContext.adoc#taskMetrics[task's `TaskMetrics`]).

NOTE: `addUpdatedBlockStatusToTaskMetrics` is used when BlockManager <<doPutBytes, doPutBytes>> (for a block that was successfully stored), <<doPut, doPut>>, <<doPutIterator, doPutIterator>>, <<dropFromMemory, removes blocks from memory>> (possibly spilling it to disk) and <<removeBlock, removes block from memory and disk>>.

== [[shuffleMetricsSource]] Requesting Shuffle-Related Spark Metrics Source

[source, scala]
----
shuffleMetricsSource: Source
----

`shuffleMetricsSource` requests the <<shuffleClient, ShuffleClient>> for the xref:core:ShuffleClient.adoc#shuffleMetrics[shuffle-related metrics] and creates a link:spark-BlockManager-ShuffleMetricsSource.adoc[ShuffleMetricsSource] with the link:spark-BlockManager-ShuffleMetricsSource.adoc#sourceName[source name] per xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property:

* *ExternalShuffle* when xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is on (`true`)

* *NettyBlockTransfer* when xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is off (`false`)

NOTE: `shuffleMetricsSource` is used exclusively when `Executor` is xref:executor:Executor.adoc#creating-instance[created] (for non-local / cluster modes).

== [[replicate]] Replicating Block To Peers

[source, scala]
----
replicate(
  blockId: BlockId,
  data: BlockData,
  level: StorageLevel,
  classTag: ClassTag[_],
  existingReplicas: Set[BlockManagerId] = Set.empty): Unit
----

`replicate`...FIXME

NOTE: `replicate` is used when BlockManager is requested to <<doPutBytes, doPutBytes>>, <<doPutIterator, doPutIterator>> and <<replicateBlock, replicateBlock>>.

== [[replicateBlock]] `replicateBlock` Method

[source, scala]
----
replicateBlock(
  blockId: BlockId,
  existingReplicas: Set[BlockManagerId],
  maxReplicas: Int): Unit
----

`replicateBlock`...FIXME

NOTE: `replicateBlock` is used exclusively when `BlockManagerSlaveEndpoint` is requested to link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc#receiveAndReply-ReplicateBlock[handle ReplicateBlock messages].

== [[putIterator]] `putIterator` Method

[source, scala]
----
putIterator[T: ClassTag](
  blockId: BlockId,
  values: Iterator[T],
  level: StorageLevel,
  tellMaster: Boolean = true): Boolean
----

`putIterator`...FIXME

[NOTE]
====
`putIterator` is used when:

* BlockManager is requested to <<putSingle, putSingle>>

* Spark Streaming's `BlockManagerBasedBlockHandler` is requested to `storeBlock`
====

== [[putSingle]] putSingle Method

[source, scala]
----
putSingle[T: ClassTag](
  blockId: BlockId,
  value: T,
  level: StorageLevel,
  tellMaster: Boolean = true): Boolean
----

putSingle...FIXME

putSingle is used when TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#writeBlocks[write the blocks] and xref:core:TorrentBroadcast.adoc#readBroadcastBlock[readBroadcastBlock].

== [[getRemoteBytes]] Fetching Block From Remote Nodes

[source, scala]
----
getRemoteBytes(blockId: BlockId): Option[ChunkedByteBuffer]
----

`getRemoteBytes`...FIXME

[NOTE]
====
`getRemoteBytes` is used when:

* BlockManager is requested to <<getRemoteValues, getRemoteValues>>

* `TorrentBroadcast` is requested to xref:core:TorrentBroadcast.adoc#readBlocks[readBlocks]

* `TaskResultGetter` is requested to xref:scheduler:TaskResultGetter.adoc#enqueueSuccessfulTask[enqueuing a successful IndirectTaskResult]
====

== [[getRemoteValues]] `getRemoteValues` Internal Method

[source, scala]
----
getRemoteValues[T: ClassTag](blockId: BlockId): Option[BlockResult]
----

`getRemoteValues`...FIXME

NOTE: `getRemoteValues` is used exclusively when BlockManager is requested to <<get, get a block by BlockId>>.

== [[getSingle]] `getSingle` Method

[source, scala]
----
getSingle[T: ClassTag](blockId: BlockId): Option[T]
----

`getSingle`...FIXME

NOTE: `getSingle` is used exclusively in Spark tests.

== [[shuffleClient]] `shuffleClient` Property

[source, scala]
----
shuffleClient: ShuffleClient
----

`shuffleClient` is a xref:core:ShuffleClient.adoc[ShuffleClient] that BlockManager uses for the following:

* <<shuffleMetricsSource, shuffleMetricsSource>>

* <<registerWithExternalShuffleServer, Registering the BlockManager of an executor with an external shuffle server>>

`shuffleClient` is also used when `BlockStoreShuffleReader` is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined key-value records for a reduce task] (and creates a xref:storage:ShuffleBlockFetcherIterator.adoc#shuffleClient[ShuffleBlockFetcherIterator]).

`shuffleClient` can be either a xref:core:ExternalShuffleClient.adoc[ExternalShuffleClient] or the <<blockTransferService, BlockTransferService>> (that is the xref:core:NettyBlockTransferService.adoc[NettyBlockTransferService] given by xref:core:SparkEnv.adoc#create-BlockManager[SparkEnv]).

CAUTION: FIXME Figure

[[shuffleClient-externalShuffleServiceEnabled]]
`shuffleClient` uses xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property that controls whether to use xref:core:ExternalShuffleClient.adoc[ExternalShuffleClient] (`true`) or the <<blockTransferService, BlockTransferService>> (i.e. xref:core:NettyBlockTransferService.adoc[NettyBlockTransferService]).

== [[getOrElseUpdate]] Getting Block From Block Managers Or Computing and Storing It Otherwise

[source, scala]
----
getOrElseUpdate[T](
  blockId: BlockId,
  level: StorageLevel,
  classTag: ClassTag[T],
  makeIterator: () => Iterator[T]): Either[BlockResult, Iterator[T]]
----

[NOTE]
====
_I think_ it is fair to say that `getOrElseUpdate` is like link:++https://www.scala-lang.org/api/current/scala/collection/mutable/Map.html#getOrElseUpdate(key:K,op:=%3EV):V++[getOrElseUpdate] of https://www.scala-lang.org/api/current/scala/collection/mutable/Map.html[scala.collection.mutable.Map] in Scala.

[source, scala]
----
getOrElseUpdate(key: K, op: â‡’ V): V
----

Quoting the official scaladoc:

If given key `K` is already in this map, `getOrElseUpdate` returns the associated value `V`.

Otherwise, `getOrElseUpdate` computes a value `V` from given expression `op`, stores with the key `K` in the map and returns that value.

Since BlockManager is a key-value store of blocks of data identified by a block ID that works just fine.
====

`getOrElseUpdate` first attempts to <<get, get the block>> by the `BlockId` (from the local block manager first and, if unavailable, requesting remote peers).

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.storage.BlockManager` logger to see what happens when BlockManager tries to <<get, get a block>>.

See <<logging, logging>> in this document.
====

`getOrElseUpdate` gives the `BlockResult` of the block if found.

If however the block was not found (in any block manager in a Spark cluster), `getOrElseUpdate` <<doPutIterator, doPutIterator>> (for the input `BlockId`, the `makeIterator` function and the `StorageLevel`).

`getOrElseUpdate` branches off per the result.

For `None`, `getOrElseUpdate` <<getLocalValues, getLocalValues>> for the `BlockId` and eventually returns the `BlockResult` (unless terminated by a `SparkException` due to some internal error).

For `Some(iter)`, `getOrElseUpdate` returns an iterator of `T` values.

NOTE: `getOrElseUpdate` is used exclusively when `RDD` is requested to xref:rdd:RDD.adoc#getOrCompute[get or compute an RDD partition] (for a `RDDBlockId` with a RDD ID and a partition index).

== [[doPutIterator]] `doPutIterator` Internal Method

[source, scala]
----
doPutIterator[T](
  blockId: BlockId,
  iterator: () => Iterator[T],
  level: StorageLevel,
  classTag: ClassTag[T],
  tellMaster: Boolean = true,
  keepReadLock: Boolean = false): Option[PartiallyUnrolledIterator[T]]
----

`doPutIterator` simply <<doPut, doPut>> with the `putBody` function that accepts a `BlockInfo` and does the following:

. `putBody` branches off per whether the `StorageLevel` indicates to use a xref:storage:StorageLevel.adoc#useMemory[memory] or simply a xref:storage:StorageLevel.adoc#useDisk[disk], i.e.

* When the input `StorageLevel` indicates to xref:storage:StorageLevel.adoc#useMemory[use a memory] for storage in xref:storage:StorageLevel.adoc#deserialized[deserialized] format, `putBody` requests <<memoryStore, MemoryStore>> to xref:storage:MemoryStore.adoc#putIteratorAsValues[putIteratorAsValues] (for the `BlockId` and with the `iterator` factory function).
+
If the <<memoryStore, MemoryStore>> returned a correct value, the internal `size` is set to the value.
+
If however the <<memoryStore, MemoryStore>> failed to give a correct value, FIXME

* When the input `StorageLevel` indicates to xref:storage:StorageLevel.adoc#useMemory[use memory] for storage in xref:storage:StorageLevel.adoc#deserialized[serialized] format, `putBody`...FIXME

* When the input `StorageLevel` does not indicate to use memory for storage but xref:storage:StorageLevel.adoc#useDisk[disk] instead, `putBody`...FIXME

. `putBody` requests the <<getCurrentBlockStatus, current block status>>

. Only when the block was successfully stored in either the memory or disk store:

* `putBody` <<reportBlockStatus, reports the block status>> to the <<master, BlockManagerMaster>> when the input `tellMaster` flag (default: enabled) and the `tellMaster` flag of the block info are both enabled.

* `putBody` <<addUpdatedBlockStatusToTaskMetrics, addUpdatedBlockStatusToTaskMetrics>> (with the `BlockId` and `BlockStatus`)

* `putBody` prints out the following DEBUG message to the logs:
+
```
Put block [blockId] locally took [time] ms
```

* When the input `StorageLevel` indicates to use xref:storage:StorageLevel.adoc#replication[replication], `putBody` <<doGetLocalBytes, doGetLocalBytes>> followed by <<replicate, replicate>> (with the input `BlockId` and the `StorageLevel` as well as the `BlockData` to replicate)

* With a successful replication, `putBody` prints out the following DEBUG message to the logs:
+
```
Put block [blockId] remotely took [time] ms
```

. In the end, `putBody` may or may not give a `PartiallyUnrolledIterator` if...FIXME

NOTE: `doPutIterator` is used when BlockManager is requested to <<getOrElseUpdate, get a block from block managers or computing and storing it otherwise>> and <<putIterator, putIterator>>.

== [[dropFromMemory]] Removing Blocks From Memory Only

[source, scala]
----
dropFromMemory(
  blockId: BlockId,
  data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel
----

NOTE: `dropFromMemory` is part of the link:spark-BlockEvictionHandler.adoc#dropFromMemory[BlockEvictionHandler Contract] to...FIXME

When `dropFromMemory` is executed, you should see the following INFO message in the logs:

```
INFO BlockManager: Dropping block [blockId] from memory
```

It then asserts that the `blockId` block is link:spark-BlockInfoManager.adoc#assertBlockIsLockedForWriting[locked for writing].

If the block's xref:storage:StorageLevel.adoc[StorageLevel] uses disks and the internal xref:DiskStore.adoc[DiskStore] object (`diskStore`) does not contain the block, it is saved then. You should see the following INFO message in the logs:

```
INFO BlockManager: Writing block [blockId] to disk
```

CAUTION: FIXME Describe the case with saving a block to disk.

The block's memory size is fetched and recorded (using `MemoryStore.getSize`).

The block is xref:storage:MemoryStore.adoc#remove[removed from memory] if exists. If not, you should see the following WARN message in the logs:

```
WARN BlockManager: Block [blockId] could not be dropped from memory as it does not exist
```

It then <<getCurrentBlockStatus, calculates the current storage status of the block>> and <<reportBlockStatus, reports it to the driver>>. It only happens when `info.tellMaster`.

CAUTION: FIXME When would `info.tellMaster` be `true`?

A block is considered updated when it was written to disk or removed from memory or both. If either happened, the xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change].

Ultimately, `dropFromMemory` returns the current storage level of the block.

== [[handleLocalReadFailure]] `handleLocalReadFailure` Internal Method

[source, scala]
----
handleLocalReadFailure(blockId: BlockId): Nothing
----

`handleLocalReadFailure`...FIXME

NOTE: `handleLocalReadFailure` is used when...FIXME

== [[releaseLockAndDispose]] releaseLockAndDispose Method

[source, scala]
----
releaseLockAndDispose(
  blockId: BlockId,
  data: BlockData,
  taskAttemptId: Option[Long] = None): Unit
----

releaseLockAndDispose...FIXME

releaseLockAndDispose is used when...FIXME

== [[logging]] Logging

Enable `ALL` logging level for `org.apache.spark.storage.BlockManager` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

[source,plaintext]
----
log4j.logger.org.apache.spark.storage.BlockManager=ALL
----

Refer to xref:ROOT:spark-logging.adoc[Logging].

== [[internal-properties]] Internal Properties

=== [[blockInfoManager]] BlockInfoManager

link:spark-BlockInfoManager.adoc[BlockInfoManager] for...FIXME

=== [[maxMemory]] Maximum Memory

Total maximum value that BlockManager can ever possibly use (that depends on <<memoryManager, MemoryManager>> and may vary over time).

Total available xref:memory:MemoryManager.adoc#maxOnHeapStorageMemory[on-heap] and xref:memory:MemoryManager.adoc#maxOffHeapStorageMemory[off-heap] memory for storage (in bytes)

=== [[maxOffHeapMemory]] Maximum Off-Heap Memory

=== [[maxOnHeapMemory]] Maximum On-Heap Memory
