= [[UnsafeShuffleWriter]] UnsafeShuffleWriter

`UnsafeShuffleWriter` is a xref:ShuffleWriter.adoc[ShuffleWriter] for xref:spark-shuffle-SerializedShuffleHandle.adoc[SerializedShuffleHandles]:

* When requested to <<write, write>>, `UnsafeShuffleWriter` simply <<insertRecordIntoSorter, inserts every record into ShuffleExternalSorter>> followed by <<closeAndWriteOutput, closing the internal resources and writing out merged spill files>> (that, among other things, creates the <<mapStatus, MapStatus>>).

* When requested to <<stop, stop>>, `UnsafeShuffleWriter` records the peak execution memory metric and returns the <<mapStatus, mapStatus>> (that was created when requested to <<write, write>>).

`UnsafeShuffleWriter` is <<creating-instance, created>> when `SortShuffleManager` is requested for a xref:SortShuffleManager.adoc#getWriter[ShuffleWriter] for a xref:spark-shuffle-SerializedShuffleHandle.adoc[SerializedShuffleHandle].

[[transferToEnabled]]
`UnsafeShuffleWriter` can use a <<mergeSpillsWithTransferTo, specialized NIO-based fast merge procedure>> that avoids extra serialization/deserialization when <<spark-configuration-properties.adoc#spark.file.transferTo, spark.file.transferTo>> configuration property (default: `true`) is enabled.

[[DEFAULT_INITIAL_SORT_BUFFER_SIZE]]
[[initialSortBufferSize]]
`UnsafeShuffleWriter` uses the <<initialSortBufferSize, initial buffer size for sorting>> (default: `4096`) when creating a <<sorter, ShuffleExternalSorter>> (when requested to <<open, open>>).

TIP: Use <<spark-configuration-properties.adoc#spark.shuffle.sort.initialBufferSize, spark.shuffle.sort.initialBufferSize>> configuration property to change the default buffer size.

[[DEFAULT_INITIAL_SER_BUFFER_SIZE]]
`UnsafeShuffleWriter` uses a fixed buffer size for the <<serBuffer, output stream of serialized data written into a byte array>> (default: `1024 * 1024`).

[[inputBufferSizeInBytes]]
`UnsafeShuffleWriter` uses the <<spark-configuration-properties.adoc#spark.shuffle.file.buffer, spark.shuffle.file.buffer>> configuration property (default: `32k`) for...FIXME

[[outputBufferSizeInBytes]]
`UnsafeShuffleWriter` uses the <<spark-configuration-properties.adoc#spark.shuffle.unsafe.file.output.buffer, spark.shuffle.unsafe.file.output.buffer>> configuration property (default: `32k`) for...FIXME

== [[mergeSpillsWithTransferTo]] `mergeSpillsWithTransferTo` Internal Method

[source, java]
----
long[] mergeSpillsWithTransferTo(
  SpillInfo[] spills,
  File outputFile)
----

`mergeSpillsWithTransferTo`...FIXME

NOTE: `mergeSpillsWithTransferTo` is used exclusively when `UnsafeShuffleWriter` is requested to <<mergeSpills, mergeSpills>> (with the <<transferToEnabled, transferToEnabled>> flag enabled and no encryption).

== [[forceSorterToSpill]] Forcing ShuffleExternalSorter to Spill To Disk (Freeing Up Execution Memory) -- `forceSorterToSpill` Internal Method

[source, java]
----
void forceSorterToSpill()
----

`forceSorterToSpill` simply requests the <<sorter, ShuffleExternalSorter>> to xref:shuffle:ShuffleExternalSorter.adoc#spill[spill to disk (and free up execution memory)].

NOTE: `forceSorterToSpill` is used exclusively for testing.

== [[mergeSpills]] `mergeSpills` Internal Method

[source, java]
----
long[] mergeSpills(
  SpillInfo[] spills,
  File outputFile)
----

`mergeSpills`...FIXME

NOTE: `mergeSpills` is used exclusively when `UnsafeShuffleWriter` is requested to <<closeAndWriteOutput, close internal resources and write out merged spill files>>.

== [[updatePeakMemoryUsed]] `updatePeakMemoryUsed` Internal Method

[source, java]
----
void updatePeakMemoryUsed()
----

`updatePeakMemoryUsed`...FIXME

NOTE: `updatePeakMemoryUsed` is used when `UnsafeShuffleWriter` is requested for the <<getPeakMemoryUsedBytes, peak memory used>> and to <<closeAndWriteOutput, close internal resources and write out merged spill files>>

== [[write]] Writing Records to Shuffle System -- `write` Method

[source, java]
----
void write(Iterator<Product2<K, V>> records)
----

NOTE: `write` is part of the <<ShuffleWriter.adoc#write, ShuffleWriter Contract>> to write records to a shuffle system.

Internally, `write` traverses the input sequence of records (for a RDD partition) and <<insertRecordIntoSorter, insertRecordIntoSorter>> one by one. When all the records have been processed, `write` <<closeAndWriteOutput, closes internal resources and writes spill files merged>>.

In the end, `write` xref:shuffle:ShuffleExternalSorter.adoc#cleanupResources[requests `ShuffleExternalSorter` to clean after itself].

CAUTION: FIXME

== [[stop]] Stopping ShuffleWriter -- `stop` Method

[source, scala]
----
Option<MapStatus> stop(boolean success)
----

NOTE: `stop` is part of the <<ShuffleWriter.adoc#stop, ShuffleWriter Contract>> to stop the <<ShuffleWriter.adoc#, ShuffleWriter>>.

`stop`...FIXME

== [[creating-instance]] Creating UnsafeShuffleWriter Instance

`UnsafeShuffleWriter` takes the following to be created:

* [[blockManager]] xref:storage:BlockManager.adoc[BlockManager]
* [[shuffleBlockResolver]] <<IndexShuffleBlockResolver.adoc#, IndexShuffleBlockResolver>>
* [[memoryManager]] xref:memory:TaskMemoryManager.adoc[TaskMemoryManager]
* [[handle]] xref:spark-shuffle-SerializedShuffleHandle.adoc[SerializedShuffleHandle]
* [[mapId]] Map ID
* [[taskContext]] <<spark-TaskContext.adoc#, TaskContext>>
* [[sparkConf]] <<spark-SparkConf.adoc#, SparkConf>>

`UnsafeShuffleWriter` requests the <<handle, SerializedShuffleHandle>> for the <<spark-shuffle-BaseShuffleHandle.adoc#dependency, ShuffleDependency>> that is then requested for the xref:rdd:ShuffleDependency.adoc#partitioner[Partitioner] and, in the end, for the <<spark-rdd-Partitioner.adoc#numPartitions, number of partitions>>. `UnsafeShuffleWriter` makes sure that the number of shuffle output partitions is below xref:SortShuffleManager.adoc#MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE[`(1 << 24)` partition identifiers that can be encoded] and throws an `IllegalArgumentException` if not met:

```
UnsafeShuffleWriter can only be used for shuffles with at most 16777215 reduce partitions
```

NOTE: The number of shuffle output partitions is first enforced when xref:SortShuffleManager.adoc#canUseSerializedShuffle[`SortShuffleManager` checks if `SerializedShuffleHandle` can be used for `ShuffleHandle`] (that eventually leads to `UnsafeShuffleWriter`).

In the end, `UnsafeShuffleWriter` <<open, creates a ShuffleExternalSorter and a SerializationStream>>.

== [[open]] Opening UnsafeShuffleWriter (Creating ShuffleExternalSorter and SerializationStream) -- `open` Internal Method

[source, java]
----
void open()
----

`open` makes sure that the internal reference to xref:shuffle:ShuffleExternalSorter.adoc[ShuffleExternalSorter] (as `sorter`) is not defined and xref:shuffle:ShuffleExternalSorter.adoc#creating-instance[creates one itself].

`open` creates a new byte array output stream (as `serBuffer`) with the buffer capacity of `1M`.

`open` creates a new link:spark-SerializationStream.adoc[SerializationStream] for the new byte array output stream using link:spark-SerializerInstance.adoc[SerializerInstance].

NOTE: `SerializerInstance` was defined when <<creating-instance, `UnsafeShuffleWriter` was created>> (and is exactly the one used to xref:rdd:ShuffleDependency.adoc.adoc#creating-instance[create the `ShuffleDependency`]).

NOTE: `open` is used exclusively when `UnsafeShuffleWriter` is <<creating-instance, created>>.

== [[insertRecordIntoSorter]] Inserting Record Into ShuffleExternalSorter -- `insertRecordIntoSorter` Method

[source, java]
----
void insertRecordIntoSorter(Product2<K, V> record)
----

`insertRecordIntoSorter` link:spark-rdd-Partitioner.adoc#getPartition[calculates the partition for the key of the input `record`].

NOTE: `Partitioner` is defined when <<creating-instance, `UnsafeShuffleWriter` is created>>.

`insertRecordIntoSorter` then writes the key and the value of the input `record` to link:spark-SerializationStream.adoc[SerializationStream] and calculates the size of the serialized buffer.

NOTE: `SerializationStream` is created when <<open, `UnsafeShuffleWriter` opens>>.

In the end, `insertRecordIntoSorter` xref:shuffle:ShuffleExternalSorter.adoc#insertRecord[inserts the serialized buffer to `ShuffleExternalSorter`] (as `Platform.BYTE_ARRAY_OFFSET` ).

NOTE: `ShuffleExternalSorter` is created when <<open, `UnsafeShuffleWriter` opens>>.

NOTE: `insertRecordIntoSorter` is used exclusively when `UnsafeShuffleWriter` is requested to <<write, write records>>.

== [[closeAndWriteOutput]] Closing Internal Resources and Writing Out Merged Spill Files -- `closeAndWriteOutput` Method

[source, java]
----
void closeAndWriteOutput()
----

`closeAndWriteOutput` first <<updatePeakMemoryUsed, updates peak memory used>>.

`closeAndWriteOutput` removes the internal `ByteArrayOutputStream` and link:spark-SerializationStream.adoc[SerializationStream].

`closeAndWriteOutput` requests xref:shuffle:ShuffleExternalSorter.adoc#closeAndGetSpills[`ShuffleExternalSorter` to close itself and return `SpillInfo` metadata].

`closeAndWriteOutput` removes the internal `ShuffleExternalSorter`.

`closeAndWriteOutput` requests `IndexShuffleBlockResolver` for the data file for the `shuffleId` and `mapId`.

`closeAndWriteOutput` creates a temporary file to <<mergeSpills, merge spill files>>, deletes them afterwards, and requests `IndexShuffleBlockResolver` to write index file and commit.

`closeAndWriteOutput` creates a xref:scheduler:MapStatus.adoc[MapStatus] with the xref:storage:BlockManager.adoc#shuffleServerId[location of the executor's `BlockManager`] and partition lengths in the merged file.

If there is an issue with deleting spill files, you should see the following ERROR message in the logs:

```
ERROR Error while deleting spill file [path]
```

If there is an issue with deleting the temporary file, you should see the following ERROR message in the logs:

```
ERROR Error while deleting temp file [path]
```

NOTE: `closeAndWriteOutput` is used exclusively when `UnsafeShuffleWriter` is requested to <<write, write records>>.

== [[mergeSpillsWithFileStream]] `mergeSpillsWithFileStream` Internal Method

[source, java]
----
long[] mergeSpillsWithFileStream(
  SpillInfo[] spills,
  File outputFile,
  @Nullable CompressionCodec compressionCodec)
----

`mergeSpillsWithFileStream`...FIXME

NOTE: `mergeSpillsWithFileStream` is used exclusively when `UnsafeShuffleWriter` is requested to <<mergeSpills, mergeSpills>>.

== [[getPeakMemoryUsedBytes]] Getting Peak Memory Used -- `getPeakMemoryUsedBytes` Method

[source, java]
----
long getPeakMemoryUsedBytes()
----

`getPeakMemoryUsedBytes` simply <<updatePeakMemoryUsed, updatePeakMemoryUsed>> and returns the internal <<peakMemoryUsedBytes, peakMemoryUsedBytes>> registry.

NOTE: `getPeakMemoryUsedBytes` is used exclusively when `UnsafeShuffleWriter` is requested to <<stop, stop>>.

== [[logging]] Logging

Enable `ALL` logging levels for `org.apache.spark.shuffle.sort.UnsafeShuffleWriter` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.shuffle.sort.UnsafeShuffleWriter=ALL
```

Refer to xref:ROOT:spark-logging.adoc[Logging].

== [[internal-registries]] Internal Properties

.UnsafeShuffleWriter's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,3",options="header",width="100%"]
|===
| Name
| Description

| mapStatus
a| [[mapStatus]] xref:scheduler:MapStatus.adoc[MapStatus]

Created when `UnsafeShuffleWriter` is requested to <<closeAndWriteOutput, close internal resources and write out merged spill files>> (with the xref:storage:BlockManager.adoc#shuffleServerId[BlockManagerId] of the <<blockManager, BlockManager>> and `partitionLengths`)

Returned when `UnsafeShuffleWriter` is requested to <<stop, stop>>

| partitioner
a| [[partitioner]] <<spark-rdd-Partitioner.adoc#, Partitioner>> (as used by the <<spark-shuffle-BaseShuffleHandle.adoc#dependency, ShuffleDependency>> of the <<handle, SerializedShuffleHandle>>)

Used when `UnsafeShuffleWriter` is requested for the following:

* <<open, open>> (and create a xref:shuffle:ShuffleExternalSorter.adoc[ShuffleExternalSorter] with the given <<spark-rdd-Partitioner.adoc#numPartitions, number of partitions>>)

* <<insertRecordIntoSorter, insertRecordIntoSorter>> (and request the <<spark-rdd-Partitioner.adoc#getPartition, partition for the key>>)

* <<mergeSpills, mergeSpills>>, <<mergeSpillsWithFileStream, mergeSpillsWithFileStream>> and <<mergeSpillsWithTransferTo, mergeSpillsWithTransferTo>> (for the <<spark-rdd-Partitioner.adoc#numPartitions, number of partitions>> to create partition lengths)

| peakMemoryUsedBytes
a| [[peakMemoryUsedBytes]] Peak memory used (in bytes) that is updated exclusively in <<updatePeakMemoryUsed, updatePeakMemoryUsed>> (after requesting the <<sorter, ShuffleExternalSorter>> for xref:shuffle:ShuffleExternalSorter.adoc#getPeakMemoryUsedBytes[getPeakMemoryUsedBytes])

Use <<getPeakMemoryUsedBytes, getPeakMemoryUsedBytes>> to access the current value

| serBuffer
a| [[serBuffer]] https://docs.oracle.com/javase/8/docs/api/java/io/ByteArrayOutputStream.html[java.io.ByteArrayOutputStream] of serialized data (written into a byte array of <<DEFAULT_INITIAL_SER_BUFFER_SIZE, 1MB>> initial size)

Used when `UnsafeShuffleWriter` is requested for the following:

* <<open, open>> (and create the internal <<serOutputStream, SerializationStream>>)

* <<insertRecordIntoSorter, insertRecordIntoSorter>>

Destroyed (`null`) when requested to <<closeAndWriteOutput, close internal resources and write out merged spill files>>

| serializer
a| [[serializer]] <<spark-SerializerInstance.adoc#, SerializerInstance>> (that is a new instance of the xref:rdd:ShuffleDependency.adoc#serializer[Serializer] of the <<spark-shuffle-BaseShuffleHandle.adoc#dependency, ShuffleDependency>> of the <<handle, SerializedShuffleHandle>>)

Used exclusively when `UnsafeShuffleWriter` is requested to <<open, open>> (and creates the <<serOutputStream, SerializationStream>>)

| serOutputStream
a| [[serOutputStream]] <<spark-SerializationStream.adoc#, SerializationStream>>  (that is created when the <<serializer, SerializerInstance>> is requested to <<spark-SerializerInstance.adoc#serializeStream, serializeStream>> with the <<serBuffer, ByteArrayOutputStream>>)

Used exclusively when `UnsafeShuffleWriter` is requested to <<insertRecordIntoSorter, insertRecordIntoSorter>>

Destroyed (`null`) when requested to <<closeAndWriteOutput, close internal resources and write out merged spill files>>

| shuffleId
a| [[shuffleId]] xref:rdd:ShuffleDependency.adoc#shuffleId[Shuffle ID] (of the <<spark-shuffle-BaseShuffleHandle.adoc#dependency, ShuffleDependency>> of the <<handle, SerializedShuffleHandle>>)

Used exclusively when requested to <<closeAndWriteOutput, close internal resources and write out merged spill files>>

| sorter
a| [[sorter]] xref:shuffle:ShuffleExternalSorter.adoc[ShuffleExternalSorter]

Initialized when `UnsafeShuffleWriter` is requested to <<open, open>> (while being <<creating-instance, created>>)

Used when `UnsafeShuffleWriter` is requested for the following:

* <<updatePeakMemoryUsed, Updating peak memory used>>

* <<write, Writing records>>

* <<closeAndWriteOutput, Closing internal resources and writing out merged spill files>>

* <<insertRecordIntoSorter, Inserting a record into ShuffleExternalSorter>>

* <<forceSorterToSpill, Forcing ShuffleExternalSorter to spill to disk (freeing up execution memory)>> (for testing)

* <<stop, stop>>

Destroyed (`null`) when requested to <<closeAndWriteOutput, close internal resources and write out merged spill files>>

| writeMetrics
a| [[writeMetrics]] <<spark-executor-ShuffleWriteMetrics.adoc#, ShuffleWriteMetrics>> (of the <<spark-TaskContext.adoc#taskMetrics, TaskMetrics>> of the <<taskContext, TaskContext>>)

Used when `UnsafeShuffleWriter` is requested for the following:

* <<open, open>> (and creates the <<sorter, ShuffleExternalSorter>>)

* <<mergeSpills, mergeSpills>>

* <<mergeSpillsWithFileStream, mergeSpillsWithFileStream>>

* <<mergeSpillsWithTransferTo, mergeSpillsWithTransferTo>>

|===
