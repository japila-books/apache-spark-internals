== [[NettyBlockRpcServer]] NettyBlockRpcServer -- NettyBlockTransferService's RpcHandler

`NettyBlockRpcServer` is a link:spark-RpcHandler.adoc[RpcHandler] that handles <<messages, messages>> for link:spark-NettyBlockTransferService.adoc[NettyBlockTransferService].

`NettyBlockRpcServer` is <<creating-instance, created>> when...FIXME

[[internal-registries]]
[[streamManager]]
`NettyBlockRpcServer` uses a link:spark-OneForOneStreamManager.adoc[OneForOneStreamManager] for...FIXME

[[messages]]
.NettyBlockRpcServer Messages
[cols="1,2",options="header",width="100%"]
|===
| Message
| Behaviour

| <<OpenBlocks, OpenBlocks>>
| Obtaining local blocks and registering them with the internal link:spark-OneForOneStreamManager.adoc[OneForOneStreamManager]

| <<UploadBlock, UploadBlock>>
| Deserializes a block and stores it in link:spark-BlockDataManager.adoc[BlockDataManager]
|===

TIP: Enable `TRACE` logging level to see received messages in the logs.

[[logging]]
[TIP]
====
Enable `TRACE` logging level for `org.apache.spark.network.netty.NettyBlockRpcServer` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.network.netty.NettyBlockRpcServer=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[OpenBlocks]][[receive-OpenBlocks]] Obtaining Local Blocks and Registering with Internal `OneForOneStreamManager` -- `OpenBlocks` Message Handler

When `OpenBlocks` arrives, `NettyBlockRpcServer` link:spark-BlockDataManager.adoc#getBlockData[requests block data (from `BlockDataManager`) for every block id in the message]. The block data is a collection of `ManagedBuffer` for every block id in the incoming message.

NOTE: `BlockDataManager` is given when <<creating-instance, `NettyBlockRpcServer` is created>>.

`NettyBlockRpcServer` then link:spark-OneForOneStreamManager.adoc#registerStream[registers a stream of ``ManagedBuffer``s (for the blocks) with the internal `StreamManager`] under `streamId`.

NOTE: The internal `StreamManager` is link:spark-OneForOneStreamManager.adoc[OneForOneStreamManager] and is created when <<creating-instance, `NettyBlockRpcServer` is created>>.

You should see the following TRACE message in the logs:

```
TRACE NettyBlockRpcServer: Registered streamId [streamId]  with [size] buffers
```

In the end, `NettyBlockRpcServer` responds with a `StreamHandle` (with the `streamId` and the number of blocks). The response is serialized as a `ByteBuffer`.

=== [[UploadBlock]][[receive-UploadBlock]] Deserializing Block and Storing in `BlockDataManager` -- `UploadBlock` Message Handler

When `UploadBlock` arrives, `NettyBlockRpcServer` deserializes the `metadata` of the input message to get the link:spark-rdd-StorageLevel.adoc[StorageLevel] and `ClassTag` of the block being uploaded.

NOTE: `metadata` is serialized before link:spark-NettyBlockTransferService.adoc#uploadBlock[`NettyBlockTransferService` sends a `UploadBlock` message] (using the internal `JavaSerializer`) that is given as `serializer` when <<creating-instance, `NettyBlockRpcServer` is created>>.

`NettyBlockRpcServer` creates a `BlockId` for the block id and requests the link:spark-BlockDataManager.adoc#putBlockData[`BlockDataManager` to store the block].

NOTE: The `BlockDataManager` is passed in when <<creating-instance, `NettyBlockRpcServer` is created>>.

In the end, `NettyBlockRpcServer` responds with a `0`-capacity `ByteBuffer`.

NOTE: `UploadBlock` is sent when link:spark-NettyBlockTransferService.adoc#uploadBlock[`NettyBlockTransferService` uploads a block].

=== [[creating-instance]] Creating NettyBlockRpcServer Instance

`NettyBlockRpcServer` takes the following when created:

* [[appId]] Application ID
* [[serializer]] `Serializer`
* [[blockManager]] link:spark-BlockDataManager.adoc[BlockDataManager]

`NettyBlockRpcServer` initializes the <<internal-registries, internal registries and counters>>.

=== [[receive]] Receiving RPC Messages -- `receive` Method

[source, scala]
----
receive(
  client: TransportClient,
  rpcMessage: ByteBuffer,
  responseContext: RpcResponseCallback): Unit
----

NOTE: `receive` is part of link:spark-RpcHandler.adoc#receive[RpcHandler Contract] to...FIXME.

`receive`...FIXME

=== [[receiveStream]] Receiving RPC Message with Streamed Data -- `receiveStream` Method

[source, scala]
----
receiveStream(
  client: TransportClient,
  messageHeader: ByteBuffer,
  responseContext: RpcResponseCallback): StreamCallbackWithID
----

NOTE: `receiveStream` is part of link:spark-RpcHandler.adoc#receiveStream[RpcHandler Contract] to receive a RPC message with a streamed data.

`receiveStream`...FIXME
