== [[ExecutorsListener]] ExecutorsListener Spark Listener

`ExecutorsListener` is a  link:spark-scheduler-SparkListener.adoc[SparkListener] that tracks <<internal-registries, executors and their tasks>> in a Spark application for link:spark-webui-StagePage.adoc[Stage Details] page, link:spark-webui-jobs.adoc[Jobs] tab and `/allexecutors` REST endpoint.

[[SparkListener-callbacks]]
.ExecutorsListener's SparkListener Callbacks (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Event Handler
| Description

| <<onApplicationStart, onApplicationStart>>
| May create an entry for the driver in <<executorToTaskSummary, executorToTaskSummary>> registry

| <<onExecutorAdded, onExecutorAdded>>
| May create an entry in <<executorToTaskSummary, executorToTaskSummary>> registry. It also makes sure that the number of entries for dead executors does not exceed link:spark-webui-properties.adoc#spark.ui.retainedDeadExecutors[spark.ui.retainedDeadExecutors] and removes excess.

Adds an entry to <<executorEvents, executorEvents>> registry and optionally removes the oldest if the number of entries exceeds link:spark-webui-properties.adoc#spark.ui.timeline.executors.maximum[spark.ui.timeline.executors.maximum].

| <<onExecutorBlacklisted, onExecutorBlacklisted>>
| FIXME

| <<onExecutorRemoved, onExecutorRemoved>>
| Marks an executor dead in <<executorToTaskSummary, executorToTaskSummary>> registry.

Adds an entry to <<executorEvents, executorEvents>> registry and optionally removes the oldest if the number of entries exceeds link:spark-webui-properties.adoc#spark.ui.timeline.executors.maximum[spark.ui.timeline.executors.maximum].

| <<onExecutorUnblacklisted, onExecutorUnblacklisted>>
| FIXME

| <<onNodeBlacklisted, onNodeBlacklisted>>
| FIXME

| <<onNodeUnblacklisted, onNodeUnblacklisted>>
| FIXME

| <<onTaskStart, onTaskStart>>
| May create an entry for an executor in <<executorToTaskSummary, executorToTaskSummary>> registry.

| <<onTaskEnd, onTaskEnd>>
| May create an entry for an executor in <<executorToTaskSummary, executorToTaskSummary>> registry.
|===

`ExecutorsListener` requires a link:spark-webui-StorageStatusListener.adoc[StorageStatusListener] and link:spark-SparkConf.adoc[SparkConf].

[[internal-registries]]
.ExecutorsListener's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Registry
| Description

| [[executorToTaskSummary]] `executorToTaskSummary`
| The lookup table for `ExecutorTaskSummary` per executor id.

Used to build a `ExecutorSummary` for `/allexecutors` REST endpoint, to display stdout and stderr logs in link:spark-webui-StagePage.adoc#tasks[Tasks] and link:spark-webui-StagePage.adoc#aggregated-metrics-by-executor[Aggregated Metrics by Executor] sections in link:spark-webui-StagePage.adoc[Stage Details] page.

| [[executorEvents]] `executorEvents`
| A collection of link:spark-scheduler-SparkListener.adoc#SparkListenerEvent[SparkListenerEvent]s.

Used to build the event timeline in link:spark-webui-AllJobsPage.adoc[AllJobsPage] and link:spark-webui-jobs.adoc#JobPage[Details for Job] pages.
|===

=== [[updateExecutorBlacklist]] `updateExecutorBlacklist` Method

CAUTION: FIXME

=== [[onExecutorBlacklisted]] Intercepting Executor Was Blacklisted Events -- `onExecutorBlacklisted` Callback

CAUTION: FIXME

=== [[onExecutorUnblacklisted]] Intercepting Executor Is No Longer Blacklisted Events -- `onExecutorUnblacklisted` Callback

CAUTION: FIXME

=== [[onNodeBlacklisted]] Intercepting Node Was Blacklisted Events -- `onNodeBlacklisted` Callback

CAUTION: FIXME

=== [[onNodeUnblacklisted]] Intercepting Node Is No Longer Blacklisted Events -- `onNodeUnblacklisted` Callback

CAUTION: FIXME

=== [[onApplicationStart]] Intercepting Application Started Events -- `onApplicationStart` Callback

[source, scala]
----
onApplicationStart(applicationStart: SparkListenerApplicationStart): Unit
----

NOTE: `onApplicationStart` is part of link:spark-scheduler-SparkListener.adoc#onApplicationStart[SparkListener contract] to announce that a Spark application has been started.

`onApplicationStart` takes `driverLogs` property from the input `applicationStart` (if defined) and finds the driver's active link:spark-blockmanager-StorageStatus.adoc[StorageStatus] (using the current link:spark-webui-StorageStatusListener.adoc[StorageStatusListener]). `onApplicationStart` then uses the driver's link:spark-blockmanager-StorageStatus.adoc[StorageStatus] (if defined) to set `executorLogs`.

.ExecutorTaskSummary and ExecutorInfo Attributes
[options="header",width="100%"]
|===
| ExecutorTaskSummary Attribute | SparkListenerApplicationStart Attribute
| `executorLogs` | `driverLogs` (if defined)
|===

=== [[onExecutorAdded]] Intercepting Executor Added Events -- `onExecutorAdded` Callback

[source, scala]
----
onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit
----

NOTE: `onExecutorAdded` is part of link:spark-scheduler-SparkListener.adoc#onExecutorAdded[SparkListener contract] to announce that a new executor has been registered with the Spark application.

`onExecutorAdded` finds the executor (using the input `executorAdded`) in the internal <<executorToTaskSummary, `executorToTaskSummary` registry>> and sets the attributes. If not found, `onExecutorAdded` creates a new entry.

.ExecutorTaskSummary and ExecutorInfo Attributes
[options="header",width="100%"]
|===
| ExecutorTaskSummary Attribute | ExecutorInfo Attribute
| `executorLogs` | `logUrlMap`
| `totalCores` | `totalCores`
| `tasksMax` | `totalCores` / xref:ROOT:configuration-properties.adoc#spark.task.cpus[spark.task.cpus]
|===

`onExecutorAdded` adds the input `executorAdded` to <<executorEvents, `executorEvents` collection>>. If the number of elements in `executorEvents` collection is greater than link:spark-webui-properties.adoc#spark.ui.timeline.executors.maximum[spark.ui.timeline.executors.maximum] configuration property, the first/oldest event is removed.

`onExecutorAdded` removes the oldest dead executor from <<executorToTaskSummary, `executorToTaskSummary` lookup table>> if their number is greater than link:spark-webui-properties.adoc#spark.ui.retainedDeadExecutors[spark.ui.retainedDeadExecutors].

=== [[onExecutorRemoved]] Intercepting Executor Removed Events -- `onExecutorRemoved` Callback

[source, scala]
----
onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit
----

NOTE: `onExecutorRemoved` is part of link:spark-scheduler-SparkListener.adoc#onExecutorRemoved[SparkListener contract] to announce that an executor has been unregistered with the Spark application.

`onExecutorRemoved` adds the input `executorRemoved` to <<executorEvents, `executorEvents` collection>>. It then removes the oldest event if the number of elements in `executorEvents` collection is greater than link:spark-webui-properties.adoc#spark.ui.timeline.executors.maximum[spark.ui.timeline.executors.maximum] configuration property.

The executor is marked as removed/inactive in <<executorToTaskSummary, `executorToTaskSummary` lookup table>>.

=== [[onTaskStart]] Intercepting Task Started Events -- `onTaskStart` Callback

[source, scala]
----
onTaskStart(taskStart: SparkListenerTaskStart): Unit
----

NOTE: `onTaskStart` is part of link:spark-scheduler-SparkListener.adoc#onTaskStart[SparkListener contract] to announce that a task has been started.

`onTaskStart` increments `tasksActive` for the executor (using the input `SparkListenerTaskStart`).

.ExecutorTaskSummary and SparkListenerTaskStart Attributes
[options="header",width="100%"]
|===
| ExecutorTaskSummary Attribute | Description
| `tasksActive` | Uses `taskStart.taskInfo.executorId`.
|===

=== [[onTaskEnd]] Intercepting Task End Events -- `onTaskEnd` Callback

[source, scala]
----
onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit
----

NOTE: `onTaskEnd` is part of link:spark-scheduler-SparkListener.adoc#onTaskEnd[SparkListener contract] to announce that a task has ended.

`onTaskEnd` takes link:spark-scheduler-TaskInfo.adoc[TaskInfo] from the input `taskEnd` (if available).

Depending on the reason for `SparkListenerTaskEnd` `onTaskEnd` does the following:

.`onTaskEnd` Behaviour per `SparkListenerTaskEnd` Reason
[cols="1,2",options="header",width="100%"]
|===
| `SparkListenerTaskEnd` Reason | `onTaskEnd` Behaviour
| `Resubmitted` | Does nothing
| `ExceptionFailure` | Increment `tasksFailed`
| _anything_ | Increment `tasksComplete`
|===

`tasksActive` is decremented but only when the number of active tasks for the executor is greater than `0`.

.ExecutorTaskSummary and `onTaskEnd` Behaviour
[options="header",width="100%"]
|===
| ExecutorTaskSummary Attribute | Description
| `tasksActive` | Decremented if greater than 0.
| `duration` | Uses `taskEnd.taskInfo.duration`
|===

If the `TaskMetrics` (in the input `taskEnd`) is available, the metrics are added to the `taskSummary` for the task's executor.

.Task Metrics and Task Summary
[cols="1,2",options="header",width="100%"]
|===
| Task Summary | Task Metric
| `inputBytes` | `inputMetrics.bytesRead`
| `inputRecords` | `inputMetrics.recordsRead`
| `outputBytes` | `outputMetrics.bytesWritten`
| `outputRecords` | `outputMetrics.recordsWritten`
| `shuffleRead` | `shuffleReadMetrics.remoteBytesRead`
| `shuffleWrite` | link:spark-executor-ShuffleWriteMetrics.adoc#bytesWritten[shuffleWriteMetrics.bytesWritten]
| `jvmGCTime` | `metrics.jvmGCTime`
|===

=== [[activeStorageStatusList]] Finding Active BlockManagers -- `activeStorageStatusList` Method

[source, scala]
----
activeStorageStatusList: Seq[StorageStatus]
----

`activeStorageStatusList` requests <<storageStatusListener, StorageStatusListener>> for link:spark-webui-StorageStatusListener.adoc#storageStatusList[active BlockManagers (on executors)].

[NOTE]
====
`activeStorageStatusList` is used when:

* FIXME

* `AllExecutorListResource` does `executorList`
* `ExecutorListResource` does `executorList`
* `ExecutorsListener` gets informed that the <<onApplicationStart, Spark application has started>>, <<onNodeBlacklisted, onNodeBlacklisted>>, and <<onNodeUnblacklisted, onNodeUnblacklisted>>
====
