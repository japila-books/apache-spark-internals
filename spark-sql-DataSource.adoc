== [[DataSource]] DataSource -- Pluggable Data Provider Framework

`DataSource` is among the main components of *Data Source API* in Spark SQL (together with link:spark-sql-DataFrameReader.adoc[DataFrameReader] for loading datasets, link:spark-sql-DataFrameWriter.adoc[DataFrameWriter] for saving datasets and `StreamSourceProvider` for creating streaming sources).

`DataSource` models a **pluggable data provider framework** with <<providers, extension points>> for Spark SQL integrators to expand the list of supported external data sources in Spark SQL.

[[providers]]
.DataSource's Provider (and Format) Contracts
[cols="1,3",options="header",width="100%"]
|===
| Extension Point
| Description

| [[CreatableRelationProvider]] link:spark-sql-CreatableRelationProvider.adoc[CreatableRelationProvider]
| Data source that saves the result of a structured query per save mode and returns the schema

| [[FileFormat]] `FileFormat`
a| Used in:

* <<sourceSchema, sourceSchema>> for streamed reading

* <<write, write>> for writing a `DataFrame` to a `DataSource` (as part of creating a table as select)

| [[RelationProvider]] link:spark-sql-RelationProvider.adoc[RelationProvider]
| Data source that supports schema inference and can be accessed using SQL's `USING` clause

| [[SchemaRelationProvider]] link:spark-sql-SchemaRelationProvider.adoc[SchemaRelationProvider]
| Data source that requires a user-defined schema

| `StreamSourceProvider`
a| Used in:

* <<sourceSchema, sourceSchema>> and <<createSource, createSource>> for streamed reading

* <<createSink, createSink>> for streamed writing

* <<resolveRelation, resolveRelation>> for resolved link:spark-sql-BaseRelation.adoc[BaseRelation].
|===

As a user, you interact with `DataSource` by link:spark-sql-DataFrameReader.adoc[DataFrameReader] (when you execute link:spark-sql-SparkSession.adoc#read[spark.read] or link:spark-sql-SparkSession.adoc#readStream[spark.readStream]) or SQL's `CREATE TABLE USING`.

[source, scala]
----
// Batch reading
val people: DataFrame = spark.read
  .format("csv")
  .load("people.csv")

// Streamed reading
val messages: DataFrame = spark.readStream
  .format("kafka")
  .option("subscribe", "topic")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .load
----

`DataSource` uses a link:spark-sql-SparkSession.adoc[SparkSession], a class name, a collection of `paths`, optional user-specified link:spark-sql-schema.adoc[schema], a collection of partition columns, a bucket specification, and configuration options.

NOTE: Data source is also called a *table provider*.

=== [[writeAndRead]] Writing DataFrame to Data Source per Save Mode Followed by Reading Rows Back (as BaseRelation) -- `writeAndRead` Method

[source, scala]
----
writeAndRead(mode: SaveMode, data: DataFrame): BaseRelation
----

CAUTION: FIXME

NOTE: `writeAndRead` is used exclusively when link:spark-sql-LogicalPlan-RunnableCommand.adoc#CreateDataSourceTableAsSelectCommand[CreateDataSourceTableAsSelectCommand] is executed.

=== [[providingClass]] `providingClass` Property

CAUTION: FIXME

=== [[write]] Writing DataFrame to Data Source Per Save Mode -- `write` Method

[source, scala]
----
write(mode: SaveMode, data: DataFrame): BaseRelation
----

`write` writes the result of executing a structured query (as link:spark-sql-DataFrame.adoc[DataFrame]) to a data source per save `mode`.

Internally, `write` <<lookupDataSource, looks up the data source>> and branches off per <<providingClass, providingClass>>.

[[write-providingClass-branches]]
.write's Branches per Supported providingClass (in execution order)
[width="100%",cols="1,2",options="header"]
|===
| providingClass
| Description

| link:spark-sql-CreatableRelationProvider.adoc[CreatableRelationProvider]
| Executes link:spark-sql-CreatableRelationProvider.adoc#createRelation[CreatableRelationProvider.createRelation]

| `FileFormat`
| <<writeInFileFormat, writeInFileFormat>>

| _others_
| Reports a `RuntimeException`
|===

NOTE: `write` does not support the internal `CalendarIntervalType` in the link:spark-sql-schema.adoc[schema of `data` `DataFrame`] and throws a `AnalysisException` when there is one.

NOTE: `write` is used exclusively when link:spark-sql-LogicalPlan-RunnableCommand.adoc#SaveIntoDataSourceCommand[SaveIntoDataSourceCommand] is executed.

=== [[writeInFileFormat]] `writeInFileFormat` Internal Method

CAUTION: FIXME

For `FileFormat` data sources, `write` takes all `paths` and `path` option and makes sure that there is only one.

NOTE: `write` uses Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/Path.html[Path] to access the https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem] and calculate the qualified output path.

`write` does `PartitioningUtils.validatePartitionColumn`.

CAUTION: FIXME What is `PartitioningUtils.validatePartitionColumn` for?

When appending to a table, ...FIXME

In the end, `write` (for a `FileFormat` data source) link:spark-sql-SessionState.adoc#executePlan[prepares a `InsertIntoHadoopFsRelationCommand` logical plan] with link:spark-sql-QueryExecution.adoc#toRdd[executes] it.

CAUTION: FIXME Is `toRdd` a job execution?

=== [[createSource]] `createSource` Method

[source, scala]
----
createSource(metadataPath: String): Source
----

CAUTION: FIXME

=== [[createSink]] `createSink` Method

CAUTION: FIXME

=== [[creating-instance]] Creating DataSource Instance

[source, scala]
----
class DataSource(
  sparkSession: SparkSession,
  className: String,
  paths: Seq[String] = Nil,
  userSpecifiedSchema: Option[StructType] = None,
  partitionColumns: Seq[String] = Seq.empty,
  bucketSpec: Option[BucketSpec] = None,
  options: Map[String, String] = Map.empty,
  catalogTable: Option[CatalogTable] = None)
----

When being created, `DataSource` first <<lookupDataSource, looks up the providing class>> given `className` (considering it an alias or a fully-qualified class name) and computes the <<sourceSchema, name and schema>> of the data source.

NOTE: `DataSource` does the initialization lazily on demand and only once.

==== [[sourceSchema]] `sourceSchema` Internal Method

[source, scala]
----
sourceSchema(): SourceInfo
----

`sourceSchema` returns the name and link:spark-sql-schema.adoc[schema] of the data source for streamed reading.

CAUTION: FIXME Why is the method called? Why does this bother with streamed reading and data sources?!

It supports two class hierarchies, i.e. `FileFormat` and Structured Streaming's `StreamSourceProvider` data sources.

Internally, `sourceSchema` first creates an instance of the data source and...

CAUTION: FIXME Finish...

For Structured Streaming's `StreamSourceProvider` data sources, `sourceSchema` relays calls to `StreamSourceProvider.sourceSchema`.

For `FileFormat` data sources, `sourceSchema` makes sure that `path` option was specified.

TIP: `path` is looked up in a case-insensitive way so `paTh` and `PATH` and `pAtH` are all acceptable. Use the lower-case version of `path`, though.

NOTE: `path` can use https://en.wikipedia.org/wiki/Glob_%28programming%29[glob pattern] (not regex syntax), i.e. contain any of `{}[]*?\` characters.

It checks whether the path exists if a glob pattern is not used. In case it did not exist you will see the following `AnalysisException` exception in the logs:

```
scala> spark.read.load("the.file.does.not.exist.parquet")
org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/jacek/dev/oss/spark/the.file.does.not.exist.parquet;
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:375)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:364)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
  at scala.collection.immutable.List.flatMap(List.scala:344)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:364)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:132)
  ... 48 elided
```

If link:spark-sql-SQLConf.adoc#spark.sql.streaming.schemaInference[spark.sql.streaming.schemaInference] is disabled and the data source is different than `TextFileFormat`, and the input `userSpecifiedSchema` is not specified, the following `IllegalArgumentException` exception is thrown:

[options="wrap"]
----
Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.
----

CAUTION: FIXME I don't think the exception will ever happen for non-streaming sources since the schema is going to be defined earlier. When?

Eventually, it returns a `SourceInfo` with `FileSource[path]` and the schema (as calculated using the <<inferFileFormatSchema, inferFileFormatSchema>> internal method).

For any other data source, it throws `UnsupportedOperationException` exception:

```
Data source [className] does not support streamed reading
```

==== [[inferFileFormatSchema]] `inferFileFormatSchema` Internal Method

[source, scala]
----
inferFileFormatSchema(format: FileFormat): StructType
----

`inferFileFormatSchema` private method computes (aka _infers_) schema (as link:spark-sql-StructType.adoc[StructType]). It returns `userSpecifiedSchema` if specified or uses `FileFormat.inferSchema`. It throws a `AnalysisException` when is unable to infer schema.

It uses `path` option for the list of directory paths.

NOTE: It is used by <<sourceSchema, DataSource.sourceSchema>> and <<createSource, DataSource.createSource>> when `FileFormat` is processed.

==== [[lookupDataSource]] `lookupDataSource` Internal Method

[source, scala]
----
lookupDataSource(provider0: String): Class[_]
----

Internally, `lookupDataSource` first searches the classpath for available link:spark-sql-DataSourceRegister.adoc[DataSourceRegister] providers (using Java's link:++https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader.load] method) to find the requested data source by short name (alias), e.g. `parquet` or `kafka`.

If a `DataSource` could not be found by short name, `lookupDataSource` tries to load the class given the input `provider0` or its variant `provider0.DefaultSource` (with `.DefaultSource` suffix).

NOTE: You can reference your own custom `DataSource` in your code by link:spark-sql-DataFrameWriter.adoc#format[DataFrameWriter.format] method which is the alias or fully-qualified class name.

There has to be one data source registered only or you will see the following `RuntimeException`:

[options="wrap"]
----
Multiple sources found for [provider] ([comma-separated class names]), please specify the fully qualified class name.
----

=== [[resolveRelation]] Creating BaseRelation -- `resolveRelation` Method

[source, scala]
----
resolveRelation(checkFilesExist: Boolean = true): BaseRelation
----

`resolveRelation` resolves (i.e. creates) a link:spark-sql-BaseRelation.adoc[BaseRelation].

Internally, `resolveRelation` creates an instance of <<providingClass, providingClass>> (of a `DataSource`) and branches off per its type, i.e. link:spark-sql-SchemaRelationProvider.adoc[SchemaRelationProvider], link:spark-sql-RelationProvider.adoc[RelationProvider] or <<FileFormat, FileFormat>>.

.Resolving BaseRelation per Providers
[cols="1,3",options="header",width="100%"]
|===
| Provider
| Behaviour

| `SchemaRelationProvider`
| Executes link:spark-sql-SchemaRelationProvider.adoc#createRelation[SchemaRelationProvider.createRelation] with the provided schema

| `RelationProvider`
| Executes link:spark-sql-RelationProvider.adoc#createRelation[RelationProvider.createRelation]

| `FileFormat`
| Creates a link:spark-sql-BaseRelation.adoc#HadoopFsRelation[HadoopFsRelation]
|===

[NOTE]
====
`resolveRelation` is used when:

* `DataSource` <<writeAndRead, writes and reads>> the result of a link:spark-sql-DataFrame.adoc[structured query] (when <<providingClass, providingClass>> is of type `FileFormat`)
* `DataFrameReader` link:spark-sql-DataFrameReader.adoc#load[loads data from a data source that supports multiple paths]
* `TextInputCSVDataSource` and `TextInputJsonDataSource` are requested to infer schema
* `CreateDataSourceTableCommand` runnable command is link:spark-sql-LogicalPlan-RunnableCommand-CreateDataSourceTableCommand.adoc#run[executed]
* `CreateTempViewUsing` runnable command is executed
* `FindDataSourceTable` does `readDataSourceTable`
* `ResolveSQLOnFile` converts a logical plan (when <<providingClass, providingClass>> is of type `FileFormat`)
* `HiveMetastoreCatalog` does `convertToLogicalRelation`
* Structured Streaming's `FileStreamSource` creates batches of records

====
