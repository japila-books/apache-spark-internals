== [[TaskContext]] TaskContext

`TaskContext` is the <<contract, contract>> for contextual information about a link:spark-taskscheduler-tasks.adoc[Task] in Spark that allows for <<registering-task-listeners, registering task listeners>>.

You can access the active `TaskContext` instance using <<get, TaskContext.get>> method.

[source, scala]
----
import org.apache.spark.TaskContext
val ctx = TaskContext.get
----

Using `TaskContext` you can <<getLocalProperty, access local properties>> that were set by the driver.

NOTE: `TaskContext` is serializable.

=== [[contract]] TaskContext Contract

[source, scala]
----
trait TaskContext {
  def taskSucceeded(index: Int, result: Any)
  def jobFailed(exception: Exception)
}
----

.TaskContext Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[stageId]] `stageId`
| Id of the link:spark-DAGScheduler-Stage.adoc[Stage] the task belongs to.

Used when...

| [[partitionId]] `partitionId`
| Id of the link:spark-rdd-Partition.adoc[Partition] computed by the task.

Used when...

| [[attemptNumber]] `attemptNumber`
| Specifies how many times the task has been attempted (starting from 0).

Used when...

| [[taskAttemptId]] `taskAttemptId`
| Id of the attempt of the task.

Used when...

| [[getMetricsSources]] `getMetricsSources`
| Gives all the metrics sources by `sourceName` which are associated with the instance that runs the task.

| [[getLocalProperty]] `getLocalProperty`
| Used when...

Accesses local properties set by the driver using link:spark-sparkcontext-local-properties.adoc#setLocalProperty[SparkContext.setLocalProperty].

| [[taskMetrics]] `taskMetrics`
| link:spark-taskscheduler-taskmetrics.adoc[TaskMetrics] of the active link:spark-taskscheduler-tasks.adoc[Task].

Used when...

| [[taskMemoryManager]] `taskMemoryManager`
| Used when...

| [[registerAccumulator]] `registerAccumulator`
| Used when...

| [[isCompleted]] `isCompleted`
| Used when...

| [[isInterrupted]] `isInterrupted`
| A flag that is enabled when a task was killed.

Used when...

| [[addTaskCompletionListener]] `addTaskCompletionListener`
| Registers a `TaskCompletionListener`

Used when...

| [[addTaskFailureListener]] `addTaskFailureListener`
| Registers a `TaskFailureListener`

Used when...

|===

=== [[unset]] `unset` Method

CAUTION: FIXME

=== [[setTaskContext]] `setTaskContext` Method

CAUTION: FIXME

=== [[get]] Accessing Active TaskContext -- `get` Method

[source, scala]
----
get(): TaskContext
----

`get` method returns the `TaskContext` instance for an active task (as a link:spark-taskscheduler-TaskContextImpl.adoc[TaskContextImpl]). There can only be one instance and tasks can use the object to access contextual information about themselves.

[source, scala]
----
val rdd = sc.range(0, 3, numSlices = 3)

scala> rdd.partitions.size
res0: Int = 3

rdd.foreach { n =>
  import org.apache.spark.TaskContext
  val tc = TaskContext.get
  val msg = s"""|-------------------
                |partitionId:   ${tc.partitionId}
                |stageId:       ${tc.stageId}
                |attemptNum:    ${tc.attemptNumber}
                |taskAttemptId: ${tc.taskAttemptId}
                |-------------------""".stripMargin
  println(msg)
}
----

NOTE: `TaskContext` object uses https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[ThreadLocal] to keep it thread-local, i.e. to associate state with the thread of a task.

=== [[registering-task-listeners]] Registering Task Listeners

Using `TaskContext` object you can register task listeners for <<addTaskCompletionListener, task completion regardless of the final state>> and <<addTaskFailureListener, task failures only>>.

==== [[addTaskCompletionListener]] `addTaskCompletionListener` Method

[source, scala]
----
addTaskCompletionListener(listener: TaskCompletionListener): TaskContext
addTaskCompletionListener(f: (TaskContext) => Unit): TaskContext
----

`addTaskCompletionListener` methods register a `TaskCompletionListener` listener to be executed on task completion.

NOTE: It will be executed regardless of the final state of a task - success, failure, or cancellation.

[source, scala]
----
val rdd = sc.range(0, 5, numSlices = 1)

import org.apache.spark.TaskContext
val printTaskInfo = (tc: TaskContext) => {
  val msg = s"""|-------------------
                |partitionId:   ${tc.partitionId}
                |stageId:       ${tc.stageId}
                |attemptNum:    ${tc.attemptNumber}
                |taskAttemptId: ${tc.taskAttemptId}
                |-------------------""".stripMargin
  println(msg)
}

rdd.foreachPartition { _ =>
  val tc = TaskContext.get
  tc.addTaskCompletionListener(printTaskInfo)
}
----

==== [[addTaskFailureListener]] `addTaskFailureListener` Method

[source, scala]
----
addTaskFailureListener(listener: TaskFailureListener): TaskContext
addTaskFailureListener(f: (TaskContext, Throwable) => Unit): TaskContext
----

`addTaskFailureListener` methods register a `TaskFailureListener` listener to be executed on task failure only. It can be executed multiple times since a task can be re-attempted when it fails.

[source, scala]
----
val rdd = sc.range(0, 2, numSlices = 2)

import org.apache.spark.TaskContext
val printTaskErrorInfo = (tc: TaskContext, error: Throwable) => {
  val msg = s"""|-------------------
                |partitionId:   ${tc.partitionId}
                |stageId:       ${tc.stageId}
                |attemptNum:    ${tc.attemptNumber}
                |taskAttemptId: ${tc.taskAttemptId}
                |error:         ${error.toString}
                |-------------------""".stripMargin
  println(msg)
}

val throwExceptionForOddNumber = (n: Long) => {
  if (n % 2 == 1) {
    throw new Exception(s"No way it will pass for odd number: $n")
  }
}

// FIXME It won't work.
rdd.map(throwExceptionForOddNumber).foreachPartition { _ =>
  val tc = TaskContext.get
  tc.addTaskFailureListener(printTaskErrorInfo)
}

// Listener registration matters.
rdd.mapPartitions { (it: Iterator[Long]) =>
  val tc = TaskContext.get
  tc.addTaskFailureListener(printTaskErrorInfo)
  it
}.map(throwExceptionForOddNumber).count
----

=== [[getPartitionId]] (Unused) Accessing Partition Id -- `getPartitionId` Method

[source, scala]
----
getPartitionId(): Int
----

`getPartitionId` <<get, gets the active `TaskContext`>> and returns <<partitionId, partitionId>> or `0` (if `TaskContext` not available).

NOTE: `getPartitionId` is not used.
