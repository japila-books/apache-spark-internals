== [[DataFrameReader]] DataFrameReader -- Reading Datasets from External Data Sources

`DataFrameReader` is an interface to read datasets from external data sources, e.g. <<creating-dataframes-from-files, files>>, <<creating-dataframes-from-tables, Hive tables>>, <<jdbc, JDBC>> or <<loading-dataset-of-string, Dataset[String]>>, into untyped `DataFrames` (mostly) or typed `Datasets`.

`DataFrameReader` is available using link:spark-sql-SparkSession.adoc#read[SparkSession.read].

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...

import org.apache.spark.sql.DataFrameReader
val reader: DataFrameReader = spark.read
----

`DataFrameReader` supports many <<creating-dataframes-from-files, file formats>> natively and offers the <<format, interface to define custom file formats>>.

NOTE: `DataFrameReader` assumes <<parquet, parquet>> file format by default that you can change using link:spark-sql-settings.adoc#spark.sql.sources.default[spark.sql.sources.default] property.

After you have described the *reading pipeline* to read datasets from an external data source, you eventually trigger the loading using format-agnostic <<load, load>> or format-specific (e.g. <<json, json>>, <<csv, csv>>) operators that create untyped `DataFrames`.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...

import org.apache.spark.sql.DataFrame

// Using format-agnostic load operator
val csvs: DataFrame = spark
  .read
  .format("csv")
  .option("header", true)
  .option("inferSchema", true)
  .load("*.csv")

// Using format-specific load operator
val jsons: DataFrame = spark
  .read
  .json("metrics/*.json")
----

---

(*New in Spark 2.0*) `DataFrameReader` can read text files using <<textFile, textFile>> methods that return typed `Datasets`.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...

import org.apache.spark.sql.Dataset
val lines: Dataset[String] = spark
  .read
  .textFile("README.md")
----

---

[[loading-dataset-of-string]]
(*New in Spark 2.2*) `DataFrameReader` can load datasets from `Dataset[String]` (with lines being complete "files") using format-specific <<csv, csv>> and <<json, json>> operators.

[source, scala]
----
val csvLine = "0,Warsaw,Poland"

import org.apache.spark.sql.Dataset
val cities: Dataset[String] = Seq(csvLine).toDS
scala> cities.show
+---------------+
|          value|
+---------------+
|0,Warsaw,Poland|
+---------------+

// Define schema explicitly (as below)
// or
// option("header", true) + option("inferSchema", true)
import org.apache.spark.sql.types.StructType
val schema = new StructType()
  .add($"id".long.copy(nullable = false))
  .add($"city".string)
  .add($"country".string)
scala> schema.printTreeString
root
 |-- id: long (nullable = false)
 |-- city: string (nullable = true)
 |-- country: string (nullable = true)

import org.apache.spark.sql.DataFrame
val citiesDF: DataFrame = spark
  .read
  .schema(schema)
  .csv(cities)
scala> citiesDF.show
+---+------+-------+
| id|  city|country|
+---+------+-------+
|  0|Warsaw| Poland|
+---+------+-------+
----

=== [[format]] Defining Format -- `format` method

[source, scala]
----
format(source: String): DataFrameReader
----

You use `format` to configure `DataFrameReader` to use appropriate `source` format.

Supported data formats:

* `json`
* `csv` (since **2.0.0**)
* `parquet` (see link:spark-parquet.adoc[Parquet])
* `orc`
* `text`
* <<jdbc, jdbc>>
* `libsvm` -- only when used in `format("libsvm")`

NOTE: You can link:spark-sql-datasource-custom-formats.adoc[define your own custom file formats].

=== [[schema]] Defining Schema -- `schema` method

[source, scala]
----
schema(schema: StructType): DataFrameReader
----

You can specify a `schema` of the input data source.

[source, scala]
----
import org.apache.spark.sql.types.StructType
val schema = new StructType()
  .add($"id".long.copy(nullable = false))
  .add($"city".string)
  .add($"country".string)

scala> schema.printTreeString
root
 |-- id: long (nullable = false)
 |-- city: string (nullable = true)
 |-- country: string (nullable = true)

import org.apache.spark.sql.DataFrameReader
val r: DataFrameReader = spark.read.schema(schema)
----

NOTE: Some formats can infer schema from datasets, e.g. <<csv, csv>>, using <<options, options>>.

TIP: Read up on link:spark-sql-schema.adoc[Schema].

=== [[option]][[options]] Defining Loading Options -- `option` and `options` Methods

[source, scala]
----
option(key: String, value: String): DataFrameReader
option(key: String, value: Boolean): DataFrameReader  // <1>
option(key: String, value: Long): DataFrameReader     // <1>
option(key: String, value: Double): DataFrameReader   // <1>
----
<1> Available as of Spark 2.0

You can also use `options` method to describe different options in a single `Map`.

[source, scala]
----
options(options: scala.collection.Map[String, String]): DataFrameReader
----

=== [[load]] Loading Data from Data Sources with Multiple Files Support -- `load` Method

[source, scala]
----
load(): DataFrame
load(path: String): DataFrame
load(paths: String*): DataFrame
----

`load` loads a data from data sources that support multiple `paths` and represents it as an untyped link:spark-sql-DataFrame.adoc[DataFrame].

Internally, `load` link:spark-sql-DataSource.adoc#creating-instance[creates a `DataSource`] (for the current link:spark-sql-SparkSession.adoc[SparkSession], a user-specified <<schema, schema>>, a source <<format, format>> and <<options, options>>). It then immediately link:spark-sql-DataSource.adoc#resolveRelation[resolves it] and link:spark-sql-SparkSession.adoc#baseRelationToDataFrame[converts `BaseRelation` into a `DataFrame`].

=== [[creating-dataframes-from-files]] Loading Datasets from Files (into DataFrames) Using Format-Specific Load Operators

`DataFrameReader` supports the following file formats:

* <<json, JSON>>
* <<csv, CSV>>
* <<parquet, parquet>>
* <<orc, ORC>>
* <<text, text>>

==== [[json]] `json` method

[source, scala]
----
json(path: String): DataFrame
json(paths: String*): DataFrame
json(jsonRDD: RDD[String]): DataFrame
----

New in **2.0.0**: `prefersDecimal`

==== [[csv]] `csv` method

[source, scala]
----
csv(path: String): DataFrame
csv(paths: String*): DataFrame
----

==== [[parquet]] `parquet` method

[source, scala]
----
parquet(path: String): DataFrame
parquet(paths: String*): DataFrame
----

The supported options:

* <<compression, compression>> (default: `snappy`)

New in *2.0.0*: `snappy` is the default Parquet codec. See https://github.com/apache/spark/commit/2f0b882e5c8787b09bedcc8208e6dcc5662dbbab[[SPARK-14482\][SQL\] Change default Parquet codec from gzip to snappy].

[[compression]] The compressions supported:

* `none` or `uncompressed`
* `snappy` - the default codec in Spark *2.0.0*.
* `gzip` - the default codec in Spark before *2.0.0*
* `lzo`

[source, scala]
----
val tokens = Seq("hello", "henry", "and", "harry")
  .zipWithIndex
  .map(_.swap)
  .toDF("id", "token")

val parquetWriter = tokens.write
parquetWriter.option("compression", "none").save("hello-none")

// The exception is mostly for my learning purposes
// so I know where and how to find the trace to the compressions
// Sorry...
scala> parquetWriter.option("compression", "unsupported").save("hello-unsupported")
java.lang.IllegalArgumentException: Codec [unsupported] is not available. Available codecs are uncompressed, gzip, lzo, snappy, none.
  at org.apache.spark.sql.execution.datasources.parquet.ParquetOptions.<init>(ParquetOptions.scala:43)
  at org.apache.spark.sql.execution.datasources.parquet.DefaultSource.prepareWrite(ParquetRelation.scala:77)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$4.apply(InsertIntoHadoopFsRelation.scala:122)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$4.apply(InsertIntoHadoopFsRelation.scala:122)
  at org.apache.spark.sql.execution.datasources.BaseWriterContainer.driverSideSetup(WriterContainer.scala:103)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:141)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:116)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:116)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:53)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:116)
  at org.apache.spark.sql.execution.command.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:61)
  at org.apache.spark.sql.execution.command.ExecutedCommand.sideEffectResult(commands.scala:59)
  at org.apache.spark.sql.execution.command.ExecutedCommand.doExecute(commands.scala:73)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:118)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:118)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:137)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:134)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:117)
  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:65)
  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:65)
  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:390)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:230)
  ... 48 elided
----

==== [[orc]] `orc` method

[source, scala]
----
orc(path: String): DataFrame
orc(paths: String*): DataFrame
----

*Optimized Row Columnar (ORC)* file format is a highly efficient columnar format to store Hive data with more than 1,000 columns and improve performance. ORC format was introduced in Hive version 0.11 to use and retain the type information from the table definition.

TIP: Read https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC[ORC Files] document to learn about the ORC file format.

==== [[text]] `text` method

`text` method loads a text file.

[source, scala]
----
text(path: String): DataFrame
text(paths: String*): DataFrame
----

===== [[text-example]] Example

[source, scala]
----
val lines: Dataset[String] = spark.read.text("README.md").as[String]

scala> lines.show
+--------------------+
|               value|
+--------------------+
|      # Apache Spark|
|                    |
|Spark is a fast a...|
|high-level APIs i...|
|supports general ...|
|rich set of highe...|
|MLlib for machine...|
|and Spark Streami...|
|                    |
|<http://spark.apa...|
|                    |
|                    |
|## Online Documen...|
|                    |
|You can find the ...|
|guide, on the [pr...|
|and [project wiki...|
|This README file ...|
|                    |
|   ## Building Spark|
+--------------------+
only showing top 20 rows
----

=== [[table]][[creating-dataframes-from-tables]] Loading Datasets from Tables (into DataFrames) -- `table` Method

[source, scala]
----
table(tableName: String): DataFrame
----

`table` loads `tableName` table into an untyped link:spark-sql-DataFrame.adoc[DataFrame].

[source, scala]
----
scala> spark.sql("SHOW TABLES").show(false)
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
|dafa     |false      |
+---------+-----------+

scala> spark.read.table("dafa").show(false)
+---+-------+
|id |text   |
+---+-------+
|1  |swiecie|
|0  |hello  |
+---+-------+
----

CAUTION: FIXME The method uses `spark.sessionState.sqlParser.parseTableIdentifier(tableName)` and `spark.sessionState.catalog.lookupRelation`. Would be nice to learn a bit more on their internals, huh?

=== [[jdbc]] Loading Data From External Table using JDBC -- `jdbc` Method

[source, scala]
----
jdbc(url: String, table: String, properties: Properties): DataFrame
jdbc(url: String,
  table: String,
  predicates: Array[String],
  connectionProperties: Properties): DataFrame
jdbc(url: String,
  table: String,
  columnName: String,
  lowerBound: Long,
  upperBound: Long,
  numPartitions: Int,
  connectionProperties: Properties): DataFrame
----

`jdbc` loads data from an external table using JDBC and represents it as an untyped link:spark-sql-DataFrame.adoc[DataFrame].

[[JDBCOptions]]
[[jdbc-options]]
.Options for JDBC Data Source (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Option
| Description

| `batchsize`
| The minimum value is `1`

Defaults to `1000`

| `createTableColumnTypes`
|

| `createTableOptions`
|

| `dbtable`
| (*required*)

| `driver`
| (*recommended*) JDBC driver's class name.

When defined, the class will get registered with Java's https://docs.oracle.com/javase/8/docs/api/java/sql/DriverManager.html[java.sql.DriverManager]

| `fetchsize`
|

Defaults to `0`

| `isolationLevel`
a|

One of the following:

* NONE
* READ_UNCOMMITTED (default)
* READ_COMMITTED
* REPEATABLE_READ
* SERIALIZABLE

| [[jdbc-lowerBound]] `lowerBound`
| Lower bound of partition column

| [[jdbc-numPartitions]] `numPartitions`
| Number of partitions

| [[jdbc-partitionColumn]] `partitionColumn`
| Name of the column used to partition dataset (using a `JDBCPartitioningInfo`).

Used in `JdbcRelationProvider` to link:spark-sql-JdbcRelationProvider.adoc#createRelation-RelationProvider[create a `JDBCRelation`] (with proper `JDBCPartitions` with `WHERE` clause).

When defined, <<jdbc-lowerBound, lowerBound>>, <<jdbc-upperBound, upperBound>> and <<jdbc-numPartitions, numPartitions>> options are required.

When undefined, <<jdbc-lowerBound, lowerBound>> and <<jdbc-upperBound, upperBound>> have to be undefined.

| `truncate`
| (used only for writing) Enables table truncation.

Defaults to `false`

| [[jdbc-upperBound]] `upperBound`
| Upper bound of the partition column

| `url`
| (*required*)
|===

Internally, `jdbc` creates a <<JDBCOptions, JDBCOptions>> from `url`, `table` and `extraOptions` with `connectionProperties`.

`jdbc` then creates one `JDBCPartition` per `predicates`.

In the end, `jdbc` requests the <<sparkSession, SparkSession>> to link:spark-sql-SparkSession.adoc#baseRelationToDataFrame[create a `DataFrame`] for a link:spark-sql-BaseRelation-JDBCRelation.adoc[JDBCRelation] (given `JDBCPartitions` and `JDBCOptions` created earlier).

[NOTE]
====
`jdbc` does not support a custom <<schema, schema>> and reports an `AnalysisException` if defined:

```
User specified schema not supported with `[jdbc]`
```
====

NOTE: `jdbc` method uses `java.util.Properties` (and appears overly Java-centric). Use <<format, format("jdbc")>> instead.

TIP: Review the exercise link:exercises/spark-exercise-dataframe-jdbc-postgresql.adoc[Creating DataFrames from Tables using JDBC and PostgreSQL].

=== [[textFile]] Loading Datasets From Text Files -- `textFile` Method

[source, scala]
----
textFile(path: String): Dataset[String]
textFile(paths: String*): Dataset[String]
----

`textFile` loads one or many text files into a typed link:spark-sql-Dataset.adoc[Dataset[String\]].

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...

import org.apache.spark.sql.Dataset
val lines: Dataset[String] = spark
  .read
  .textFile("README.md")
----

NOTE: `textFile` are similar to <<text, text>> family of methods in that they both read text files but `text` methods return untyped `DataFrame` while `textFile` return typed `Dataset[String]`.

Internally, `textFile` passes calls on to <<text, text>> method and link:spark-sql-Dataset.adoc#select[selects] the only `value` column before it applies `Encoders.STRING` link:spark-sql-Encoder.adoc[encoder].

=== [[creating-instance]] Creating DataFrameReader Instance

`DataFrameReader` takes the following when created:

* [[sparkSession]] link:spark-sql-SparkSession.adoc[SparkSession]
