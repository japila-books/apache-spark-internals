== [[ShuffleWriteMetrics]] ShuffleWriteMetrics

`ShuffleWriteMetrics` is a <<accumulators, collection of accumulators>> that represents task metrics about writing shuffle data.

`ShuffleWriteMetrics` tracks the following task metrics:

1. <<bytesWritten, Shuffle Bytes Written>>
2. <<writeTime, Shuffle Write Time>>
3. <<recordsWritten, Shuffle Records Written>>

NOTE: link:spark-accumulators.adoc[Accumulators] allow tasks (running on executors) to communicate with the driver.

[[accumulators]]
.ShuffleWriteMetrics's Accumulators
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[_bytesWritten]] `_bytesWritten`
| Accumulator to track how many shuffle bytes were written in a shuffle task.

Used when `ShuffleWriteMetrics` is requested the <<bytesWritten, shuffle bytes written>> and to <<incBytesWritten, increment>> or <<decBytesWritten, decrement>> it.

NOTE: `_bytesWritten` is available as `internal.metrics.shuffle.write.bytesWritten` (internally `shuffleWrite.BYTES_WRITTEN`) in link:spark-taskscheduler-taskmetrics.adoc[TaskMetrics].

| [[_writeTime]] `_writeTime`
| Accumulator to track shuffle write time (as 64-bit integer) of a shuffle task.

Used when `ShuffleWriteMetrics` is requested the <<writeTime, shuffle write time>> and to <<incWriteTime, increment it>>.

NOTE: `_writeTime` is available as `internal.metrics.shuffle.write.writeTime` (internally `shuffleWrite.WRITE_TIME`) in link:spark-taskscheduler-taskmetrics.adoc[TaskMetrics].

| [[_recordsWritten]] `_recordsWritten`
| Accumulator to track how many shuffle records were written in a shuffle task.

Used when `ShuffleWriteMetrics` is requested the <<recordsWritten, shuffle records written>> and to <<incRecordsWritten, increment>> or <<decRecordsWritten, decrement>> it.

NOTE: `_recordsWritten` is available as `internal.metrics.shuffle.write.recordsWritten` (internally `shuffleWrite.RECORDS_WRITTEN`) in link:spark-taskscheduler-taskmetrics.adoc[TaskMetrics].

|===

=== [[decRecordsWritten]] `decRecordsWritten` Method

CAUTION: FIXME

=== [[decBytesWritten]] `decBytesWritten` Method

CAUTION: FIXME

=== [[writeTime]] `writeTime` Method

CAUTION: FIXME

=== [[recordsWritten]] `recordsWritten` Method

CAUTION: FIXME

=== [[bytesWritten]] Returning Number of Shuffle Bytes Written -- `bytesWritten` Method

[source, scala]
----
bytesWritten: Long
----

`bytesWritten` represents the *shuffle bytes written* metrics of a shuffle task.

Internally, `bytesWritten` returns the sum of <<_bytesWritten, _bytesWritten>> internal accumulator.

[NOTE]
====
`bytesWritten` is used when:

1. `ShuffleWriteMetricsUIData` is created

2. In <<decBytesWritten, decBytesWritten>>

3. link:spark-scheduler-listeners-statsreportlistener.adoc#onStageCompleted[`StatsReportListener` intercepts stage completed events] to show shuffle bytes written

4. link:spark-ShuffleExternalSorter.adoc#writeSortedFile[`ShuffleExternalSorter` does `writeSortedFile`] (to `incDiskBytesSpilled`)

5. link:spark-JsonProtocol.adoc#taskMetricsToJson[`JsonProtocol` converts `ShuffleWriteMetrics` to JSON]

6. link:spark-webui-executors-ExecutorsListener.adoc#onTaskEnd[`ExecutorsListener` intercepts task end events] to update executor metrics

7. link:spark-webui-JobProgressListener.adoc#updateAggregateMetrics[`JobProgressListener` updates stage and executor metrics]
====

=== [[incBytesWritten]] Incrementing Shuffle Bytes Written Metrics -- `incBytesWritten` Method

[source, scala]
----
incBytesWritten(v: Long): Unit
----

`incBytesWritten` simply adds `v` to <<_bytesWritten, _bytesWritten>> internal accumulator.

[NOTE]
====
`incBytesWritten` is used when:

1. link:spark-UnsafeShuffleWriter.adoc#mergeSpills[`UnsafeShuffleWriter` does `mergeSpills`]

2. link:spark-blockmanager-DiskBlockObjectWriter.adoc#updateBytesWritten[`DiskBlockObjectWriter` does `updateBytesWritten`]

3. link:spark-JsonProtocol.adoc#taskMetricsFromJson[`JsonProtocol` creates `TaskMetrics` from JSON]

====

=== [[incWriteTime]] Incrementing Shuffle Write Time Metrics -- `incWriteTime` Method

[source, scala]
----
incWriteTime(v: Long): Unit
----

`incWriteTime` simply adds `v` to <<_writeTime, _writeTime>> internal accumulator.

[NOTE]
====
`incWriteTime` is used when:

1. link:spark-SortShuffleWriter.adoc#stop[`SortShuffleWriter` stops].

2. `BypassMergeSortShuffleWriter` link:spark-BypassMergeSortShuffleWriter.adoc#write[writes records] (i.e. when it initializes `DiskBlockObjectWriter` partition writers) and later when link:spark-BypassMergeSortShuffleWriter.adoc#writePartitionedFile[concatenates per-partition files into a single file].

3. link:spark-UnsafeShuffleWriter.adoc#mergeSpillsWithTransferTo[`UnsafeShuffleWriter` does `mergeSpillsWithTransferTo`].

4. link:spark-blockmanager-DiskBlockObjectWriter.adoc#commitAndGet[`DiskBlockObjectWriter` does `commitAndGet`] (but only when `syncWrites` flag is enabled that forces outstanding writes to disk).

5. link:spark-JsonProtocol.adoc#taskMetricsFromJson[`JsonProtocol` creates `TaskMetrics` from JSON]

6. `TimeTrackingOutputStream` does its operation (after all it is an output stream to track shuffle write time).
====

=== [[incRecordsWritten]] Incrementing Shuffle Records Written Metrics -- `incRecordsWritten` Method

[source, scala]
----
incRecordsWritten(v: Long): Unit
----

`incRecordsWritten` simply adds `v` to <<_recordsWritten, _recordsWritten>> internal accumulator.

[NOTE]
====
`incRecordsWritten` is used when:

1. link:spark-ShuffleExternalSorter.adoc#writeSortedFile[`ShuffleExternalSorter` does `writeSortedFile`]

2. link:spark-blockmanager-DiskBlockObjectWriter.adoc#recordWritten[`DiskBlockObjectWriter` does `recordWritten`]

3. link:spark-JsonProtocol.adoc#taskMetricsFromJson[`JsonProtocol` creates `TaskMetrics` from JSON]

====
